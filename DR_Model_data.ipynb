{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbedac78-9156-47cb-ae8c-23c113d02422",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %pip install mlflow==2.19.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b32d160-eba5-4e46-b1c0-2a60be85eadb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %pip install databricks-sdk==0.47.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2b916d1-3b06-4a20-9efc-318df8bb56e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e135a58a-4199-4349-b8f9-0173d6190c4d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip show databricks-sdk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21079b6d-587b-46da-9415-af8ebc002f75",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "print(mlflow.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93ee5629-626d-4ad6-adb4-3a7605c391fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# dbutils.widgets.removeAll()\n",
    "dbutils.widgets.text(\"catalog_names\", \"\")#source and target catalog names like dictionary\n",
    "dbutils.widgets.text(\"target_schema_owner\", \"\")\n",
    "dbutils.widgets.text(\"max_workers\", \"\")\n",
    "dbutils.widgets.text(\"log_table_name\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff63f959-a4f1-46a1-ad76-524b7ef329fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "catalog_pair_names_list = [catalog_pair.strip() for catalog_pair in dbutils.widgets.get(\"catalog_names\").split(\",\")]\n",
    "target_schema_owner_list = [catalog_schema_pair.strip() for catalog_schema_pair in dbutils.widgets.get(\"target_schema_owner\").split(\",\")]\n",
    "max_workers = int(dbutils.widgets.get(\"max_workers\"))\n",
    "log_table_name = dbutils.widgets.get(\"log_table_name\")\n",
    "print(catalog_pair_names_list, target_schema_owner_list, max_workers, log_table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "946565cb-9b3c-4b5d-ad9d-f7274b949ca2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# for catalog_pair in catalog_pair_names_list:\n",
    "#     second_value = catalog_pair.split(\":\")[1]\n",
    "#     print(second_value)\n",
    "#     spark.sql(f\"DROP CATALOG IF EXISTS `{second_value}` CASCADE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46f27899-04c7-4cfc-b2fe-cc9263f17377",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from mlflow.tracking import MlflowClient\n",
    "from mlflow.exceptions import RestException\n",
    "from pyspark.sql.types import StructType, StructField, StringType, ArrayType, TimestampType\n",
    "from datetime import datetime\n",
    "import traceback\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "\n",
    "# Get the current notebook name\n",
    "notebook_path = dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get()\n",
    "notebook_name = notebook_path.split('/')[-1]\n",
    "\n",
    "# Schema definition for log table\n",
    "log_table_schema = StructType([\n",
    "    StructField(\"notebook_name\", StringType(), True),\n",
    "    StructField(\"entity_type\", StringType(), True),\n",
    "    StructField(\"entity_name\", StringType(), True),\n",
    "    StructField(\"action\", StringType(), True),\n",
    "    StructField(\"status\", StringType(), True),\n",
    "    StructField(\"message\", StringType(), True),\n",
    "    StructField(\"timestamp\", TimestampType(), True),\n",
    "    StructField(\"results_data\", ArrayType(StructType([\n",
    "        StructField(\"key\", StringType(), True),\n",
    "        StructField(\"value\", StringType(), True)\n",
    "    ])), True)\n",
    "])\n",
    "\n",
    "# Create log table\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {log_table_name}\n",
    "    (\n",
    "        notebook_name STRING,\n",
    "        entity_type STRING,\n",
    "        entity_name STRING,\n",
    "        action STRING,\n",
    "        status STRING,\n",
    "        message STRING,\n",
    "        timestamp TIMESTAMP,\n",
    "        results_data ARRAY<STRUCT<key: STRING, value: STRING>>\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "# --- Helper Functions (modified from template) ---\n",
    "def execute_and_log_sql(sql_command, entity_type, entity_name, action, success_message, error_message):\n",
    "    try:\n",
    "        results_df = spark.sql(sql_command)\n",
    "        results = [\n",
    "            {\"key\": col_name, \"value\": str(row[col_name])}\n",
    "            for row in results_df.collect()\n",
    "            for col_name in row.__fields__\n",
    "        ]\n",
    "        log_entry = {\n",
    "            \"entity_type\": entity_type,\n",
    "            \"entity_name\": entity_name,\n",
    "            \"action\": action,\n",
    "            \"status\": \"success\",\n",
    "            \"message\": success_message,\n",
    "            \"timestamp\": datetime.now(),\n",
    "            \"results_data\": results\n",
    "        }\n",
    "    except Exception as e:\n",
    "        log_entry = {\n",
    "            \"entity_type\": entity_type,\n",
    "            \"entity_name\": entity_name,\n",
    "            \"action\": action,\n",
    "            \"status\": \"error\",\n",
    "            \"message\": f\"{error_message}. Error: {e}\",\n",
    "            \"timestamp\": datetime.now(),\n",
    "            \"results_data\": []\n",
    "        }\n",
    "    insert_log_entry(log_entry)\n",
    "\n",
    "def insert_log_entry(log_data):\n",
    "    try:\n",
    "        # Add the notebook_name to the log_data dictionary\n",
    "        log_data[\"notebook_name\"] = notebook_name\n",
    "        spark.createDataFrame([log_data], schema=log_table_schema).write.mode(\"append\").saveAsTable(log_table_name)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to insert log entry: {log_data}. Error: {e}\")\n",
    "\n",
    "def create_catalog_if_not_exists(target_catalog):\n",
    "    sql_command = f\"CREATE CATALOG IF NOT EXISTS `{target_catalog}`\"\n",
    "    execute_and_log_sql(\n",
    "        sql_command,\n",
    "        \"catalog\",\n",
    "        f\"`{target_catalog}`\",\n",
    "        \"create\",\n",
    "        f\"Catalog `{target_catalog}` created or already exists.\",\n",
    "        f\"Failed to create catalog `{target_catalog}`.\"\n",
    "    )\n",
    "\n",
    "def create_schema_if_not_exists(target_catalog, schema_name):\n",
    "    target_schema_fqn = f\"`{target_catalog}`.`{schema_name}`\"\n",
    "    execute_and_log_sql(\n",
    "        f\"CREATE SCHEMA IF NOT EXISTS {target_schema_fqn}\",\n",
    "        \"schema\",\n",
    "        target_schema_fqn,\n",
    "        \"create\",\n",
    "        f\"Schema {target_schema_fqn} created or already exists.\",\n",
    "        f\"Failed to create schema {target_schema_fqn}.\"\n",
    "    )\n",
    "\n",
    "# --- Core Logic for Model Synchronization ---\n",
    "def get_all_registered_models_once():\n",
    "    \"\"\"\n",
    "    Retrieves all registered models from the workspace a single time.\n",
    "    Returns a list of all MLflow registered model objects.\n",
    "    \"\"\"\n",
    "    client = MlflowClient()\n",
    "    models = []\n",
    "    page_token = None\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            results = client.search_registered_models(max_results=100, page_token=page_token)\n",
    "            models.extend(results)\n",
    "            if results.token:\n",
    "                page_token = results.token\n",
    "            else:\n",
    "                break\n",
    "        except RestException as e:\n",
    "            error_message = f\"Failed to search all models in the workspace. Error: {e}\"\n",
    "            insert_log_entry({\n",
    "                \"entity_type\": \"model_search\",\n",
    "                \"entity_name\": \"workspace\",\n",
    "                \"action\": \"search\",\n",
    "                \"status\": \"error\",\n",
    "                \"message\": error_message,\n",
    "                \"timestamp\": datetime.now(),\n",
    "                \"results_data\": []\n",
    "            })\n",
    "            return []\n",
    "    return models\n",
    "\n",
    "def get_target_model_versions(target_model_fqn):\n",
    "    \"\"\"\n",
    "    Retrieves a set of version numbers for a given model in the target catalog.\n",
    "    Returns a set of integers.\n",
    "    \"\"\"\n",
    "    client = MlflowClient()\n",
    "    try:\n",
    "        model = client.get_registered_model(target_model_fqn)\n",
    "        return {v.version for v in model.latest_versions}\n",
    "    except RestException:\n",
    "        # Model does not exist, which is expected for new models.\n",
    "        return set()\n",
    "\n",
    "def update_aliases(source_model_fqn, source_version, target_model_fqn, target_version):\n",
    "    \"\"\"\n",
    "    Copies aliases from a source model version to a target model version.\n",
    "    \"\"\"\n",
    "    client = MlflowClient()\n",
    "    try:\n",
    "        source_version_details = client.get_model_version(source_model_fqn, source_version)\n",
    "        source_aliases = source_version_details.aliases\n",
    "        if source_aliases:\n",
    "            for alias in source_aliases:\n",
    "                print(f\"  Setting alias '{alias}' on target version {target_version}\")\n",
    "                client.set_registered_model_alias(target_model_fqn, alias, target_version)\n",
    "                insert_log_entry({\n",
    "                    \"entity_type\": \"model_alias\",\n",
    "                    \"entity_name\": f\"{target_model_fqn}/{target_version}/{alias}\",\n",
    "                    \"action\": \"set\",\n",
    "                    \"status\": \"success\",\n",
    "                    \"message\": f\"Successfully set alias '{alias}' on model version {target_version}.\",\n",
    "                    \"timestamp\": datetime.now(),\n",
    "                    \"results_data\": [\n",
    "                        {\"key\": \"model_name\", \"value\": target_model_fqn},\n",
    "                        {\"key\": \"version\", \"value\": str(target_version)},\n",
    "                        {\"key\": \"alias\", \"value\": alias}\n",
    "                    ]\n",
    "                })\n",
    "    except RestException as e:\n",
    "        error_message = f\"Failed to copy aliases for version {source_version}. Error: {e}\"\n",
    "        insert_log_entry({\n",
    "            \"entity_type\": \"model_alias\",\n",
    "            \"entity_name\": f\"{target_model_fqn}/{target_version}\",\n",
    "            \"action\": \"set\",\n",
    "            \"status\": \"error\",\n",
    "            \"message\": error_message,\n",
    "            \"timestamp\": datetime.now(),\n",
    "            \"results_data\": []\n",
    "        })\n",
    "\n",
    "def process_model_sync(source_catalog, source_schema, target_catalog, target_schema, model):\n",
    "    \"\"\"\n",
    "    Processes a single registered model, checking and copying new versions.\n",
    "    \"\"\"\n",
    "    source_model_fqn = model.name\n",
    "    # Construct the target model's fully qualified name\n",
    "    source_parts = source_model_fqn.split('.')\n",
    "    model_name = source_parts[-1]\n",
    "    target_model_fqn = f\"{target_catalog}.{target_schema}.{model_name}\"\n",
    "\n",
    "    print(f\"Processing model: {source_model_fqn} -> {target_model_fqn}\")\n",
    "\n",
    "    # Get existing versions in the target\n",
    "    target_versions = get_target_model_versions(target_model_fqn)\n",
    "\n",
    "    # Get all versions from the source model\n",
    "    client = MlflowClient()\n",
    "    try:\n",
    "        source_model_versions = client.search_model_versions(filter_string=f\"name='{source_model_fqn}'\")\n",
    "    except RestException as e:\n",
    "        error_message = f\"Failed to retrieve versions for source model {source_model_fqn}. Error: {e}\"\n",
    "        insert_log_entry({\n",
    "            \"entity_type\": \"model\",\n",
    "            \"entity_name\": source_model_fqn,\n",
    "            \"action\": \"get_versions\",\n",
    "            \"status\": \"error\",\n",
    "            \"message\": error_message,\n",
    "            \"timestamp\": datetime.now(),\n",
    "            \"results_data\": []\n",
    "        })\n",
    "        return\n",
    "    \n",
    "    # Sort the versions by version number in ascending order\n",
    "    sorted_versions = sorted(source_model_versions, key=lambda version: int(version.version))\n",
    "\n",
    "    # Loop through source versions and copy if they don't exist in the target\n",
    "    for version in sorted_versions:\n",
    "        source_version_number = int(version.version)\n",
    "        if source_version_number not in target_versions:\n",
    "            print(f\".  Copying new version {source_version_number} for model {model_name}\")\n",
    "            try:\n",
    "                copy_result = client.copy_model_version(\n",
    "                    f\"models:/{source_model_fqn}/{source_version_number}\",\n",
    "                    f\"{target_model_fqn}\"\n",
    "                )\n",
    "                \n",
    "                # Extract and format the output for results_data\n",
    "                results_data_output = [\n",
    "                    {\"key\": \"copy_status\", \"value\": \"success\"},\n",
    "                    {\"key\": \"source_uri\", \"value\": f\"models:/{source_model_fqn}/{source_version_number}\"},\n",
    "                    {\"key\": \"target_uri\", \"value\": f\"{target_model_fqn}\"},\n",
    "                    {\"key\": \"version_copied\", \"value\": str(source_version_number)},\n",
    "                    {\"key\": \"version_result\", \"value\": str(copy_result)}\n",
    "                ]\n",
    "\n",
    "                target_version_number = copy_result.version\n",
    "\n",
    "                insert_log_entry({\n",
    "                    \"entity_type\": \"model_version\",\n",
    "                    \"entity_name\": f\"{target_model_fqn}/{source_version_number}\",\n",
    "                    \"action\": \"copy\",\n",
    "                    \"status\": \"success\",\n",
    "                    \"message\": f\"Successfully copied model version {source_version_number} from {source_model_fqn} to {target_model_fqn}.\",\n",
    "                    \"timestamp\": datetime.now(),\n",
    "                    \"results_data\": results_data_output\n",
    "                })\n",
    "\n",
    "                # --- NEW: Copy aliases after successful version copy ---\n",
    "                update_aliases(source_model_fqn, source_version_number, target_model_fqn, target_version_number)\n",
    "\n",
    "            except RestException as e:\n",
    "                error_message = f\"Failed to copy model version {source_version_number} from {source_model_fqn} to {target_model_fqn}. Error: {e}\"\n",
    "                insert_log_entry({\n",
    "                    \"entity_type\": \"model_version\",\n",
    "                    \"entity_name\": f\"{target_model_fqn}/{source_version_number}\",\n",
    "                    \"action\": \"copy\",\n",
    "                    \"status\": \"error\",\n",
    "                    \"message\": error_message,\n",
    "                    \"timestamp\": datetime.now(),\n",
    "                    \"results_data\": []\n",
    "                })\n",
    "        else:\n",
    "            print(f\"  Version {source_version_number} already exists in target. Skipping.\")\n",
    "            insert_log_entry({\n",
    "                \"entity_type\": \"model_version\",\n",
    "                \"entity_name\": f\"{target_model_fqn}/{source_version_number}\",\n",
    "                \"action\": \"copy\",\n",
    "                \"status\": \"skipped\",\n",
    "                \"message\": f\"Version {source_version_number} for model {target_model_fqn} already exists. Skipping copy.\",\n",
    "                \"timestamp\": datetime.now(),\n",
    "                \"results_data\": []\n",
    "            })\n",
    "            \n",
    "# --- Main function to orchestrate the process ---\n",
    "def main(catalog_pair_names_list, target_schema_owner_list, max_workers):\n",
    "    print(\"Starting model synchronization process...\")\n",
    "    \n",
    "    # Get all models once at the beginning\n",
    "    all_workspace_models = get_all_registered_models_once()\n",
    "    print(f\"Total models found in workspace: {len(all_workspace_models)}\")\n",
    "    \n",
    "    all_tasks = []\n",
    "\n",
    "    for catalog_pair_string in catalog_pair_names_list:\n",
    "        try:\n",
    "            source_catalog, target_catalog = catalog_pair_string.split(':')\n",
    "            print(f\"Processing catalog pair: {source_catalog} -> {target_catalog}\")\n",
    "            \n",
    "            create_catalog_if_not_exists(target_catalog)\n",
    "            \n",
    "            source_schemas_df = spark.sql(f\"SHOW SCHEMAS IN `{source_catalog}`\")\n",
    "            source_schemas = [row.databaseName for row in source_schemas_df.collect()]\n",
    "            \n",
    "            for schema in source_schemas:\n",
    "                if schema == 'information_schema':\n",
    "                    continue\n",
    "\n",
    "                source_schema_fqn = f\"`{source_catalog}`.`{schema}`\"\n",
    "                target_schema_fqn = f\"`{target_catalog}`.`{schema}`\"\n",
    "                print(f\"  Processing schema: {source_schema_fqn} -> {target_schema_fqn}\")\n",
    "\n",
    "                create_schema_if_not_exists(target_catalog, schema)\n",
    "                \n",
    "                # Filter the pre-fetched list of models for the current catalog and schema\n",
    "                models_to_process = [\n",
    "                    model for model in all_workspace_models\n",
    "                    if model.name.startswith(f\"{source_catalog}.{schema}.\")\n",
    "                ]\n",
    "\n",
    "                if not models_to_process:\n",
    "                    print(f\"  No models found in schema {source_schema_fqn}. Skipping.\")\n",
    "                    continue\n",
    "\n",
    "                for model in models_to_process:\n",
    "                    all_tasks.append((source_catalog, schema, target_catalog, schema, model))\n",
    "\n",
    "        except Exception as e:\n",
    "            insert_log_entry({\n",
    "                \"entity_type\": \"catalog_pair\",\n",
    "                \"entity_name\": f\"Processing of {catalog_pair_string}\",\n",
    "                \"action\": \"unhandled_error\",\n",
    "                \"status\": \"error\",\n",
    "                \"message\": f\"An unhandled error occurred: {e}\\n{traceback.format_exc()}\",\n",
    "                \"timestamp\": datetime.now(),\n",
    "                \"results_data\": []\n",
    "            })\n",
    "    \n",
    "    print(f\"Collected {len(all_tasks)} models to process.\")\n",
    "\n",
    "    # Process all models in parallel\n",
    "    print(\"Starting parallel model processing...\")\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        executor.map(\n",
    "            lambda task: process_model_sync(*task),\n",
    "            all_tasks\n",
    "        )\n",
    "    \n",
    "    print(\"Model synchronization complete.\")\n",
    "\n",
    "# --- Run the main job ---\n",
    "# Assuming these variables are defined in the environment.\n",
    "# For example:\n",
    "# catalog_pair_names_list = [\"source_catalog:target_catalog\"]\n",
    "# target_schema_owner_list = [\"target_catalog.satyendranath_sure:owner_group\"]\n",
    "# max_workers = 8\n",
    "# log_table_name = \"your_catalog.your_schema.model_sync_log\"\n",
    "main(catalog_pair_names_list, target_schema_owner_list, max_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fdc11813-2c00-4e77-b7c3-26c5f5dc1075",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1754948112992}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from ${log_table_name} order by timestamp desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99752649-dcbe-4fff-b1d7-de708a0652a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0c1e6bd-1c5a-4213-a6b3-3ceb7cb46cc8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d40b06da-3b23-412b-a7c5-e1356fb0336e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68de7672-75ff-4d70-b73a-43f3fd2173e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90aa59aa-7073-4fe7-9fed-47c6bc3318ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97ebd6ef-8e17-4a9f-bccf-f86aab80bb3e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9de1c82-4ed0-4f29-9965-81a2ea4dc5cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "dbe_41a50460-c90b-4840-9288-afcb847395d5",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2795296647743778,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "DR_Model_data",
   "widgets": {
    "catalog_names": {
     "currentValue": "_eo_amperity:dr_eo_amperity, accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample:dr_accuweather, test_not:dr_test_not, satya_share:satya_dr_catalog",
     "nuid": "5f06e975-cdb8-43fe-a80b-4c55190ca561",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "catalog_names",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "catalog_names",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "log_table_name": {
     "currentValue": "users.satyendranath_sure.dr_log_table_name",
     "nuid": "887916a1-7c6d-4394-96ee-6803513fc545",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "log_table_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "log_table_name",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "max_workers": {
     "currentValue": "5",
     "nuid": "6d8539ad-a4d2-4f54-bf5c-9b7689d1d058",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "max_workers",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "max_workers",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "target_schema_owner": {
     "currentValue": "dr_eo_amperity.bronze:grp_test_clt_usa, dr_eo_amperity.silver:grp_test_clt_usa, dr_eo_amperity.default:grp_test_clt_usa",
     "nuid": "af424e30-5de1-47ac-8079-3fea6dfda1df",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "target_schema_owner",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "target_schema_owner",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}