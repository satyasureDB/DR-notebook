{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ad0f10f1-c82e-4c1c-b1ad-bac2366c9e23",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# This notebook is designed to automate the process of replicating all data volumes from **source catalogs** to **target catalogs** in Databricks Unity Catalog, while ignoring the `information_schema`. The replication is incremental, copying only new or updated files.\n",
    "\n",
    "# ## Parameters Overview\n",
    "\n",
    "# 1.  **catalog_names**:\n",
    "#     - **Type**: String\n",
    "#     - **Description**: A comma-separated list of catalog pairs in the format `source_catalog:target_catalog`. The script will discover and replicate all volumes from the source catalog to the target catalog for each pair.\n",
    "#     - **Example Value**: `'source_catalog_A:target_catalog_A, source_catalog_B:target_catalog_B'`\n",
    "\n",
    "# 2.  **max_workers**:\n",
    "#     - **Type**: Integer\n",
    "#     - **Description**: Maximum number of worker threads for parallel processing of volume replication tasks.\n",
    "\n",
    "# 3.  **log_table_name**:\n",
    "#     - **Type**: String\n",
    "#     - **Description**: The fully qualified name of the log table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93ee5629-626d-4ad6-adb4-3a7605c391fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# dbutils.widgets.removeAll()\n",
    "dbutils.widgets.text(\"catalog_names\", \"\")#source and target catalog names like dictionary\n",
    "dbutils.widgets.text(\"target_schema_owner\", \"\")\n",
    "dbutils.widgets.text(\"max_workers\", \"5\")\n",
    "dbutils.widgets.text(\"log_table_name\", \"users.satyendranath_sure.dr_log_table_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff63f959-a4f1-46a1-ad76-524b7ef329fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_eo_amperity:dr_eo_amperity', 'accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample:dr_accuweather', 'test_not:dr_test_not', 'satya_share:satya_dr_catalog'] ['dr_eo_amperity.bronze:grp_test_clt_usa', 'dr_eo_amperity.silver:grp_test_clt_usa', 'dr_eo_amperity.default:grp_test_clt_usa'] 5 users.satyendranath_sure.dr_log_table_name\n"
     ]
    }
   ],
   "source": [
    "catalog_pair_names_list = [catalog_pair.strip() for catalog_pair in dbutils.widgets.get(\"catalog_names\").split(\",\")]\n",
    "target_schema_owner_list = [catalog_schema_pair.strip() for catalog_schema_pair in dbutils.widgets.get(\"target_schema_owner\").split(\",\")]\n",
    "max_workers = int(dbutils.widgets.get(\"max_workers\"))\n",
    "log_table_name = dbutils.widgets.get(\"log_table_name\")\n",
    "print(catalog_pair_names_list, target_schema_owner_list, max_workers, log_table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "946565cb-9b3c-4b5d-ad9d-f7274b949ca2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# for catalog_pair in catalog_pair_names_list:\n",
    "#     second_value = catalog_pair.split(\":\")[1]\n",
    "#     print(second_value)\n",
    "#     spark.sql(f\"DROP CATALOG IF EXISTS `{second_value}` CASCADE\")\n",
    "# spark.sql(f\"DROP table if exists `{log_table_name}`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe5b881f-feae-477f-a041-0e9f2aae4166",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "working - not deletes"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Databricks Volume DR Sync.\nStarting sync for volume: _eo_amperity.bronze.chuck\nStarting sync for volume: satya_share.satyendranath_sure.satya_external_volumeStarting sync for volume: satya_share.satyendranath_sure.satya_volume\n\nDatabricks Volume DR Sync completed.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import traceback\n",
    "import concurrent.futures\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType, ArrayType\n",
    "from datetime import datetime\n",
    "import json\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "# The 'spark' and 'dbutils' objects are pre-defined in a Databricks environment.\n",
    "\n",
    "# Get the current notebook name\n",
    "notebook_path = dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get()\n",
    "notebook_name = notebook_path.split('/')[-1]\n",
    "\n",
    "# Schema definition for log table\n",
    "log_table_schema = StructType([\n",
    "    StructField(\"notebook_name\", StringType(), True),\n",
    "    StructField(\"entity_type\", StringType(), True),\n",
    "    StructField(\"entity_name\", StringType(), True),\n",
    "    StructField(\"action\", StringType(), True),\n",
    "    StructField(\"status\", StringType(), True),\n",
    "    StructField(\"message\", StringType(), True),\n",
    "    StructField(\"timestamp\", TimestampType(), True),\n",
    "    StructField(\"results_data\", ArrayType(StructType([\n",
    "        StructField(\"key\", StringType(), True),\n",
    "        StructField(\"value\", StringType(), True)\n",
    "    ])), True)\n",
    "])\n",
    "\n",
    "# Create log table\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {log_table_name}\n",
    "    (\n",
    "        notebook_name STRING,\n",
    "        entity_type STRING,\n",
    "        entity_name STRING,\n",
    "        action STRING,\n",
    "        status STRING,\n",
    "        message STRING,\n",
    "        timestamp TIMESTAMP,\n",
    "        results_data ARRAY<STRUCT<key: STRING, value: STRING>>\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "def insert_log_entry(log_data):\n",
    "    \"\"\"\n",
    "    Inserts a log entry into the designated log table.\n",
    "    \n",
    "    Args:\n",
    "        log_data (dict): A dictionary containing log information\n",
    "    \"\"\"\n",
    "    try:\n",
    "        log_data[\"notebook_name\"] = notebook_name\n",
    "        # Ensure results_data is a list of dictionaries as per schema\n",
    "        if \"results_data\" not in log_data or not isinstance(log_data[\"results_data\"], list):\n",
    "            log_data[\"results_data\"] = []\n",
    "        \n",
    "        df = spark.createDataFrame([log_data], schema=log_table_schema)\n",
    "        df.write.mode(\"append\").saveAsTable(log_table_name)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to insert log entry: {log_data}. Error: {e}\")\n",
    "\n",
    "def get_previous_run_info(source_path):\n",
    "    \"\"\"\n",
    "    Retrieves the modification times of files from the last successful run for a given source path.\n",
    "    \n",
    "    This is used to identify which files are new or have been modified since the last copy.\n",
    "    \n",
    "    Args:\n",
    "        source_path (str): The source path (directory) to check for previously copied files.\n",
    "    \n",
    "    Returns:\n",
    "        dict: A dictionary mapping file names to their last successful modification timestamp.\n",
    "    \"\"\"\n",
    "    spark_session = SparkSession.builder.getOrCreate()\n",
    "    try:\n",
    "        df = spark_session.sql(f\"\"\"\n",
    "            SELECT\n",
    "                entity_name,\n",
    "                results_data\n",
    "            FROM {log_table_name}\n",
    "            WHERE\n",
    "                entity_type = 'file' AND\n",
    "                action = 'copy' AND\n",
    "                status = 'success'\n",
    "            ORDER BY timestamp DESC\n",
    "        \"\"\")\n",
    "        \n",
    "        previous_runs = {}\n",
    "        for row in df.collect():\n",
    "            try:\n",
    "                file_path = row['entity_name']\n",
    "                results_data = row['results_data']\n",
    "                mod_time = None\n",
    "                for item in results_data:\n",
    "                    if item.key == 'modificationTime':\n",
    "                        mod_time = int(item.value)\n",
    "                        break\n",
    "                \n",
    "                if mod_time:\n",
    "                    # Extract the relative file path to handle nested folders\n",
    "                    relative_path = file_path.replace(source_path, '', 1).lstrip('/')\n",
    "                    if relative_path not in previous_runs:\n",
    "                        previous_runs[relative_path] = mod_time\n",
    "            except Exception as e:\n",
    "                print(f\"Error parsing log entry: {e}\")\n",
    "                continue\n",
    "        return previous_runs\n",
    "    except Exception as e:\n",
    "        log_entry = {\n",
    "            \"entity_type\": \"info\",\n",
    "            \"entity_name\": source_path,\n",
    "            \"action\": \"get_previous_run_info\",\n",
    "            \"status\": \"error\",\n",
    "            \"message\": f\"Error fetching previous run info. Error: {e}\",\n",
    "            \"timestamp\": datetime.now(),\n",
    "            \"results_data\": []\n",
    "        }\n",
    "        insert_log_entry(log_entry)\n",
    "        return {}\n",
    "\n",
    "def sync_volume_recursively(source_path, target_path, prev_run_info):\n",
    "    \"\"\"\n",
    "    Recursively copies files from a source path to a target path,\n",
    "    copying only new or modified files.\n",
    "    \n",
    "    Args:\n",
    "        source_path (str): The full source path (dbfs:/Volumes/...) to sync from.\n",
    "        target_path (str): The full target path (dbfs:/Volumes/...) to sync to.\n",
    "        prev_run_info (dict): A dictionary of previously copied files and their modification times.\n",
    "    \"\"\"\n",
    "    # Ensure the target directory exists before copying files into it.\n",
    "    try:\n",
    "        dbutils.fs.mkdirs(target_path)\n",
    "    except Exception as e:\n",
    "        log_entry = {\n",
    "            \"entity_type\": \"directory\",\n",
    "            \"entity_name\": target_path,\n",
    "            \"action\": \"create\",\n",
    "            \"status\": \"error\",\n",
    "            \"message\": f\"Failed to create target directory. Error: {e}\",\n",
    "            \"timestamp\": datetime.now(),\n",
    "            \"results_data\": []\n",
    "        }\n",
    "        insert_log_entry(log_entry)\n",
    "\n",
    "    try:\n",
    "        # List contents of the source path.\n",
    "        files = dbutils.fs.ls(source_path)\n",
    "    except Exception as e:\n",
    "        log_entry = {\n",
    "            \"entity_type\": \"directory\",\n",
    "            \"entity_name\": source_path,\n",
    "            \"action\": \"list\",\n",
    "            \"status\": \"error\",\n",
    "            \"message\": f\"Error listing contents of directory. Error: {e}\",\n",
    "            \"timestamp\": datetime.now(),\n",
    "            \"results_data\": []\n",
    "        }\n",
    "        insert_log_entry(log_entry)\n",
    "        return\n",
    "\n",
    "    for file_info in files:\n",
    "        # Check if the item is a directory by looking for a trailing slash in its name\n",
    "        if file_info.name.endswith('/'):\n",
    "            # It's a directory, so we need to process it recursively.\n",
    "            log_entry = {\n",
    "                \"entity_type\": \"directory\",\n",
    "                \"entity_name\": file_info.path,\n",
    "                \"action\": \"list\",\n",
    "                \"status\": \"success\",\n",
    "                \"message\": \"Found directory to traverse.\",\n",
    "                \"timestamp\": datetime.now(),\n",
    "                \"results_data\": []\n",
    "            }\n",
    "            insert_log_entry(log_entry)\n",
    "            # Construct the new target path correctly for the subdirectory\n",
    "            new_target_path = os.path.join(target_path, file_info.name)\n",
    "            sync_volume_recursively(file_info.path, new_target_path, prev_run_info)\n",
    "        else:\n",
    "            # It's a file. Check if it needs to be copied.\n",
    "            relative_path = file_info.path.replace(source_path, '', 1).lstrip('/')\n",
    "            \n",
    "            # Compare modification times.\n",
    "            if relative_path not in prev_run_info or file_info.modificationTime > prev_run_info[relative_path]:\n",
    "                try:\n",
    "                    # Attempt to copy the file.\n",
    "                    # The dbutils.fs.cp command copies the source file into the destination directory.\n",
    "                    dbutils.fs.cp(file_info.path, target_path, recurse=True)\n",
    "                    \n",
    "                    # Log successful copy with file modification time.\n",
    "                    results_data = [\n",
    "                        {\"key\": \"result\", \"value\": \"True\"},\n",
    "                        {\"key\": \"modificationTime\", \"value\": str(file_info.modificationTime)},\n",
    "                        {\"key\": \"fileSize\", \"value\": str(file_info.size)},\n",
    "                        {\"key\": \"source_path\", \"value\": file_info.path},\n",
    "                        {\"key\": \"target_path\", \"value\": target_path}\n",
    "                    ]\n",
    "                    log_entry = {\n",
    "                        \"entity_type\": \"file\",\n",
    "                        \"entity_name\": file_info.path,\n",
    "                        \"action\": \"copy\",\n",
    "                        \"status\": \"success\",\n",
    "                        \"message\": \"File copied successfully.\",\n",
    "                        \"timestamp\": datetime.now(),\n",
    "                        \"results_data\": results_data\n",
    "                    }\n",
    "                    insert_log_entry(log_entry)\n",
    "                except Exception as e:\n",
    "                    # Log a failure to copy the file.\n",
    "                    log_entry = {\n",
    "                        \"entity_type\": \"file\",\n",
    "                        \"entity_name\": file_info.path,\n",
    "                        \"action\": \"copy\",\n",
    "                        \"status\": \"error\",\n",
    "                        \"message\": f\"Error during file copy. Error: {e}\",\n",
    "                        \"timestamp\": datetime.now(),\n",
    "                        \"results_data\": []\n",
    "                    }\n",
    "                    insert_log_entry(log_entry)\n",
    "            else:\n",
    "                log_entry = {\n",
    "                    \"entity_type\": \"file\",\n",
    "                    \"entity_name\": file_info.path,\n",
    "                    \"action\": \"skip\",\n",
    "                    \"status\": \"success\",\n",
    "                    \"message\": \"File already exists with same or newer modification date. Skipping.\",\n",
    "                    \"timestamp\": datetime.now(),\n",
    "                    \"results_data\": []\n",
    "                }\n",
    "                insert_log_entry(log_entry)\n",
    "\n",
    "def execute_and_log_sql(sql_command, entity_type, entity_name, action, success_msg, error_msg):\n",
    "    \"\"\"\n",
    "    Executes a SQL command and logs the result.\n",
    "    \n",
    "    Args:\n",
    "        sql_command (str): The SQL command to execute.\n",
    "        entity_type (str): The type of entity being acted upon (e.g., 'catalog', 'schema').\n",
    "        entity_name (str): The name of the entity.\n",
    "        action (str): The action being performed (e.g., 'create').\n",
    "        success_msg (str): Message to log on success.\n",
    "        error_msg (str): Message to log on error.\n",
    "    \"\"\"\n",
    "    spark_session = SparkSession.builder.getOrCreate()\n",
    "    try:\n",
    "        spark_session.sql(sql_command)\n",
    "        log_entry = {\n",
    "            \"entity_type\": entity_type,\n",
    "            \"entity_name\": entity_name,\n",
    "            \"action\": action,\n",
    "            \"status\": \"success\",\n",
    "            \"message\": success_msg,\n",
    "            \"timestamp\": datetime.now(),\n",
    "            \"results_data\": []\n",
    "        }\n",
    "        insert_log_entry(log_entry)\n",
    "    except Exception as e:\n",
    "        log_entry = {\n",
    "            \"entity_type\": entity_type,\n",
    "            \"entity_name\": entity_name,\n",
    "            \"action\": action,\n",
    "            \"status\": \"error\",\n",
    "            \"message\": f\"{error_msg} Error: {e}\",\n",
    "            \"timestamp\": datetime.now(),\n",
    "            \"results_data\": []\n",
    "        }\n",
    "        insert_log_entry(log_entry)\n",
    "\n",
    "\n",
    "def process_volume_sync(source_catalog, source_schema, target_catalog, target_schema, volume_name):\n",
    "    \"\"\"\n",
    "    Processes the synchronization for a single volume.\n",
    "    \n",
    "    Args:\n",
    "        source_catalog (str): The source catalog name.\n",
    "        source_schema (str): The source schema name.\n",
    "        target_catalog (str): The target catalog name.\n",
    "        target_schema (str): The target schema name.\n",
    "        volume_name (str): The name of the volume to sync.\n",
    "    \"\"\"\n",
    "    print(f\"Starting sync for volume: {source_catalog}.{source_schema}.{volume_name}\")\n",
    "    spark_session = SparkSession.builder.getOrCreate()\n",
    "    try:\n",
    "        # 1. Create the target volume if it doesn't exist.\n",
    "        sql_command = f\"CREATE VOLUME IF NOT EXISTS `{target_catalog}`.`{target_schema}`.`{volume_name}`\"\n",
    "        try:\n",
    "            spark_session.sql(sql_command)\n",
    "            log_entry = {\n",
    "                \"entity_type\": \"volume\",\n",
    "                \"entity_name\": f\"{target_catalog}.{target_schema}.{volume_name}\",\n",
    "                \"action\": \"create\",\n",
    "                \"status\": \"success\",\n",
    "                \"message\": \"Volume created or already exists.\",\n",
    "                \"timestamp\": datetime.now(),\n",
    "                \"results_data\": []\n",
    "            }\n",
    "            insert_log_entry(log_entry)\n",
    "        except Exception as e:\n",
    "            log_entry = {\n",
    "                \"entity_type\": \"volume\",\n",
    "                \"entity_name\": f\"{target_catalog}.{target_schema}.{volume_name}\",\n",
    "                \"action\": \"create\",\n",
    "                \"status\": \"error\",\n",
    "                \"message\": f\"Failed to create volume. Error: {e}\",\n",
    "                \"timestamp\": datetime.now(),\n",
    "                \"results_data\": []\n",
    "            }\n",
    "            insert_log_entry(log_entry)\n",
    "            return\n",
    "\n",
    "        # 2. Construct source and target paths for file system operations.\n",
    "        source_path = os.path.join(\"/Volumes\", source_catalog, source_schema, volume_name)\n",
    "        target_path = os.path.join(\"/Volumes\", target_catalog, target_schema, volume_name)\n",
    "        \n",
    "        # 3. Get modification dates from previous runs to enable efficient sync.\n",
    "        prev_run_info = get_previous_run_info(source_path)\n",
    "\n",
    "        # 4. Start the recursive file sync.\n",
    "        sync_volume_recursively(source_path, target_path, prev_run_info)\n",
    "        \n",
    "    except Exception as e:\n",
    "        log_entry = {\n",
    "            \"entity_type\": \"volume_sync\",\n",
    "            \"entity_name\": f\"{source_catalog}.{source_schema}.{volume_name}\",\n",
    "            \"action\": \"sync\",\n",
    "            \"status\": \"error\",\n",
    "            \"message\": f\"An unexpected error occurred during volume sync. Error: {e}\",\n",
    "            \"timestamp\": datetime.now(),\n",
    "            \"results_data\": []\n",
    "        }\n",
    "        insert_log_entry(log_entry)\n",
    "\n",
    "# ==============================================================================\n",
    "# MAIN EXECUTION LOGIC\n",
    "# ==============================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "    spark.sql(\"SET spark.sql.catalog.spark_catalog = 'spark_catalog'\")\n",
    "    \n",
    "    print(\"Starting Databricks Volume DR Sync.\")\n",
    "    \n",
    "    log_entry = {\n",
    "        \"entity_type\": \"info\",\n",
    "        \"entity_name\": \"N/A\",\n",
    "        \"action\": \"start_sync\",\n",
    "        \"status\": \"success\",\n",
    "        \"message\": \"Starting Databricks Volume DR Sync.\",\n",
    "        \"timestamp\": datetime.now(),\n",
    "        \"results_data\": []\n",
    "    }\n",
    "    insert_log_entry(log_entry)\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = []\n",
    "\n",
    "        for catalog_pair in catalog_pair_names_list:\n",
    "            try:\n",
    "                source_catalog, target_catalog = catalog_pair.split(':')\n",
    "            except ValueError:\n",
    "                log_entry = {\n",
    "                    \"entity_type\": \"info\",\n",
    "                    \"entity_name\": catalog_pair,\n",
    "                    \"action\": \"parse_catalog_pair\",\n",
    "                    \"status\": \"error\",\n",
    "                    \"message\": f\"Invalid catalog pair format: {catalog_pair}. Skipping.\",\n",
    "                    \"timestamp\": datetime.now(),\n",
    "                    \"results_data\": []\n",
    "                }\n",
    "                insert_log_entry(log_entry)\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                # 1. Create target catalog if it doesn't exist.\n",
    "                execute_and_log_sql(\n",
    "                    f\"CREATE CATALOG IF NOT EXISTS `{target_catalog}`\",\n",
    "                    \"catalog\",\n",
    "                    target_catalog,\n",
    "                    \"create\",\n",
    "                    f\"Catalog `{target_catalog}` created or already exists.\",\n",
    "                    f\"Failed to create catalog `{target_catalog}`.\"\n",
    "                )\n",
    "                \n",
    "                # 2. Get list of schemas in the source catalog.\n",
    "                schemas_df = spark.sql(f\"SHOW SCHEMAS IN `{source_catalog}`\")\n",
    "                schemas_to_process = [row['databaseName'] for row in schemas_df.collect()]\n",
    "                \n",
    "                for source_schema in schemas_to_process:\n",
    "                    # 3. Ignore 'information_schema'.\n",
    "                    if source_schema.lower() == 'information_schema':\n",
    "                        log_entry = {\n",
    "                            \"entity_type\": \"schema\",\n",
    "                            \"entity_name\": f\"{source_catalog}.{source_schema}\",\n",
    "                            \"action\": \"skip\",\n",
    "                            \"status\": \"success\",\n",
    "                            \"message\": \"Skipping 'information_schema'.\",\n",
    "                            \"timestamp\": datetime.now(),\n",
    "                            \"results_data\": []\n",
    "                        }\n",
    "                        insert_log_entry(log_entry)\n",
    "                        continue\n",
    "\n",
    "                    try:\n",
    "                        # 4. Create target schema if it doesn't exist.\n",
    "                        execute_and_log_sql(\n",
    "                            f\"CREATE SCHEMA IF NOT EXISTS `{target_catalog}`.`{source_schema}`\",\n",
    "                            \"schema\",\n",
    "                            f\"{target_catalog}.{source_schema}\",\n",
    "                            \"create\",\n",
    "                            f\"Schema `{source_schema}` in catalog `{target_catalog}` created or already exists.\",\n",
    "                            f\"Failed to create schema `{source_schema}`.\"\n",
    "                        )\n",
    "                        \n",
    "                        # 5. Get list of volumes in the source schema.\n",
    "                        volumes_df = spark.sql(f\"SHOW VOLUMES IN `{source_catalog}`.`{source_schema}`\")\n",
    "                        volumes_to_process = [row['volume_name'] for row in volumes_df.collect()]\n",
    "                        \n",
    "                        for volume_name in volumes_to_process:\n",
    "                            # 6. Submit volume sync tasks to the thread pool.\n",
    "                            future = executor.submit(\n",
    "                                process_volume_sync,\n",
    "                                source_catalog,\n",
    "                                source_schema,\n",
    "                                target_catalog,\n",
    "                                source_schema, # Use source_schema as target_schema for simplicity\n",
    "                                volume_name\n",
    "                            )\n",
    "                            futures.append(future)\n",
    "\n",
    "                    except Exception as e:\n",
    "                        log_entry = {\n",
    "                            \"entity_type\": \"schema\",\n",
    "                            \"entity_name\": f\"{source_catalog}.{source_schema}\",\n",
    "                            \"action\": \"process\",\n",
    "                            \"status\": \"error\",\n",
    "                            \"message\": f\"An error occurred while processing schema. Error: {e}\",\n",
    "                            \"timestamp\": datetime.now(),\n",
    "                            \"results_data\": []\n",
    "                        }\n",
    "                        insert_log_entry(log_entry)\n",
    "\n",
    "            except Exception as e:\n",
    "                log_entry = {\n",
    "                    \"entity_type\": \"catalog\",\n",
    "                    \"entity_name\": source_catalog,\n",
    "                    \"action\": \"process\",\n",
    "                    \"status\": \"error\",\n",
    "                    \"message\": f\"An error occurred while processing catalog pair. Error: {e}\",\n",
    "                    \"timestamp\": datetime.now(),\n",
    "                    \"results_data\": []\n",
    "                }\n",
    "                insert_log_entry(log_entry)\n",
    "\n",
    "        # Wait for all tasks to complete and retrieve results.\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            try:\n",
    "                future.result()\n",
    "            except Exception as e:\n",
    "                log_entry = {\n",
    "                    \"entity_type\": \"info\",\n",
    "                    \"entity_name\": \"N/A\",\n",
    "                    \"action\": \"future_result\",\n",
    "                    \"status\": \"error\",\n",
    "                    \"message\": f\"An unexpected error occurred in a thread. Error: {e}\",\n",
    "                    \"timestamp\": datetime.now(),\n",
    "                    \"results_data\": []\n",
    "                }\n",
    "                insert_log_entry(log_entry)\n",
    "                \n",
    "    print(\"Databricks Volume DR Sync completed.\")\n",
    "\n",
    "    log_entry = {\n",
    "        \"entity_type\": \"info\",\n",
    "        \"entity_name\": \"N/A\",\n",
    "        \"action\": \"end_sync\",\n",
    "        \"status\": \"success\",\n",
    "        \"message\": \"Databricks Volume DR Sync completed.\",\n",
    "        \"timestamp\": datetime.now(),\n",
    "        \"results_data\": []\n",
    "    }\n",
    "    insert_log_entry(log_entry)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f309a787-ad33-4ff8-9a97-25ea41c8bc72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {
        "catalog_names": "_eo_amperity:dr_eo_amperity, accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample:dr_accuweather, test_not:dr_test_not, satya_share:satya_dr_catalog",
        "log_table_name": "users.satyendranath_sure.dr_log_table_name",
        "max_workers": "5",
        "target_schema_owner": "dr_eo_amperity.bronze:grp_test_clt_usa, dr_eo_amperity.silver:grp_test_clt_usa, dr_eo_amperity.default:grp_test_clt_usa"
       },
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# spark.sql(f\"\"\"\n",
    "#     DELETE\n",
    "#     FROM {log_table_name}\n",
    "#     WHERE notebook_name like 'DR_Volume_data%'\n",
    "# \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99752649-dcbe-4fff-b1d7-de708a0652a8",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"entity_name\":574},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1758226483849}",
       "filterBlob": "{\"version\":1,\"filterGroups\":[{\"enabled\":true,\"filterGroupId\":\"fg_85bac5a2\",\"op\":\"OR\",\"filters\":[{\"filterId\":\"f_4f77b071\",\"enabled\":true,\"columnId\":\"notebook_name\",\"dataType\":\"string\",\"filterType\":\"oneof\",\"filterValues\":[\"DR_Volume_data_temp\"],\"filterConfig\":{\"caseSensitive\":true}}],\"local\":false,\"updatedAt\":1758228797534},{\"enabled\":true,\"filterGroupId\":\"fg_1980b4af\",\"op\":\"OR\",\"filters\":[{\"filterId\":\"f_9c333a1c\",\"enabled\":true,\"columnId\":\"entity_name\",\"dataType\":\"string\",\"filterType\":\"contains\",\"filterValue\":\"satya\",\"filterConfig\":{}}],\"local\":false,\"updatedAt\":1758228836206}],\"syncTimestamp\":1758228836206}",
       "queryPlanFiltersBlob": "[{\"kind\":\"call\",\"function\":\"and\",\"args\":[{\"kind\":\"call\",\"function\":\"or\",\"args\":[{\"kind\":\"call\",\"function\":\"in\",\"args\":[{\"kind\":\"identifier\",\"identifier\":\"notebook_name\"},{\"kind\":\"literal\",\"value\":\"DR_Volume_data_temp\",\"type\":\"string\"}]}]},{\"kind\":\"call\",\"function\":\"or\",\"args\":[{\"kind\":\"call\",\"function\":\"ilike\",\"args\":[{\"kind\":\"identifier\",\"identifier\":\"entity_name\"},{\"kind\":\"literal\",\"value\":\"%satya%\",\"type\":\"string\"}]}]}]}]",
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n--- Displaying logs from users.satyendranath_sure.dr_log_table_name for the last 24 hours ---\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>notebook_name</th><th>entity_type</th><th>entity_name</th><th>action</th><th>status</th><th>message</th><th>timestamp</th><th>results_data</th></tr></thead><tbody><tr><td>DR_Volume_data_temp</td><td>info</td><td>N/A</td><td>end_sync</td><td>success</td><td>Databricks Volume DR Sync completed.</td><td>2025-09-18T21:20:03.37281Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/tiny.json</td><td>skip</td><td>success</td><td>File already exists with same or newer modification date. Skipping.</td><td>2025-09-18T21:20:02.469058Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch.txt</td><td>skip</td><td>success</td><td>File already exists with same or newer modification date. Skipping.</td><td>2025-09-18T21:20:01.555607Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-06-11_10-46.json</td><td>skip</td><td>success</td><td>File already exists with same or newer modification date. Skipping.</td><td>2025-09-18T21:20:00.692051Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-06-10_11-26.json</td><td>skip</td><td>success</td><td>File already exists with same or newer modification date. Skipping.</td><td>2025-09-18T21:19:59.799818Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-06-08_18-02.json</td><td>skip</td><td>success</td><td>File already exists with same or newer modification date. Skipping.</td><td>2025-09-18T21:19:58.983252Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-06-08_15-57.json</td><td>skip</td><td>success</td><td>File already exists with same or newer modification date. Skipping.</td><td>2025-09-18T21:19:58.034685Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-06-05_13-39.json</td><td>skip</td><td>success</td><td>File already exists with same or newer modification date. Skipping.</td><td>2025-09-18T21:19:57.36587Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-06-04_18-14.json</td><td>skip</td><td>success</td><td>File already exists with same or newer modification date. Skipping.</td><td>2025-09-18T21:19:56.391117Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-06-04_17-57.json</td><td>skip</td><td>success</td><td>File already exists with same or newer modification date. Skipping.</td><td>2025-09-18T21:19:55.536806Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-06-04_09-48.json</td><td>skip</td><td>success</td><td>File already exists with same or newer modification date. Skipping.</td><td>2025-09-18T21:19:53.629036Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-06-04_07-39.json</td><td>skip</td><td>success</td><td>File already exists with same or newer modification date. Skipping.</td><td>2025-09-18T21:19:52.762565Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-27_11-32.json</td><td>skip</td><td>success</td><td>File already exists with same or newer modification date. Skipping.</td><td>2025-09-18T21:19:51.936918Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-27_11-31.json</td><td>skip</td><td>success</td><td>File already exists with same or newer modification date. Skipping.</td><td>2025-09-18T21:19:51.125157Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-27_09-44.json</td><td>skip</td><td>success</td><td>File already exists with same or newer modification date. Skipping.</td><td>2025-09-18T21:19:50.288529Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-27_09-43.json</td><td>skip</td><td>success</td><td>File already exists with same or newer modification date. Skipping.</td><td>2025-09-18T21:19:49.467932Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-22_14-06.json</td><td>skip</td><td>success</td><td>File already exists with same or newer modification date. Skipping.</td><td>2025-09-18T21:19:48.651115Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-22_07-41.json</td><td>skip</td><td>success</td><td>File already exists with same or newer modification date. Skipping.</td><td>2025-09-18T21:19:47.820868Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-22_06-42.json</td><td>skip</td><td>success</td><td>File already exists with same or newer modification date. Skipping.</td><td>2025-09-18T21:19:47.025697Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-20_10-25.json</td><td>skip</td><td>success</td><td>File already exists with same or newer modification date. Skipping.</td><td>2025-09-18T21:19:46.20171Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-20_09-27.json</td><td>skip</td><td>success</td><td>File already exists with same or newer modification date. Skipping.</td><td>2025-09-18T21:19:45.397738Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-19_15-16.json</td><td>skip</td><td>success</td><td>File already exists with same or newer modification date. Skipping.</td><td>2025-09-18T21:19:44.572887Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-15_15-16.json</td><td>skip</td><td>success</td><td>File already exists with same or newer modification date. Skipping.</td><td>2025-09-18T21:19:43.747801Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-15_14-55.json</td><td>skip</td><td>success</td><td>File already exists with same or newer modification date. Skipping.</td><td>2025-09-18T21:19:42.925986Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-15_14-54.json</td><td>skip</td><td>success</td><td>File already exists with same or newer modification date. Skipping.</td><td>2025-09-18T21:19:42.135871Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-15_14-31.json</td><td>skip</td><td>success</td><td>File already exists with same or newer modification date. Skipping.</td><td>2025-09-18T21:19:41.323378Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-15_14-22.json</td><td>skip</td><td>success</td><td>File already exists with same or newer modification date. Skipping.</td><td>2025-09-18T21:19:40.502436Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-15_14-21.json</td><td>skip</td><td>success</td><td>File already exists with same or newer modification date. Skipping.</td><td>2025-09-18T21:19:39.698118Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-15_13-55.json</td><td>skip</td><td>success</td><td>File already exists with same or newer modification date. Skipping.</td><td>2025-09-18T21:19:38.894359Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-15_13-38.json</td><td>skip</td><td>success</td><td>File already exists with same or newer modification date. Skipping.</td><td>2025-09-18T21:19:38.102521Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-15_13-30.json</td><td>skip</td><td>success</td><td>File already exists with same or newer modification date. Skipping.</td><td>2025-09-18T21:19:37.279743Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-15_12-54.json</td><td>skip</td><td>success</td><td>File already exists with same or newer modification date. Skipping.</td><td>2025-09-18T21:19:36.473005Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-15_12-10.json</td><td>skip</td><td>success</td><td>File already exists with same or newer modification date. Skipping.</td><td>2025-09-18T21:19:35.665367Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-15_11-55.json</td><td>skip</td><td>success</td><td>File already exists with same or newer modification date. Skipping.</td><td>2025-09-18T21:19:34.844343Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-15_11-38.json</td><td>skip</td><td>success</td><td>File already exists with same or newer modification date. Skipping.</td><td>2025-09-18T21:19:34.058382Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-15_11-33.json</td><td>skip</td><td>success</td><td>File already exists with same or newer modification date. Skipping.</td><td>2025-09-18T21:19:33.253783Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-15_11-31.json</td><td>skip</td><td>success</td><td>File already exists with same or newer modification date. Skipping.</td><td>2025-09-18T21:19:32.475309Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-15_10-17.json</td><td>skip</td><td>success</td><td>File already exists with same or newer modification date. Skipping.</td><td>2025-09-18T21:19:31.655833Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-14_16-23.json</td><td>skip</td><td>success</td><td>File already exists with same or newer modification date. Skipping.</td><td>2025-09-18T21:19:30.731213Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-14_15-44.json</td><td>skip</td><td>success</td><td>File already exists with same or newer modification date. Skipping.</td><td>2025-09-18T21:19:30.081065Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-14_15-39.json</td><td>skip</td><td>success</td><td>File already exists with same or newer modification date. Skipping.</td><td>2025-09-18T21:19:29.266718Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-14_15-34.json</td><td>skip</td><td>success</td><td>File already exists with same or newer modification date. Skipping.</td><td>2025-09-18T21:19:28.515953Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-14_15-32.json</td><td>skip</td><td>success</td><td>File already exists with same or newer modification date. Skipping.</td><td>2025-09-18T21:19:27.792318Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-14_14-55.json</td><td>skip</td><td>success</td><td>File already exists with same or newer modification date. Skipping.</td><td>2025-09-18T21:19:26.98471Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-14_14-45.json</td><td>skip</td><td>success</td><td>File already exists with same or newer modification date. Skipping.</td><td>2025-09-18T21:19:26.144862Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-14_13-47.json</td><td>skip</td><td>success</td><td>File already exists with same or newer modification date. Skipping.</td><td>2025-09-18T21:19:25.356949Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-29_12-32.json</td><td>skip</td><td>success</td><td>File already exists with same or newer modification date. Skipping.</td><td>2025-09-18T21:19:24.116548Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_21-09.json</td><td>skip</td><td>success</td><td>File already exists with same or newer modification date. Skipping.</td><td>2025-09-18T21:19:23.21363Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_20-52.json</td><td>skip</td><td>success</td><td>File already exists with same or newer modification date. Skipping.</td><td>2025-09-18T21:19:22.457299Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_20-48.json</td><td>skip</td><td>success</td><td>File already exists with same or newer modification date. Skipping.</td><td>2025-09-18T21:19:21.630858Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_20-44.json</td><td>skip</td><td>success</td><td>File already exists with same or newer modification date. Skipping.</td><td>2025-09-18T21:19:20.817316Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_20-38.json</td><td>skip</td><td>success</td><td>File already exists with same or newer modification date. Skipping.</td><td>2025-09-18T21:19:20.002616Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_20-26.json</td><td>skip</td><td>success</td><td>File already exists with same or newer modification date. Skipping.</td><td>2025-09-18T21:19:19.196496Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_20-18.json</td><td>skip</td><td>success</td><td>File already exists with same or newer modification date. Skipping.</td><td>2025-09-18T21:19:18.389669Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_19-59.json</td><td>skip</td><td>success</td><td>File already exists with same or newer modification date. Skipping.</td><td>2025-09-18T21:19:17.584697Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_19-56.json</td><td>skip</td><td>success</td><td>File already exists with same or newer modification date. Skipping.</td><td>2025-09-18T21:19:16.784985Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_19-37.json</td><td>skip</td><td>success</td><td>File already exists with same or newer modification date. Skipping.</td><td>2025-09-18T21:19:15.956947Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_19-31.json</td><td>skip</td><td>success</td><td>File already exists with same or newer modification date. Skipping.</td><td>2025-09-18T21:19:15.097549Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_18-42.json</td><td>skip</td><td>success</td><td>File already exists with same or newer modification date. Skipping.</td><td>2025-09-18T21:19:14.298044Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_18-41.json</td><td>skip</td><td>success</td><td>File already exists with same or newer modification date. Skipping.</td><td>2025-09-18T21:19:13.493073Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_18-40.json</td><td>skip</td><td>success</td><td>File already exists with same or newer modification date. Skipping.</td><td>2025-09-18T21:19:12.688412Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_18-33.json</td><td>skip</td><td>success</td><td>File already exists with same or newer modification date. Skipping.</td><td>2025-09-18T21:19:11.856812Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_18-30.json</td><td>skip</td><td>success</td><td>File already exists with same or newer modification date. Skipping.</td><td>2025-09-18T21:19:11.023448Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_18-26.json</td><td>skip</td><td>success</td><td>File already exists with same or newer modification date. Skipping.</td><td>2025-09-18T21:19:10.219391Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_18-15.json</td><td>skip</td><td>success</td><td>File already exists with same or newer modification date. Skipping.</td><td>2025-09-18T21:19:09.422372Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_18-14.json</td><td>skip</td><td>success</td><td>File already exists with same or newer modification date. Skipping.</td><td>2025-09-18T21:19:08.646502Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_18-11.json</td><td>skip</td><td>success</td><td>File already exists with same or newer modification date. Skipping.</td><td>2025-09-18T21:19:07.890478Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_18-10.json</td><td>skip</td><td>success</td><td>File already exists with same or newer modification date. Skipping.</td><td>2025-09-18T21:19:07.006284Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_18-09.json</td><td>skip</td><td>success</td><td>File already exists with same or newer modification date. Skipping.</td><td>2025-09-18T21:19:06.226916Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_18-08.json</td><td>skip</td><td>success</td><td>File already exists with same or newer modification date. Skipping.</td><td>2025-09-18T21:19:05.481427Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_18-03.json</td><td>skip</td><td>success</td><td>File already exists with same or newer modification date. Skipping.</td><td>2025-09-18T21:19:03.852356Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_18-02.json</td><td>skip</td><td>success</td><td>File already exists with same or newer modification date. Skipping.</td><td>2025-09-18T21:19:03.11445Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_17-58.json</td><td>skip</td><td>success</td><td>File already exists with same or newer modification date. Skipping.</td><td>2025-09-18T21:19:02.137156Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_17-55.json</td><td>skip</td><td>success</td><td>File already exists with same or newer modification date. Skipping.</td><td>2025-09-18T21:19:01.326754Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_17-53.json</td><td>skip</td><td>success</td><td>File already exists with same or newer modification date. Skipping.</td><td>2025-09-18T21:19:00.574946Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_17-31.json</td><td>skip</td><td>success</td><td>File already exists with same or newer modification date. Skipping.</td><td>2025-09-18T21:18:59.803776Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_16-23.json</td><td>skip</td><td>success</td><td>File already exists with same or newer modification date. Skipping.</td><td>2025-09-18T21:18:59.025693Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_16-22.json</td><td>skip</td><td>success</td><td>File already exists with same or newer modification date. Skipping.</td><td>2025-09-18T21:18:58.256405Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_16-21.json</td><td>skip</td><td>success</td><td>File already exists with same or newer modification date. Skipping.</td><td>2025-09-18T21:18:57.516385Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_15-54.json</td><td>skip</td><td>success</td><td>File already exists with same or newer modification date. Skipping.</td><td>2025-09-18T21:18:56.733128Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_15-53.json</td><td>skip</td><td>success</td><td>File already exists with same or newer modification date. Skipping.</td><td>2025-09-18T21:18:55.837284Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_15-48.json</td><td>skip</td><td>success</td><td>File already exists with same or newer modification date. Skipping.</td><td>2025-09-18T21:18:55.046105Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_15-47.json</td><td>skip</td><td>success</td><td>File already exists with same or newer modification date. Skipping.</td><td>2025-09-18T21:18:54.233327Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_15-46.json</td><td>skip</td><td>success</td><td>File already exists with same or newer modification date. Skipping.</td><td>2025-09-18T21:18:53.411614Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_15-42.json</td><td>skip</td><td>success</td><td>File already exists with same or newer modification date. Skipping.</td><td>2025-09-18T21:18:52.653976Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_15-29.json</td><td>skip</td><td>success</td><td>File already exists with same or newer modification date. Skipping.</td><td>2025-09-18T21:18:51.914796Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_15-23.json</td><td>skip</td><td>success</td><td>File already exists with same or newer modification date. Skipping.</td><td>2025-09-18T21:18:51.037709Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_12-40.json</td><td>skip</td><td>success</td><td>File already exists with same or newer modification date. Skipping.</td><td>2025-09-18T21:18:50.241422Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_12-01.json</td><td>skip</td><td>success</td><td>File already exists with same or newer modification date. Skipping.</td><td>2025-09-18T21:18:49.453236Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_10-57.json</td><td>skip</td><td>success</td><td>File already exists with same or newer modification date. Skipping.</td><td>2025-09-18T21:18:48.64584Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-25_17-38.json</td><td>skip</td><td>success</td><td>File already exists with same or newer modification date. Skipping.</td><td>2025-09-18T21:18:47.852961Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-25_17-34.json</td><td>skip</td><td>success</td><td>File already exists with same or newer modification date. Skipping.</td><td>2025-09-18T21:18:47.045576Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-25_17-20.json</td><td>skip</td><td>success</td><td>File already exists with same or newer modification date. Skipping.</td><td>2025-09-18T21:18:46.244033Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-25_13-29.json</td><td>skip</td><td>success</td><td>File already exists with same or newer modification date. Skipping.</td><td>2025-09-18T21:18:45.452131Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-25_12-04.json</td><td>skip</td><td>success</td><td>File already exists with same or newer modification date. Skipping.</td><td>2025-09-18T21:18:44.649239Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-24_16-31.json</td><td>skip</td><td>success</td><td>File already exists with same or newer modification date. Skipping.</td><td>2025-09-18T21:18:43.039297Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-24_16-28.json</td><td>skip</td><td>success</td><td>File already exists with same or newer modification date. Skipping.</td><td>2025-09-18T21:18:39.644097Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>directory</td><td>dbfs:/Volumes/satya_share/satyendranath_sure/satya_volume/test/test1/</td><td>list</td><td>success</td><td>Found directory to traverse.</td><td>2025-09-18T21:18:39.247705Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-24_16-16.json</td><td>skip</td><td>success</td><td>File already exists with same or newer modification date. Skipping.</td><td>2025-09-18T21:18:38.184657Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/satya_share/satyendranath_sure/satya_volume/test/file_str11.0.parquet</td><td>copy</td><td>success</td><td>File copied successfully.</td><td>2025-09-18T21:18:37.953779Z</td><td>List(List(result, True), List(modificationTime, 1758221596000), List(fileSize, 6302661), List(source_path, dbfs:/Volumes/satya_share/satyendranath_sure/satya_volume/test/file_str11.0.parquet), List(target_path, /Volumes/satya_dr_catalog/satyendranath_sure/satya_volume/test/))</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-24_15-46.json</td><td>skip</td><td>success</td><td>File already exists with same or newer modification date. Skipping.</td><td>2025-09-18T21:18:37.525047Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-24_15-42.json</td><td>skip</td><td>success</td><td>File already exists with same or newer modification date. Skipping.</td><td>2025-09-18T21:18:36.660668Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-24_15-28.txt</td><td>skip</td><td>success</td><td>File already exists with same or newer modification date. Skipping.</td><td>2025-09-18T21:18:35.601026Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>directory</td><td>dbfs:/Volumes/satya_share/satyendranath_sure/satya_volume/test/</td><td>list</td><td>success</td><td>Found directory to traverse.</td><td>2025-09-18T21:18:34.69322Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-24_15-23.txt</td><td>skip</td><td>success</td><td>File already exists with same or newer modification date. Skipping.</td><td>2025-09-18T21:18:34.125829Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/satya_share/satyendranath_sure/satya_volume/file_str13.0.parquet</td><td>skip</td><td>success</td><td>File already exists with same or newer modification date. Skipping.</td><td>2025-09-18T21:18:33.793662Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-24_15-14.txt</td><td>skip</td><td>success</td><td>File already exists with same or newer modification date. Skipping.</td><td>2025-09-18T21:18:33.082249Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/satya_share/satyendranath_sure/satya_volume/file_str11.0.parquet</td><td>skip</td><td>success</td><td>File already exists with same or newer modification date. Skipping.</td><td>2025-09-18T21:18:32.704189Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/satya_share/satyendranath_sure/satya_external_volume/file_str14.0.parquet</td><td>skip</td><td>success</td><td>File already exists with same or newer modification date. Skipping.</td><td>2025-09-18T21:18:32.36678Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-24_15-12.json</td><td>skip</td><td>success</td><td>File already exists with same or newer modification date. Skipping.</td><td>2025-09-18T21:18:32.01537Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/satya_share/satyendranath_sure/satya_volume/Group_access_revoke.csv</td><td>skip</td><td>success</td><td>File already exists with same or newer modification date. Skipping.</td><td>2025-09-18T21:18:31.136748Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/joe-stitch-2025-05-29_10-51.json</td><td>skip</td><td>success</td><td>File already exists with same or newer modification date. Skipping.</td><td>2025-09-18T21:18:30.722164Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/satya_share/satyendranath_sure/satya_external_volume/file_str13.0.parquet</td><td>skip</td><td>success</td><td>File already exists with same or newer modification date. Skipping.</td><td>2025-09-18T21:18:30.418506Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/file.txt</td><td>skip</td><td>success</td><td>File already exists with same or newer modification date. Skipping.</td><td>2025-09-18T21:18:29.445365Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/satya_share/satyendranath_sure/satya_volume/DR FY26 SOP.pdf</td><td>skip</td><td>success</td><td>File already exists with same or newer modification date. Skipping.</td><td>2025-09-18T21:18:28.989427Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/satya_share/satyendranath_sure/satya_external_volume/file_str11.0.parquet</td><td>skip</td><td>success</td><td>File already exists with same or newer modification date. Skipping.</td><td>2025-09-18T21:18:28.959274Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/democracy-manifest.json</td><td>skip</td><td>success</td><td>File already exists with same or newer modification date. Skipping.</td><td>2025-09-18T21:18:28.553817Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/cluster_init.sh</td><td>skip</td><td>success</td><td>File already exists with same or newer modification date. Skipping.</td><td>2025-09-18T21:18:26.893449Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/cluster_init-2025-06-11_10-46.sh</td><td>skip</td><td>success</td><td>File already exists with same or newer modification date. Skipping.</td><td>2025-09-18T21:18:25.569206Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>volume</td><td>satya_dr_catalog.satyendranath_sure.satya_external_volume</td><td>create</td><td>success</td><td>Volume created or already exists.</td><td>2025-09-18T21:18:25.499743Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>volume</td><td>satya_dr_catalog.satyendranath_sure.satya_volume</td><td>create</td><td>success</td><td>Volume created or already exists.</td><td>2025-09-18T21:18:25.492391Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/cluster_init-2025-06-10_11-26.sh</td><td>skip</td><td>success</td><td>File already exists with same or newer modification date. Skipping.</td><td>2025-09-18T21:18:24.682934Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>schema</td><td>satya_dr_catalog.satyendranath_sure</td><td>create</td><td>success</td><td>Schema `satyendranath_sure` in catalog `satya_dr_catalog` created or already exists.</td><td>2025-09-18T21:18:23.674571Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/cluster_init-2025-06-08_18-02.sh</td><td>skip</td><td>success</td><td>File already exists with same or newer modification date. Skipping.</td><td>2025-09-18T21:18:22.990131Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>schema</td><td>satya_share.information_schema</td><td>skip</td><td>success</td><td>Skipping 'information_schema'.</td><td>2025-09-18T21:18:22.597177Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/cluster_init-2025-06-08_15-57.sh</td><td>skip</td><td>success</td><td>File already exists with same or newer modification date. Skipping.</td><td>2025-09-18T21:18:22.172986Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/cluster_init-2025-06-05_13-39.sh</td><td>skip</td><td>success</td><td>File already exists with same or newer modification date. Skipping.</td><td>2025-09-18T21:18:21.263503Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>catalog</td><td>satya_dr_catalog</td><td>create</td><td>success</td><td>Catalog `satya_dr_catalog` created or already exists.</td><td>2025-09-18T21:18:21.058016Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/cluster_init-2025-06-04_18-14.sh</td><td>skip</td><td>success</td><td>File already exists with same or newer modification date. Skipping.</td><td>2025-09-18T21:18:20.102671Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>catalog</td><td>test_not</td><td>process</td><td>error</td><td>An error occurred while processing catalog pair. Error: [NO_SUCH_CATALOG_EXCEPTION] Catalog 'test_not' was not found. Please verify the catalog name and then retry the query or command again. SQLSTATE: 42704\n",
       "\n",
       "JVM stacktrace:\n",
       "org.apache.spark.sql.catalyst.analysis.NoSuchCatalogException\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$listSchemasProto$1(ManagedCatalogClientImpl.scala:1249)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$recordAndWrapExceptionBase$2(ManagedCatalogClientImpl.scala:7501)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:113)\n",
       "\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:159)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:104)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$recordAndWrapExceptionBase$1(ManagedCatalogClientImpl.scala:7500)\n",
       "\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:74)\n",
       "\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:66)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:268)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapExceptionBase(ManagedCatalogClientImpl.scala:7481)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:7467)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.listSchemasProto(ManagedCatalogClientImpl.scala:1215)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.listSchemas(ManagedCatalogClientImpl.scala:1201)\n",
       "\tat com.databricks.sql.managedcatalog.ManagedCatalogCommon.listSchemas(ManagedCatalogCommon.scala:824)\n",
       "\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.$anonfun$listSchemas$1(ProfiledManagedCatalog.scala:173)\n",
       "\tat org.apache.spark.sql.catalyst.MetricKeyUtils$.measure(MetricKey.scala:1836)\n",
       "\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.$anonfun$profile$1(ProfiledManagedCatalog.scala:64)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:113)\n",
       "\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:159)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:104)\n",
       "\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.profile(ProfiledManagedCatalog.scala:63)\n",
       "\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.listSchemas(ProfiledManagedCatalog.scala:173)\n",
       "\tat com.databricks.sql.managedcatalog.ManagedCatalogSessionCatalog.listDatabasesWithCatalog(ManagedCatalogSessionCatalog.scala:928)\n",
       "\tat com.databricks.sql.managedcatalog.UnityCatalogV2Proxy.listNamespaces(UnityCatalogV2Proxy.scala:141)\n",
       "\tat org.apache.spark.sql.execution.command.ShowNamespacesCommand.run(ShowNamespacesCommand.scala:44)\n",
       "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$2(commands.scala:87)\n",
       "\tat org.apache.spark.sql.execution.SparkPlan.runCommandInAetherOrSpark(SparkPlan.scala:195)\n",
       "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$1(commands.scala:87)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:113)\n",
       "\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:159)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:104)\n",
       "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:84)\n",
       "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:83)\n",
       "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:97)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$5(QueryExecution.scala:507)\n",
       "\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$4(QueryExecution.scala:507)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:253)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$3(QueryExecution.scala:506)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$13(SQLExecution.scala:536)\n",
       "\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$12(SQLExecution.scala:458)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:834)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$11(SQLExecution.scala:386)\n",
       "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n",
       "\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:121)\n",
       "\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:115)\n",
       "\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:120)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$10(SQLExecution.scala:386)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:863)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:385)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:238)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:787)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:502)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1449)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:498)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.withMVTagsIfNecessary(QueryExecution.scala:418)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:496)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:578)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:570)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:529)\n",
       "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:529)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:42)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:361)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:357)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:505)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$8(QueryExecution.scala:570)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:570)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:374)\n",
       "\tat scala.util.Try$.apply(Try.scala:217)\n",
       "\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1686)\n",
       "\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1747)\n",
       "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:75)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:379)\n",
       "\tat org.apache.spark.sql.classic.Dataset.<init>(Dataset.scala:432)\n",
       "\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$3(Dataset.scala:155)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n",
       "\tat org.apache.spark.sql.classic.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1071)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:113)\n",
       "\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:159)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:104)\n",
       "\tat org.apache.spark.sql.classic.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1071)\n",
       "\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:146)\n",
       "\tat org.apache.spark.sql.classic.SparkSession.$anonfun$sql$4(SparkSession.scala:850)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n",
       "\tat org.apache.spark.sql.classic.SparkSession.sql(SparkSession.scala:813)\n",
       "\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.executeSQL(SparkConnectPlanner.scala:3500)\n",
       "\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleSqlCommand(SparkConnectPlanner.scala:3330)\n",
       "\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3207)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:385)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:282)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:238)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:466)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:466)\n",
       "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n",
       "\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:121)\n",
       "\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:115)\n",
       "\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:120)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:465)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:238)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$execute$1(ExecuteThreadRunner.scala:141)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
       "\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries(UtilizationMetrics.scala:43)\n",
       "\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries$(UtilizationMetrics.scala:40)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.recordActiveQueries(ExecuteThreadRunner.scala:53)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:139)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:586)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
       "\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n",
       "\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n",
       "\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n",
       "\tat scala.util.Using$.resource(Using.scala:296)\n",
       "\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:586)</td><td>2025-09-18T21:18:19.607918Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/cluster_init-2025-06-04_17-57.sh</td><td>skip</td><td>success</td><td>File already exists with same or newer modification date. Skipping.</td><td>2025-09-18T21:18:18.485502Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>catalog</td><td>dr_test_not</td><td>create</td><td>success</td><td>Catalog `dr_test_not` created or already exists.</td><td>2025-09-18T21:18:17.924352Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/cluster_init-2025-06-04_09-48.sh</td><td>skip</td><td>success</td><td>File already exists with same or newer modification date. Skipping.</td><td>2025-09-18T21:18:16.759192Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>schema</td><td>accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample.information_schema</td><td>skip</td><td>success</td><td>Skipping 'information_schema'.</td><td>2025-09-18T21:18:16.021792Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/cluster_init-2025-06-04_07-39.sh</td><td>skip</td><td>success</td><td>File already exists with same or newer modification date. Skipping.</td><td>2025-09-18T21:18:15.028374Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/cluster_init-2025-06-02_20-37.sh</td><td>skip</td><td>success</td><td>File already exists with same or newer modification date. Skipping.</td><td>2025-09-18T21:18:14.077744Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>schema</td><td>dr_accuweather.forecast</td><td>create</td><td>success</td><td>Schema `forecast` in catalog `dr_accuweather` created or already exists.</td><td>2025-09-18T21:18:13.49173Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/chinese-meal.json</td><td>skip</td><td>success</td><td>File already exists with same or newer modification date. Skipping.</td><td>2025-09-18T21:18:12.317919Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>catalog</td><td>dr_accuweather</td><td>create</td><td>success</td><td>Catalog `dr_accuweather` created or already exists.</td><td>2025-09-18T21:18:11.674935Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/acme_w_fks.json</td><td>skip</td><td>success</td><td>File already exists with same or newer modification date. Skipping.</td><td>2025-09-18T21:18:11.45632Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/acme.json</td><td>skip</td><td>success</td><td>File already exists with same or newer modification date. Skipping.</td><td>2025-09-18T21:18:10.525822Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>schema</td><td>dr_eo_amperity.silver</td><td>create</td><td>success</td><td>Schema `silver` in catalog `dr_eo_amperity` created or already exists.</td><td>2025-09-18T21:18:10.347031Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>schema</td><td>_eo_amperity.information_schema</td><td>skip</td><td>success</td><td>Skipping 'information_schema'.</td><td>2025-09-18T21:18:09.313905Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>schema</td><td>dr_eo_amperity.default</td><td>create</td><td>success</td><td>Schema `default` in catalog `dr_eo_amperity` created or already exists.</td><td>2025-09-18T21:18:07.7766Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>volume</td><td>dr_eo_amperity.bronze.chuck</td><td>create</td><td>success</td><td>Volume created or already exists.</td><td>2025-09-18T21:18:07.763097Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>schema</td><td>dr_eo_amperity.bronze</td><td>create</td><td>success</td><td>Schema `bronze` in catalog `dr_eo_amperity` created or already exists.</td><td>2025-09-18T21:18:06.087835Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>catalog</td><td>dr_eo_amperity</td><td>create</td><td>success</td><td>Catalog `dr_eo_amperity` created or already exists.</td><td>2025-09-18T21:18:04.629207Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>info</td><td>N/A</td><td>start_sync</td><td>success</td><td>Starting Databricks Volume DR Sync.</td><td>2025-09-18T21:18:03.164996Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>info</td><td>N/A</td><td>end_sync</td><td>success</td><td>Databricks Volume DR Sync completed.</td><td>2025-09-18T21:16:47.807881Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/tiny.json</td><td>copy</td><td>success</td><td>File copied successfully.</td><td>2025-09-18T21:16:46.942367Z</td><td>List(List(result, True), List(modificationTime, 1745856608000), List(fileSize, 1914), List(source_path, dbfs:/Volumes/_eo_amperity/bronze/chuck/tiny.json), List(target_path, /Volumes/dr_eo_amperity/bronze/chuck))</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch.txt</td><td>copy</td><td>success</td><td>File copied successfully.</td><td>2025-09-18T21:16:45.861717Z</td><td>List(List(result, True), List(modificationTime, 1745534242000), List(fileSize, 17386), List(source_path, dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch.txt), List(target_path, /Volumes/dr_eo_amperity/bronze/chuck))</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-06-11_10-46.json</td><td>copy</td><td>success</td><td>File copied successfully.</td><td>2025-09-18T21:16:44.765383Z</td><td>List(List(result, True), List(modificationTime, 1749663990000), List(fileSize, 9824), List(source_path, dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-06-11_10-46.json), List(target_path, /Volumes/dr_eo_amperity/bronze/chuck))</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-06-10_11-26.json</td><td>copy</td><td>success</td><td>File copied successfully.</td><td>2025-09-18T21:16:43.561613Z</td><td>List(List(result, True), List(modificationTime, 1749580010000), List(fileSize, 9824), List(source_path, dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-06-10_11-26.json), List(target_path, /Volumes/dr_eo_amperity/bronze/chuck))</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-06-08_18-02.json</td><td>copy</td><td>success</td><td>File copied successfully.</td><td>2025-09-18T21:16:42.501977Z</td><td>List(List(result, True), List(modificationTime, 1749430986000), List(fileSize, 9824), List(source_path, dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-06-08_18-02.json), List(target_path, /Volumes/dr_eo_amperity/bronze/chuck))</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-06-08_15-57.json</td><td>copy</td><td>success</td><td>File copied successfully.</td><td>2025-09-18T21:16:41.422051Z</td><td>List(List(result, True), List(modificationTime, 1749423478000), List(fileSize, 9824), List(source_path, dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-06-08_15-57.json), List(target_path, /Volumes/dr_eo_amperity/bronze/chuck))</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-06-05_13-39.json</td><td>copy</td><td>success</td><td>File copied successfully.</td><td>2025-09-18T21:16:40.246613Z</td><td>List(List(result, True), List(modificationTime, 1749155980000), List(fileSize, 9886), List(source_path, dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-06-05_13-39.json), List(target_path, /Volumes/dr_eo_amperity/bronze/chuck))</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-06-04_18-14.json</td><td>copy</td><td>success</td><td>File copied successfully.</td><td>2025-09-18T21:16:39.19828Z</td><td>List(List(result, True), List(modificationTime, 1749086081000), List(fileSize, 9886), List(source_path, dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-06-04_18-14.json), List(target_path, /Volumes/dr_eo_amperity/bronze/chuck))</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-06-04_17-57.json</td><td>copy</td><td>success</td><td>File copied successfully.</td><td>2025-09-18T21:16:38.109866Z</td><td>List(List(result, True), List(modificationTime, 1749085079000), List(fileSize, 9886), List(source_path, dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-06-04_17-57.json), List(target_path, /Volumes/dr_eo_amperity/bronze/chuck))</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-06-04_09-48.json</td><td>copy</td><td>success</td><td>File copied successfully.</td><td>2025-09-18T21:16:37.027473Z</td><td>List(List(result, True), List(modificationTime, 1749055706000), List(fileSize, 9886), List(source_path, dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-06-04_09-48.json), List(target_path, /Volumes/dr_eo_amperity/bronze/chuck))</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-06-04_07-39.json</td><td>copy</td><td>success</td><td>File copied successfully.</td><td>2025-09-18T21:16:35.905054Z</td><td>List(List(result, True), List(modificationTime, 1749047992000), List(fileSize, 9886), List(source_path, dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-06-04_07-39.json), List(target_path, /Volumes/dr_eo_amperity/bronze/chuck))</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-27_11-32.json</td><td>copy</td><td>success</td><td>File copied successfully.</td><td>2025-09-18T21:16:34.797957Z</td><td>List(List(result, True), List(modificationTime, 1748370758000), List(fileSize, 16912), List(source_path, dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-27_11-32.json), List(target_path, /Volumes/dr_eo_amperity/bronze/chuck))</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-27_11-31.json</td><td>copy</td><td>success</td><td>File copied successfully.</td><td>2025-09-18T21:16:33.603589Z</td><td>List(List(result, True), List(modificationTime, 1748370708000), List(fileSize, 16912), List(source_path, dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-27_11-31.json), List(target_path, /Volumes/dr_eo_amperity/bronze/chuck))</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-27_09-44.json</td><td>copy</td><td>success</td><td>File copied successfully.</td><td>2025-09-18T21:16:32.465287Z</td><td>List(List(result, True), List(modificationTime, 1748364252000), List(fileSize, 16912), List(source_path, dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-27_09-44.json), List(target_path, /Volumes/dr_eo_amperity/bronze/chuck))</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-27_09-43.json</td><td>copy</td><td>success</td><td>File copied successfully.</td><td>2025-09-18T21:16:31.386784Z</td><td>List(List(result, True), List(modificationTime, 1748364211000), List(fileSize, 14286), List(source_path, dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-27_09-43.json), List(target_path, /Volumes/dr_eo_amperity/bronze/chuck))</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-22_14-06.json</td><td>copy</td><td>success</td><td>File copied successfully.</td><td>2025-09-18T21:16:30.283982Z</td><td>List(List(result, True), List(modificationTime, 1747937181000), List(fileSize, 16912), List(source_path, dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-22_14-06.json), List(target_path, /Volumes/dr_eo_amperity/bronze/chuck))</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-22_07-41.json</td><td>copy</td><td>success</td><td>File copied successfully.</td><td>2025-09-18T21:16:29.218639Z</td><td>List(List(result, True), List(modificationTime, 1747924876000), List(fileSize, 16912), List(source_path, dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-22_07-41.json), List(target_path, /Volumes/dr_eo_amperity/bronze/chuck))</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-22_06-42.json</td><td>copy</td><td>success</td><td>File copied successfully.</td><td>2025-09-18T21:16:27.997066Z</td><td>List(List(result, True), List(modificationTime, 1747910546000), List(fileSize, 16912), List(source_path, dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-22_06-42.json), List(target_path, /Volumes/dr_eo_amperity/bronze/chuck))</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-20_10-25.json</td><td>copy</td><td>success</td><td>File copied successfully.</td><td>2025-09-18T21:16:26.967954Z</td><td>List(List(result, True), List(modificationTime, 1747761914000), List(fileSize, 17017), List(source_path, dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-20_10-25.json), List(target_path, /Volumes/dr_eo_amperity/bronze/chuck))</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-20_09-27.json</td><td>copy</td><td>success</td><td>File copied successfully.</td><td>2025-09-18T21:16:25.908581Z</td><td>List(List(result, True), List(modificationTime, 1747758474000), List(fileSize, 16947), List(source_path, dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-20_09-27.json), List(target_path, /Volumes/dr_eo_amperity/bronze/chuck))</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-19_15-16.json</td><td>copy</td><td>success</td><td>File copied successfully.</td><td>2025-09-18T21:16:24.54968Z</td><td>List(List(result, True), List(modificationTime, 1747693019000), List(fileSize, 16947), List(source_path, dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-19_15-16.json), List(target_path, /Volumes/dr_eo_amperity/bronze/chuck))</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-15_15-16.json</td><td>copy</td><td>success</td><td>File copied successfully.</td><td>2025-09-18T21:16:23.362235Z</td><td>List(List(result, True), List(modificationTime, 1747347394000), List(fileSize, 16797), List(source_path, dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-15_15-16.json), List(target_path, /Volumes/dr_eo_amperity/bronze/chuck))</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-15_14-55.json</td><td>copy</td><td>success</td><td>File copied successfully.</td><td>2025-09-18T21:16:22.28536Z</td><td>List(List(result, True), List(modificationTime, 1747346136000), List(fileSize, 16797), List(source_path, dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-15_14-55.json), List(target_path, /Volumes/dr_eo_amperity/bronze/chuck))</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-15_14-54.json</td><td>copy</td><td>success</td><td>File copied successfully.</td><td>2025-09-18T21:16:21.226973Z</td><td>List(List(result, True), List(modificationTime, 1747346089000), List(fileSize, 16797), List(source_path, dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-15_14-54.json), List(target_path, /Volumes/dr_eo_amperity/bronze/chuck))</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-15_14-31.json</td><td>copy</td><td>success</td><td>File copied successfully.</td><td>2025-09-18T21:16:20.153336Z</td><td>List(List(result, True), List(modificationTime, 1747344689000), List(fileSize, 16797), List(source_path, dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-15_14-31.json), List(target_path, /Volumes/dr_eo_amperity/bronze/chuck))</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-15_14-22.json</td><td>copy</td><td>success</td><td>File copied successfully.</td><td>2025-09-18T21:16:19.086972Z</td><td>List(List(result, True), List(modificationTime, 1747344157000), List(fileSize, 16797), List(source_path, dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-15_14-22.json), List(target_path, /Volumes/dr_eo_amperity/bronze/chuck))</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-15_14-21.json</td><td>copy</td><td>success</td><td>File copied successfully.</td><td>2025-09-18T21:16:17.940318Z</td><td>List(List(result, True), List(modificationTime, 1747344066000), List(fileSize, 16797), List(source_path, dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-15_14-21.json), List(target_path, /Volumes/dr_eo_amperity/bronze/chuck))</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-15_13-55.json</td><td>copy</td><td>success</td><td>File copied successfully.</td><td>2025-09-18T21:16:16.833833Z</td><td>List(List(result, True), List(modificationTime, 1747342552000), List(fileSize, 16797), List(source_path, dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-15_13-55.json), List(target_path, /Volumes/dr_eo_amperity/bronze/chuck))</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-15_13-38.json</td><td>copy</td><td>success</td><td>File copied successfully.</td><td>2025-09-18T21:16:15.708205Z</td><td>List(List(result, True), List(modificationTime, 1747341490000), List(fileSize, 16797), List(source_path, dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-15_13-38.json), List(target_path, /Volumes/dr_eo_amperity/bronze/chuck))</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-15_13-30.json</td><td>copy</td><td>success</td><td>File copied successfully.</td><td>2025-09-18T21:16:14.606029Z</td><td>List(List(result, True), List(modificationTime, 1747341053000), List(fileSize, 16797), List(source_path, dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-15_13-30.json), List(target_path, /Volumes/dr_eo_amperity/bronze/chuck))</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-15_12-54.json</td><td>copy</td><td>success</td><td>File copied successfully.</td><td>2025-09-18T21:16:13.526254Z</td><td>List(List(result, True), List(modificationTime, 1747338882000), List(fileSize, 16797), List(source_path, dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-15_12-54.json), List(target_path, /Volumes/dr_eo_amperity/bronze/chuck))</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-15_12-10.json</td><td>copy</td><td>success</td><td>File copied successfully.</td><td>2025-09-18T21:16:12.372196Z</td><td>List(List(result, True), List(modificationTime, 1747336203000), List(fileSize, 16902), List(source_path, dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-15_12-10.json), List(target_path, /Volumes/dr_eo_amperity/bronze/chuck))</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-15_11-55.json</td><td>copy</td><td>success</td><td>File copied successfully.</td><td>2025-09-18T21:16:11.289145Z</td><td>List(List(result, True), List(modificationTime, 1747335356000), List(fileSize, 16902), List(source_path, dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-15_11-55.json), List(target_path, /Volumes/dr_eo_amperity/bronze/chuck))</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-15_11-38.json</td><td>copy</td><td>success</td><td>File copied successfully.</td><td>2025-09-18T21:16:10.204977Z</td><td>List(List(result, True), List(modificationTime, 1747334313000), List(fileSize, 16902), List(source_path, dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-15_11-38.json), List(target_path, /Volumes/dr_eo_amperity/bronze/chuck))</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-15_11-33.json</td><td>copy</td><td>success</td><td>File copied successfully.</td><td>2025-09-18T21:16:09.084289Z</td><td>List(List(result, True), List(modificationTime, 1747333983000), List(fileSize, 16902), List(source_path, dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-15_11-33.json), List(target_path, /Volumes/dr_eo_amperity/bronze/chuck))</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-15_11-31.json</td><td>copy</td><td>success</td><td>File copied successfully.</td><td>2025-09-18T21:16:08.0124Z</td><td>List(List(result, True), List(modificationTime, 1747333883000), List(fileSize, 299), List(source_path, dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-15_11-31.json), List(target_path, /Volumes/dr_eo_amperity/bronze/chuck))</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-15_10-17.json</td><td>copy</td><td>success</td><td>File copied successfully.</td><td>2025-09-18T21:16:06.888347Z</td><td>List(List(result, True), List(modificationTime, 1747329464000), List(fileSize, 18012), List(source_path, dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-15_10-17.json), List(target_path, /Volumes/dr_eo_amperity/bronze/chuck))</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-14_16-23.json</td><td>copy</td><td>success</td><td>File copied successfully.</td><td>2025-09-18T21:16:05.775096Z</td><td>List(List(result, True), List(modificationTime, 1747264990000), List(fileSize, 16902), List(source_path, dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-14_16-23.json), List(target_path, /Volumes/dr_eo_amperity/bronze/chuck))</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-14_15-44.json</td><td>copy</td><td>success</td><td>File copied successfully.</td><td>2025-09-18T21:16:04.759318Z</td><td>List(List(result, True), List(modificationTime, 1747262693000), List(fileSize, 16902), List(source_path, dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-14_15-44.json), List(target_path, /Volumes/dr_eo_amperity/bronze/chuck))</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-14_15-39.json</td><td>copy</td><td>success</td><td>File copied successfully.</td><td>2025-09-18T21:16:03.698591Z</td><td>List(List(result, True), List(modificationTime, 1747262388000), List(fileSize, 16832), List(source_path, dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-14_15-39.json), List(target_path, /Volumes/dr_eo_amperity/bronze/chuck))</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-14_15-34.json</td><td>copy</td><td>success</td><td>File copied successfully.</td><td>2025-09-18T21:16:02.693571Z</td><td>List(List(result, True), List(modificationTime, 1747262078000), List(fileSize, 17893), List(source_path, dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-14_15-34.json), List(target_path, /Volumes/dr_eo_amperity/bronze/chuck))</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-14_15-32.json</td><td>copy</td><td>success</td><td>File copied successfully.</td><td>2025-09-18T21:16:01.538427Z</td><td>List(List(result, True), List(modificationTime, 1747261965000), List(fileSize, 18012), List(source_path, dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-14_15-32.json), List(target_path, /Volumes/dr_eo_amperity/bronze/chuck))</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-14_14-55.json</td><td>copy</td><td>success</td><td>File copied successfully.</td><td>2025-09-18T21:16:00.478159Z</td><td>List(List(result, True), List(modificationTime, 1747259736000), List(fileSize, 18012), List(source_path, dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-14_14-55.json), List(target_path, /Volumes/dr_eo_amperity/bronze/chuck))</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-14_14-45.json</td><td>copy</td><td>success</td><td>File copied successfully.</td><td>2025-09-18T21:15:59.39073Z</td><td>List(List(result, True), List(modificationTime, 1747259154000), List(fileSize, 18012), List(source_path, dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-14_14-45.json), List(target_path, /Volumes/dr_eo_amperity/bronze/chuck))</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-14_13-47.json</td><td>copy</td><td>success</td><td>File copied successfully.</td><td>2025-09-18T21:15:58.178338Z</td><td>List(List(result, True), List(modificationTime, 1747255644000), List(fileSize, 18012), List(source_path, dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-14_13-47.json), List(target_path, /Volumes/dr_eo_amperity/bronze/chuck))</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-29_12-32.json</td><td>copy</td><td>success</td><td>File copied successfully.</td><td>2025-09-18T21:15:57.078919Z</td><td>List(List(result, True), List(modificationTime, 1745955178000), List(fileSize, 18047), List(source_path, dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-29_12-32.json), List(target_path, /Volumes/dr_eo_amperity/bronze/chuck))</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_21-09.json</td><td>copy</td><td>success</td><td>File copied successfully.</td><td>2025-09-18T21:15:56.00826Z</td><td>List(List(result, True), List(modificationTime, 1745899795000), List(fileSize, 18047), List(source_path, dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_21-09.json), List(target_path, /Volumes/dr_eo_amperity/bronze/chuck))</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_20-52.json</td><td>copy</td><td>success</td><td>File copied successfully.</td><td>2025-09-18T21:15:54.901841Z</td><td>List(List(result, True), List(modificationTime, 1745898778000), List(fileSize, 18047), List(source_path, dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_20-52.json), List(target_path, /Volumes/dr_eo_amperity/bronze/chuck))</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_20-48.json</td><td>copy</td><td>success</td><td>File copied successfully.</td><td>2025-09-18T21:15:53.788665Z</td><td>List(List(result, True), List(modificationTime, 1745898532000), List(fileSize, 18047), List(source_path, dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_20-48.json), List(target_path, /Volumes/dr_eo_amperity/bronze/chuck))</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_20-44.json</td><td>copy</td><td>success</td><td>File copied successfully.</td><td>2025-09-18T21:15:52.696591Z</td><td>List(List(result, True), List(modificationTime, 1745898284000), List(fileSize, 18047), List(source_path, dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_20-44.json), List(target_path, /Volumes/dr_eo_amperity/bronze/chuck))</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_20-38.json</td><td>copy</td><td>success</td><td>File copied successfully.</td><td>2025-09-18T21:15:51.479603Z</td><td>List(List(result, True), List(modificationTime, 1745897893000), List(fileSize, 18047), List(source_path, dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_20-38.json), List(target_path, /Volumes/dr_eo_amperity/bronze/chuck))</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_20-26.json</td><td>copy</td><td>success</td><td>File copied successfully.</td><td>2025-09-18T21:15:50.497189Z</td><td>List(List(result, True), List(modificationTime, 1745897189000), List(fileSize, 18047), List(source_path, dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_20-26.json), List(target_path, /Volumes/dr_eo_amperity/bronze/chuck))</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_20-18.json</td><td>copy</td><td>success</td><td>File copied successfully.</td><td>2025-09-18T21:15:49.277021Z</td><td>List(List(result, True), List(modificationTime, 1745896726000), List(fileSize, 18047), List(source_path, dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_20-18.json), List(target_path, /Volumes/dr_eo_amperity/bronze/chuck))</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_19-59.json</td><td>copy</td><td>success</td><td>File copied successfully.</td><td>2025-09-18T21:15:48.137651Z</td><td>List(List(result, True), List(modificationTime, 1745895594000), List(fileSize, 18047), List(source_path, dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_19-59.json), List(target_path, /Volumes/dr_eo_amperity/bronze/chuck))</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_19-56.json</td><td>copy</td><td>success</td><td>File copied successfully.</td><td>2025-09-18T21:15:46.931652Z</td><td>List(List(result, True), List(modificationTime, 1745895417000), List(fileSize, 18047), List(source_path, dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_19-56.json), List(target_path, /Volumes/dr_eo_amperity/bronze/chuck))</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_19-37.json</td><td>copy</td><td>success</td><td>File copied successfully.</td><td>2025-09-18T21:15:45.756765Z</td><td>List(List(result, True), List(modificationTime, 1745894253000), List(fileSize, 17194), List(source_path, dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_19-37.json), List(target_path, /Volumes/dr_eo_amperity/bronze/chuck))</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_19-31.json</td><td>copy</td><td>success</td><td>File copied successfully.</td><td>2025-09-18T21:15:44.681708Z</td><td>List(List(result, True), List(modificationTime, 1745893886000), List(fileSize, 17194), List(source_path, dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_19-31.json), List(target_path, /Volumes/dr_eo_amperity/bronze/chuck))</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_18-42.json</td><td>copy</td><td>success</td><td>File copied successfully.</td><td>2025-09-18T21:15:43.576033Z</td><td>List(List(result, True), List(modificationTime, 1745890946000), List(fileSize, 17194), List(source_path, dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_18-42.json), List(target_path, /Volumes/dr_eo_amperity/bronze/chuck))</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_18-41.json</td><td>copy</td><td>success</td><td>File copied successfully.</td><td>2025-09-18T21:15:42.513374Z</td><td>List(List(result, True), List(modificationTime, 1745890894000), List(fileSize, 17194), List(source_path, dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_18-41.json), List(target_path, /Volumes/dr_eo_amperity/bronze/chuck))</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_18-40.json</td><td>copy</td><td>success</td><td>File copied successfully.</td><td>2025-09-18T21:15:41.378613Z</td><td>List(List(result, True), List(modificationTime, 1745890854000), List(fileSize, 17194), List(source_path, dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_18-40.json), List(target_path, /Volumes/dr_eo_amperity/bronze/chuck))</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_18-33.json</td><td>copy</td><td>success</td><td>File copied successfully.</td><td>2025-09-18T21:15:40.306935Z</td><td>List(List(result, True), List(modificationTime, 1745890411000), List(fileSize, 17194), List(source_path, dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_18-33.json), List(target_path, /Volumes/dr_eo_amperity/bronze/chuck))</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_18-30.json</td><td>copy</td><td>success</td><td>File copied successfully.</td><td>2025-09-18T21:15:39.118781Z</td><td>List(List(result, True), List(modificationTime, 1745890252000), List(fileSize, 17194), List(source_path, dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_18-30.json), List(target_path, /Volumes/dr_eo_amperity/bronze/chuck))</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_18-26.json</td><td>copy</td><td>success</td><td>File copied successfully.</td><td>2025-09-18T21:15:38.047954Z</td><td>List(List(result, True), List(modificationTime, 1745890004000), List(fileSize, 17194), List(source_path, dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_18-26.json), List(target_path, /Volumes/dr_eo_amperity/bronze/chuck))</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_18-15.json</td><td>copy</td><td>success</td><td>File copied successfully.</td><td>2025-09-18T21:15:36.937003Z</td><td>List(List(result, True), List(modificationTime, 1745889301000), List(fileSize, 17194), List(source_path, dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_18-15.json), List(target_path, /Volumes/dr_eo_amperity/bronze/chuck))</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_18-14.json</td><td>copy</td><td>success</td><td>File copied successfully.</td><td>2025-09-18T21:15:35.820544Z</td><td>List(List(result, True), List(modificationTime, 1745889261000), List(fileSize, 55), List(source_path, dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_18-14.json), List(target_path, /Volumes/dr_eo_amperity/bronze/chuck))</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_18-11.json</td><td>copy</td><td>success</td><td>File copied successfully.</td><td>2025-09-18T21:15:34.744886Z</td><td>List(List(result, True), List(modificationTime, 1745889083000), List(fileSize, 55), List(source_path, dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_18-11.json), List(target_path, /Volumes/dr_eo_amperity/bronze/chuck))</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_18-10.json</td><td>copy</td><td>success</td><td>File copied successfully.</td><td>2025-09-18T21:15:33.681794Z</td><td>List(List(result, True), List(modificationTime, 1745889054000), List(fileSize, 55), List(source_path, dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_18-10.json), List(target_path, /Volumes/dr_eo_amperity/bronze/chuck))</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_18-09.json</td><td>copy</td><td>success</td><td>File copied successfully.</td><td>2025-09-18T21:15:32.656915Z</td><td>List(List(result, True), List(modificationTime, 1745888998000), List(fileSize, 55), List(source_path, dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_18-09.json), List(target_path, /Volumes/dr_eo_amperity/bronze/chuck))</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_18-08.json</td><td>copy</td><td>success</td><td>File copied successfully.</td><td>2025-09-18T21:15:31.632977Z</td><td>List(List(result, True), List(modificationTime, 1745888923000), List(fileSize, 55), List(source_path, dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_18-08.json), List(target_path, /Volumes/dr_eo_amperity/bronze/chuck))</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_18-03.json</td><td>copy</td><td>success</td><td>File copied successfully.</td><td>2025-09-18T21:15:30.592632Z</td><td>List(List(result, True), List(modificationTime, 1745888602000), List(fileSize, 55), List(source_path, dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_18-03.json), List(target_path, /Volumes/dr_eo_amperity/bronze/chuck))</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_18-02.json</td><td>copy</td><td>success</td><td>File copied successfully.</td><td>2025-09-18T21:15:29.515902Z</td><td>List(List(result, True), List(modificationTime, 1745888569000), List(fileSize, 55), List(source_path, dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_18-02.json), List(target_path, /Volumes/dr_eo_amperity/bronze/chuck))</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_17-58.json</td><td>copy</td><td>success</td><td>File copied successfully.</td><td>2025-09-18T21:15:28.515036Z</td><td>List(List(result, True), List(modificationTime, 1745888285000), List(fileSize, 55), List(source_path, dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_17-58.json), List(target_path, /Volumes/dr_eo_amperity/bronze/chuck))</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_17-55.json</td><td>copy</td><td>success</td><td>File copied successfully.</td><td>2025-09-18T21:15:27.407532Z</td><td>List(List(result, True), List(modificationTime, 1745888126000), List(fileSize, 55), List(source_path, dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_17-55.json), List(target_path, /Volumes/dr_eo_amperity/bronze/chuck))</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_17-53.json</td><td>copy</td><td>success</td><td>File copied successfully.</td><td>2025-09-18T21:15:26.301526Z</td><td>List(List(result, True), List(modificationTime, 1745888001000), List(fileSize, 55), List(source_path, dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_17-53.json), List(target_path, /Volumes/dr_eo_amperity/bronze/chuck))</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_17-31.json</td><td>copy</td><td>success</td><td>File copied successfully.</td><td>2025-09-18T21:15:25.243905Z</td><td>List(List(result, True), List(modificationTime, 1745886718000), List(fileSize, 55), List(source_path, dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_17-31.json), List(target_path, /Volumes/dr_eo_amperity/bronze/chuck))</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_16-23.json</td><td>copy</td><td>success</td><td>File copied successfully.</td><td>2025-09-18T21:15:24.092939Z</td><td>List(List(result, True), List(modificationTime, 1745882597000), List(fileSize, 55), List(source_path, dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_16-23.json), List(target_path, /Volumes/dr_eo_amperity/bronze/chuck))</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_16-22.json</td><td>copy</td><td>success</td><td>File copied successfully.</td><td>2025-09-18T21:15:22.974103Z</td><td>List(List(result, True), List(modificationTime, 1745882546000), List(fileSize, 55), List(source_path, dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_16-22.json), List(target_path, /Volumes/dr_eo_amperity/bronze/chuck))</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_16-21.json</td><td>copy</td><td>success</td><td>File copied successfully.</td><td>2025-09-18T21:15:21.909218Z</td><td>List(List(result, True), List(modificationTime, 1745882513000), List(fileSize, 55), List(source_path, dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_16-21.json), List(target_path, /Volumes/dr_eo_amperity/bronze/chuck))</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_15-54.json</td><td>copy</td><td>success</td><td>File copied successfully.</td><td>2025-09-18T21:15:20.851122Z</td><td>List(List(result, True), List(modificationTime, 1745880870000), List(fileSize, 55), List(source_path, dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_15-54.json), List(target_path, /Volumes/dr_eo_amperity/bronze/chuck))</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_15-53.json</td><td>copy</td><td>success</td><td>File copied successfully.</td><td>2025-09-18T21:15:19.757661Z</td><td>List(List(result, True), List(modificationTime, 1745880836000), List(fileSize, 55), List(source_path, dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_15-53.json), List(target_path, /Volumes/dr_eo_amperity/bronze/chuck))</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_15-48.json</td><td>copy</td><td>success</td><td>File copied successfully.</td><td>2025-09-18T21:15:18.681895Z</td><td>List(List(result, True), List(modificationTime, 1745880531000), List(fileSize, 55), List(source_path, dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_15-48.json), List(target_path, /Volumes/dr_eo_amperity/bronze/chuck))</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_15-47.json</td><td>copy</td><td>success</td><td>File copied successfully.</td><td>2025-09-18T21:15:17.624036Z</td><td>List(List(result, True), List(modificationTime, 1745880480000), List(fileSize, 55), List(source_path, dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_15-47.json), List(target_path, /Volumes/dr_eo_amperity/bronze/chuck))</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_15-46.json</td><td>copy</td><td>success</td><td>File copied successfully.</td><td>2025-09-18T21:15:16.408467Z</td><td>List(List(result, True), List(modificationTime, 1745880398000), List(fileSize, 55), List(source_path, dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_15-46.json), List(target_path, /Volumes/dr_eo_amperity/bronze/chuck))</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_15-42.json</td><td>copy</td><td>success</td><td>File copied successfully.</td><td>2025-09-18T21:15:15.163876Z</td><td>List(List(result, True), List(modificationTime, 1745880146000), List(fileSize, 55), List(source_path, dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_15-42.json), List(target_path, /Volumes/dr_eo_amperity/bronze/chuck))</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_15-29.json</td><td>copy</td><td>success</td><td>File copied successfully.</td><td>2025-09-18T21:15:14.107296Z</td><td>List(List(result, True), List(modificationTime, 1745879389000), List(fileSize, 55), List(source_path, dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_15-29.json), List(target_path, /Volumes/dr_eo_amperity/bronze/chuck))</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_15-23.json</td><td>copy</td><td>success</td><td>File copied successfully.</td><td>2025-09-18T21:15:13.083335Z</td><td>List(List(result, True), List(modificationTime, 1745879035000), List(fileSize, 55), List(source_path, dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_15-23.json), List(target_path, /Volumes/dr_eo_amperity/bronze/chuck))</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_12-40.json</td><td>copy</td><td>success</td><td>File copied successfully.</td><td>2025-09-18T21:15:12.07184Z</td><td>List(List(result, True), List(modificationTime, 1745869227000), List(fileSize, 17698), List(source_path, dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_12-40.json), List(target_path, /Volumes/dr_eo_amperity/bronze/chuck))</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_12-01.json</td><td>copy</td><td>success</td><td>File copied successfully.</td><td>2025-09-18T21:15:11.030927Z</td><td>List(List(result, True), List(modificationTime, 1745866879000), List(fileSize, 17698), List(source_path, dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_12-01.json), List(target_path, /Volumes/dr_eo_amperity/bronze/chuck))</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_10-57.json</td><td>copy</td><td>success</td><td>File copied successfully.</td><td>2025-09-18T21:15:09.995028Z</td><td>List(List(result, True), List(modificationTime, 1745863055000), List(fileSize, 30370), List(source_path, dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_10-57.json), List(target_path, /Volumes/dr_eo_amperity/bronze/chuck))</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-25_17-38.json</td><td>copy</td><td>success</td><td>File copied successfully.</td><td>2025-09-18T21:15:08.982163Z</td><td>List(List(result, True), List(modificationTime, 1745627907000), List(fileSize, 30370), List(source_path, dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-25_17-38.json), List(target_path, /Volumes/dr_eo_amperity/bronze/chuck))</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-25_17-34.json</td><td>copy</td><td>success</td><td>File copied successfully.</td><td>2025-09-18T21:15:07.957553Z</td><td>List(List(result, True), List(modificationTime, 1745627668000), List(fileSize, 30370), List(source_path, dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-25_17-34.json), List(target_path, /Volumes/dr_eo_amperity/bronze/chuck))</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-25_17-20.json</td><td>copy</td><td>success</td><td>File copied successfully.</td><td>2025-09-18T21:15:06.479554Z</td><td>List(List(result, True), List(modificationTime, 1745626856000), List(fileSize, 30370), List(source_path, dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-25_17-20.json), List(target_path, /Volumes/dr_eo_amperity/bronze/chuck))</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-25_13-29.json</td><td>copy</td><td>success</td><td>File copied successfully.</td><td>2025-09-18T21:15:05.314691Z</td><td>List(List(result, True), List(modificationTime, 1745613000000), List(fileSize, 30370), List(source_path, dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-25_13-29.json), List(target_path, /Volumes/dr_eo_amperity/bronze/chuck))</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-25_12-04.json</td><td>copy</td><td>success</td><td>File copied successfully.</td><td>2025-09-18T21:15:04.280904Z</td><td>List(List(result, True), List(modificationTime, 1745607856000), List(fileSize, 30370), List(source_path, dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-25_12-04.json), List(target_path, /Volumes/dr_eo_amperity/bronze/chuck))</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-24_16-31.json</td><td>copy</td><td>success</td><td>File copied successfully.</td><td>2025-09-18T21:15:03.222782Z</td><td>List(List(result, True), List(modificationTime, 1745537510000), List(fileSize, 24570), List(source_path, dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-24_16-31.json), List(target_path, /Volumes/dr_eo_amperity/bronze/chuck))</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-24_16-28.json</td><td>copy</td><td>success</td><td>File copied successfully.</td><td>2025-09-18T21:15:02.165194Z</td><td>List(List(result, True), List(modificationTime, 1745537285000), List(fileSize, 24570), List(source_path, dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-24_16-28.json), List(target_path, /Volumes/dr_eo_amperity/bronze/chuck))</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-24_16-16.json</td><td>copy</td><td>success</td><td>File copied successfully.</td><td>2025-09-18T21:15:00.997723Z</td><td>List(List(result, True), List(modificationTime, 1745536604000), List(fileSize, 23764), List(source_path, dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-24_16-16.json), List(target_path, /Volumes/dr_eo_amperity/bronze/chuck))</td></tr><tr><td>DR_Volume_data_temp</td><td>directory</td><td>dbfs:/Volumes/satya_share/satyendranath_sure/satya_volume/test/test1/</td><td>list</td><td>success</td><td>Found directory to traverse.</td><td>2025-09-18T21:14:59.946687Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-24_15-46.json</td><td>copy</td><td>success</td><td>File copied successfully.</td><td>2025-09-18T21:14:59.879595Z</td><td>List(List(result, True), List(modificationTime, 1745534811000), List(fileSize, 12523), List(source_path, dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-24_15-46.json), List(target_path, /Volumes/dr_eo_amperity/bronze/chuck))</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/satya_share/satyendranath_sure/satya_volume/test/file_str11.0.parquet</td><td>copy</td><td>success</td><td>File copied successfully.</td><td>2025-09-18T21:14:58.940053Z</td><td>List(List(result, True), List(modificationTime, 1758221596000), List(fileSize, 6302661), List(source_path, dbfs:/Volumes/satya_share/satyendranath_sure/satya_volume/test/file_str11.0.parquet), List(target_path, /Volumes/satya_dr_catalog/satyendranath_sure/satya_volume/test/))</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-24_15-42.json</td><td>copy</td><td>success</td><td>File copied successfully.</td><td>2025-09-18T21:14:58.823548Z</td><td>List(List(result, True), List(modificationTime, 1745534570000), List(fileSize, 12523), List(source_path, dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-24_15-42.json), List(target_path, /Volumes/dr_eo_amperity/bronze/chuck))</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-24_15-28.txt</td><td>copy</td><td>success</td><td>File copied successfully.</td><td>2025-09-18T21:14:57.704662Z</td><td>List(List(result, True), List(modificationTime, 1745533788000), List(fileSize, 10424), List(source_path, dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-24_15-28.txt), List(target_path, /Volumes/dr_eo_amperity/bronze/chuck))</td></tr><tr><td>DR_Volume_data_temp</td><td>directory</td><td>dbfs:/Volumes/satya_share/satyendranath_sure/satya_volume/test/</td><td>list</td><td>success</td><td>Found directory to traverse.</td><td>2025-09-18T21:14:56.78221Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-24_15-23.txt</td><td>copy</td><td>success</td><td>File copied successfully.</td><td>2025-09-18T21:14:55.882649Z</td><td>List(List(result, True), List(modificationTime, 1745533417000), List(fileSize, 10397), List(source_path, dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-24_15-23.txt), List(target_path, /Volumes/dr_eo_amperity/bronze/chuck))</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/satya_share/satyendranath_sure/satya_volume/file_str13.0.parquet</td><td>copy</td><td>success</td><td>File copied successfully.</td><td>2025-09-18T21:14:55.474236Z</td><td>List(List(result, True), List(modificationTime, 1758208344000), List(fileSize, 6403985), List(source_path, dbfs:/Volumes/satya_share/satyendranath_sure/satya_volume/file_str13.0.parquet), List(target_path, /Volumes/satya_dr_catalog/satyendranath_sure/satya_volume))</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/satya_share/satyendranath_sure/satya_external_volume/file_str14.0.parquet</td><td>copy</td><td>success</td><td>File copied successfully.</td><td>2025-09-18T21:14:54.991841Z</td><td>List(List(result, True), List(modificationTime, 1758207658000), List(fileSize, 6232030), List(source_path, dbfs:/Volumes/satya_share/satyendranath_sure/satya_external_volume/file_str14.0.parquet), List(target_path, /Volumes/satya_dr_catalog/satyendranath_sure/satya_external_volume))</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-24_15-14.txt</td><td>copy</td><td>success</td><td>File copied successfully.</td><td>2025-09-18T21:14:54.590562Z</td><td>List(List(result, True), List(modificationTime, 1745532874000), List(fileSize, 9705), List(source_path, dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-24_15-14.txt), List(target_path, /Volumes/dr_eo_amperity/bronze/chuck))</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/satya_share/satyendranath_sure/satya_volume/file_str11.0.parquet</td><td>copy</td><td>success</td><td>File copied successfully.</td><td>2025-09-18T21:14:53.707176Z</td><td>List(List(result, True), List(modificationTime, 1758227993000), List(fileSize, 6302661), List(source_path, dbfs:/Volumes/satya_share/satyendranath_sure/satya_volume/file_str11.0.parquet), List(target_path, /Volumes/satya_dr_catalog/satyendranath_sure/satya_volume))</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-24_15-12.json</td><td>copy</td><td>success</td><td>File copied successfully.</td><td>2025-09-18T21:14:53.174909Z</td><td>List(List(result, True), List(modificationTime, 1745532746000), List(fileSize, 9705), List(source_path, dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-24_15-12.json), List(target_path, /Volumes/dr_eo_amperity/bronze/chuck))</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/satya_share/satyendranath_sure/satya_external_volume/file_str13.0.parquet</td><td>copy</td><td>success</td><td>File copied successfully.</td><td>2025-09-18T21:14:52.743873Z</td><td>List(List(result, True), List(modificationTime, 1758207658000), List(fileSize, 6403985), List(source_path, dbfs:/Volumes/satya_share/satyendranath_sure/satya_external_volume/file_str13.0.parquet), List(target_path, /Volumes/satya_dr_catalog/satyendranath_sure/satya_external_volume))</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/joe-stitch-2025-05-29_10-51.json</td><td>copy</td><td>success</td><td>File copied successfully.</td><td>2025-09-18T21:14:51.254898Z</td><td>List(List(result, True), List(modificationTime, 1748542172000), List(fileSize, 4874), List(source_path, dbfs:/Volumes/_eo_amperity/bronze/chuck/joe-stitch-2025-05-29_10-51.json), List(target_path, /Volumes/dr_eo_amperity/bronze/chuck))</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/satya_share/satyendranath_sure/satya_volume/Group_access_revoke.csv</td><td>copy</td><td>success</td><td>File copied successfully.</td><td>2025-09-18T21:14:50.788988Z</td><td>List(List(result, True), List(modificationTime, 1754924360000), List(fileSize, 260), List(source_path, dbfs:/Volumes/satya_share/satyendranath_sure/satya_volume/Group_access_revoke.csv), List(target_path, /Volumes/satya_dr_catalog/satyendranath_sure/satya_volume))</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/satya_share/satyendranath_sure/satya_external_volume/file_str11.0.parquet</td><td>copy</td><td>success</td><td>File copied successfully.</td><td>2025-09-18T21:14:49.917056Z</td><td>List(List(result, True), List(modificationTime, 1758207658000), List(fileSize, 6302661), List(source_path, dbfs:/Volumes/satya_share/satyendranath_sure/satya_external_volume/file_str11.0.parquet), List(target_path, /Volumes/satya_dr_catalog/satyendranath_sure/satya_external_volume))</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/file.txt</td><td>copy</td><td>success</td><td>File copied successfully.</td><td>2025-09-18T21:14:49.403334Z</td><td>List(List(result, True), List(modificationTime, 1745523282000), List(fileSize, 2), List(source_path, dbfs:/Volumes/_eo_amperity/bronze/chuck/file.txt), List(target_path, /Volumes/dr_eo_amperity/bronze/chuck))</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/satya_share/satyendranath_sure/satya_volume/DR FY26 SOP.pdf</td><td>copy</td><td>success</td><td>File copied successfully.</td><td>2025-09-18T21:14:48.899447Z</td><td>List(List(result, True), List(modificationTime, 1754924251000), List(fileSize, 3595169), List(source_path, dbfs:/Volumes/satya_share/satyendranath_sure/satya_volume/DR FY26 SOP.pdf), List(target_path, /Volumes/satya_dr_catalog/satyendranath_sure/satya_volume))</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/democracy-manifest.json</td><td>copy</td><td>success</td><td>File copied successfully.</td><td>2025-09-18T21:14:48.414612Z</td><td>List(List(result, True), List(modificationTime, 1745605748000), List(fileSize, 1681), List(source_path, dbfs:/Volumes/_eo_amperity/bronze/chuck/democracy-manifest.json), List(target_path, /Volumes/dr_eo_amperity/bronze/chuck))</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/cluster_init.sh</td><td>copy</td><td>success</td><td>File copied successfully.</td><td>2025-09-18T21:14:46.879889Z</td><td>List(List(result, True), List(modificationTime, 1748370759000), List(fileSize, 631), List(source_path, dbfs:/Volumes/_eo_amperity/bronze/chuck/cluster_init.sh), List(target_path, /Volumes/dr_eo_amperity/bronze/chuck))</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/cluster_init-2025-06-11_10-46.sh</td><td>copy</td><td>success</td><td>File copied successfully.</td><td>2025-09-18T21:14:44.797171Z</td><td>List(List(result, True), List(modificationTime, 1749663991000), List(fileSize, 661), List(source_path, dbfs:/Volumes/_eo_amperity/bronze/chuck/cluster_init-2025-06-11_10-46.sh), List(target_path, /Volumes/dr_eo_amperity/bronze/chuck))</td></tr><tr><td>DR_Volume_data_temp</td><td>volume</td><td>satya_dr_catalog.satyendranath_sure.satya_volume</td><td>create</td><td>success</td><td>Volume created or already exists.</td><td>2025-09-18T21:14:44.400831Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>volume</td><td>satya_dr_catalog.satyendranath_sure.satya_external_volume</td><td>create</td><td>success</td><td>Volume created or already exists.</td><td>2025-09-18T21:14:44.39101Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/cluster_init-2025-06-10_11-26.sh</td><td>copy</td><td>success</td><td>File copied successfully.</td><td>2025-09-18T21:14:43.651562Z</td><td>List(List(result, True), List(modificationTime, 1749580011000), List(fileSize, 661), List(source_path, dbfs:/Volumes/_eo_amperity/bronze/chuck/cluster_init-2025-06-10_11-26.sh), List(target_path, /Volumes/dr_eo_amperity/bronze/chuck))</td></tr><tr><td>DR_Volume_data_temp</td><td>schema</td><td>satya_dr_catalog.satyendranath_sure</td><td>create</td><td>success</td><td>Schema `satyendranath_sure` in catalog `satya_dr_catalog` created or already exists.</td><td>2025-09-18T21:14:42.676031Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/cluster_init-2025-06-08_18-02.sh</td><td>copy</td><td>success</td><td>File copied successfully.</td><td>2025-09-18T21:14:42.032683Z</td><td>List(List(result, True), List(modificationTime, 1749430987000), List(fileSize, 661), List(source_path, dbfs:/Volumes/_eo_amperity/bronze/chuck/cluster_init-2025-06-08_18-02.sh), List(target_path, /Volumes/dr_eo_amperity/bronze/chuck))</td></tr><tr><td>DR_Volume_data_temp</td><td>schema</td><td>satya_share.information_schema</td><td>skip</td><td>success</td><td>Skipping 'information_schema'.</td><td>2025-09-18T21:14:41.71179Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/cluster_init-2025-06-08_15-57.sh</td><td>copy</td><td>success</td><td>File copied successfully.</td><td>2025-09-18T21:14:40.41959Z</td><td>List(List(result, True), List(modificationTime, 1749423479000), List(fileSize, 661), List(source_path, dbfs:/Volumes/_eo_amperity/bronze/chuck/cluster_init-2025-06-08_15-57.sh), List(target_path, /Volumes/dr_eo_amperity/bronze/chuck))</td></tr><tr><td>DR_Volume_data_temp</td><td>catalog</td><td>satya_dr_catalog</td><td>create</td><td>success</td><td>Catalog `satya_dr_catalog` created or already exists.</td><td>2025-09-18T21:14:39.593466Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/cluster_init-2025-06-05_13-39.sh</td><td>copy</td><td>success</td><td>File copied successfully.</td><td>2025-09-18T21:14:39.217178Z</td><td>List(List(result, True), List(modificationTime, 1749155980000), List(fileSize, 661), List(source_path, dbfs:/Volumes/_eo_amperity/bronze/chuck/cluster_init-2025-06-05_13-39.sh), List(target_path, /Volumes/dr_eo_amperity/bronze/chuck))</td></tr><tr><td>DR_Volume_data_temp</td><td>catalog</td><td>test_not</td><td>process</td><td>error</td><td>An error occurred while processing catalog pair. Error: [NO_SUCH_CATALOG_EXCEPTION] Catalog 'test_not' was not found. Please verify the catalog name and then retry the query or command again. SQLSTATE: 42704\n",
       "\n",
       "JVM stacktrace:\n",
       "org.apache.spark.sql.catalyst.analysis.NoSuchCatalogException\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$listSchemasProto$1(ManagedCatalogClientImpl.scala:1249)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$recordAndWrapExceptionBase$2(ManagedCatalogClientImpl.scala:7501)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:113)\n",
       "\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:159)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:104)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$recordAndWrapExceptionBase$1(ManagedCatalogClientImpl.scala:7500)\n",
       "\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:74)\n",
       "\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:66)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:268)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapExceptionBase(ManagedCatalogClientImpl.scala:7481)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:7467)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.listSchemasProto(ManagedCatalogClientImpl.scala:1215)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.listSchemas(ManagedCatalogClientImpl.scala:1201)\n",
       "\tat com.databricks.sql.managedcatalog.ManagedCatalogCommon.listSchemas(ManagedCatalogCommon.scala:824)\n",
       "\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.$anonfun$listSchemas$1(ProfiledManagedCatalog.scala:173)\n",
       "\tat org.apache.spark.sql.catalyst.MetricKeyUtils$.measure(MetricKey.scala:1836)\n",
       "\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.$anonfun$profile$1(ProfiledManagedCatalog.scala:64)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:113)\n",
       "\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:159)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:104)\n",
       "\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.profile(ProfiledManagedCatalog.scala:63)\n",
       "\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.listSchemas(ProfiledManagedCatalog.scala:173)\n",
       "\tat com.databricks.sql.managedcatalog.ManagedCatalogSessionCatalog.listDatabasesWithCatalog(ManagedCatalogSessionCatalog.scala:928)\n",
       "\tat com.databricks.sql.managedcatalog.UnityCatalogV2Proxy.listNamespaces(UnityCatalogV2Proxy.scala:141)\n",
       "\tat org.apache.spark.sql.execution.command.ShowNamespacesCommand.run(ShowNamespacesCommand.scala:44)\n",
       "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$2(commands.scala:87)\n",
       "\tat org.apache.spark.sql.execution.SparkPlan.runCommandInAetherOrSpark(SparkPlan.scala:195)\n",
       "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$1(commands.scala:87)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:113)\n",
       "\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:159)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:104)\n",
       "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:84)\n",
       "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:83)\n",
       "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:97)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$5(QueryExecution.scala:507)\n",
       "\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$4(QueryExecution.scala:507)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:253)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$3(QueryExecution.scala:506)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$13(SQLExecution.scala:536)\n",
       "\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$12(SQLExecution.scala:458)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:834)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$11(SQLExecution.scala:386)\n",
       "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n",
       "\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:121)\n",
       "\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:115)\n",
       "\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:120)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$10(SQLExecution.scala:386)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:863)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:385)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:238)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:787)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:502)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1449)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:498)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.withMVTagsIfNecessary(QueryExecution.scala:418)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:496)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:578)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:570)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:529)\n",
       "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:529)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:42)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:361)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:357)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:505)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$8(QueryExecution.scala:570)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:570)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:374)\n",
       "\tat scala.util.Try$.apply(Try.scala:217)\n",
       "\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1686)\n",
       "\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1747)\n",
       "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:75)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:379)\n",
       "\tat org.apache.spark.sql.classic.Dataset.<init>(Dataset.scala:432)\n",
       "\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$3(Dataset.scala:155)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n",
       "\tat org.apache.spark.sql.classic.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1071)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:113)\n",
       "\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:159)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:104)\n",
       "\tat org.apache.spark.sql.classic.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1071)\n",
       "\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:146)\n",
       "\tat org.apache.spark.sql.classic.SparkSession.$anonfun$sql$4(SparkSession.scala:850)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n",
       "\tat org.apache.spark.sql.classic.SparkSession.sql(SparkSession.scala:813)\n",
       "\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.executeSQL(SparkConnectPlanner.scala:3500)\n",
       "\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleSqlCommand(SparkConnectPlanner.scala:3330)\n",
       "\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3207)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:385)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:282)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:238)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:466)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:466)\n",
       "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n",
       "\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:121)\n",
       "\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:115)\n",
       "\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:120)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:465)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:238)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$execute$1(ExecuteThreadRunner.scala:141)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
       "\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries(UtilizationMetrics.scala:43)\n",
       "\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries$(UtilizationMetrics.scala:40)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.recordActiveQueries(ExecuteThreadRunner.scala:53)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:139)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:586)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
       "\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n",
       "\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n",
       "\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n",
       "\tat scala.util.Using$.resource(Using.scala:296)\n",
       "\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:586)</td><td>2025-09-18T21:14:38.634491Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/cluster_init-2025-06-04_18-14.sh</td><td>copy</td><td>success</td><td>File copied successfully.</td><td>2025-09-18T21:14:38.124731Z</td><td>List(List(result, True), List(modificationTime, 1749086082000), List(fileSize, 661), List(source_path, dbfs:/Volumes/_eo_amperity/bronze/chuck/cluster_init-2025-06-04_18-14.sh), List(target_path, /Volumes/dr_eo_amperity/bronze/chuck))</td></tr><tr><td>DR_Volume_data_temp</td><td>catalog</td><td>dr_test_not</td><td>create</td><td>success</td><td>Catalog `dr_test_not` created or already exists.</td><td>2025-09-18T21:14:37.667473Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/cluster_init-2025-06-04_17-57.sh</td><td>copy</td><td>success</td><td>File copied successfully.</td><td>2025-09-18T21:14:36.895568Z</td><td>List(List(result, True), List(modificationTime, 1749085080000), List(fileSize, 661), List(source_path, dbfs:/Volumes/_eo_amperity/bronze/chuck/cluster_init-2025-06-04_17-57.sh), List(target_path, /Volumes/dr_eo_amperity/bronze/chuck))</td></tr><tr><td>DR_Volume_data_temp</td><td>schema</td><td>accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample.information_schema</td><td>skip</td><td>success</td><td>Skipping 'information_schema'.</td><td>2025-09-18T21:14:36.70498Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/cluster_init-2025-06-04_09-48.sh</td><td>copy</td><td>success</td><td>File copied successfully.</td><td>2025-09-18T21:14:35.510614Z</td><td>List(List(result, True), List(modificationTime, 1749055706000), List(fileSize, 661), List(source_path, dbfs:/Volumes/_eo_amperity/bronze/chuck/cluster_init-2025-06-04_09-48.sh), List(target_path, /Volumes/dr_eo_amperity/bronze/chuck))</td></tr><tr><td>DR_Volume_data_temp</td><td>schema</td><td>dr_accuweather.forecast</td><td>create</td><td>success</td><td>Schema `forecast` in catalog `dr_accuweather` created or already exists.</td><td>2025-09-18T21:14:34.710307Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/cluster_init-2025-06-04_07-39.sh</td><td>copy</td><td>success</td><td>File copied successfully.</td><td>2025-09-18T21:14:34.364724Z</td><td>List(List(result, True), List(modificationTime, 1749047992000), List(fileSize, 661), List(source_path, dbfs:/Volumes/_eo_amperity/bronze/chuck/cluster_init-2025-06-04_07-39.sh), List(target_path, /Volumes/dr_eo_amperity/bronze/chuck))</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/cluster_init-2025-06-02_20-37.sh</td><td>copy</td><td>success</td><td>File copied successfully.</td><td>2025-09-18T21:14:33.123612Z</td><td>List(List(result, True), List(modificationTime, 1748921868000), List(fileSize, 745), List(source_path, dbfs:/Volumes/_eo_amperity/bronze/chuck/cluster_init-2025-06-02_20-37.sh), List(target_path, /Volumes/dr_eo_amperity/bronze/chuck))</td></tr><tr><td>DR_Volume_data_temp</td><td>catalog</td><td>dr_accuweather</td><td>create</td><td>success</td><td>Catalog `dr_accuweather` created or already exists.</td><td>2025-09-18T21:14:33.068882Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>schema</td><td>dr_eo_amperity.silver</td><td>create</td><td>success</td><td>Schema `silver` in catalog `dr_eo_amperity` created or already exists.</td><td>2025-09-18T21:14:31.714671Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/chinese-meal.json</td><td>copy</td><td>success</td><td>File copied successfully.</td><td>2025-09-18T21:14:31.700535Z</td><td>List(List(result, True), List(modificationTime, 1745606447000), List(fileSize, 1323), List(source_path, dbfs:/Volumes/_eo_amperity/bronze/chuck/chinese-meal.json), List(target_path, /Volumes/dr_eo_amperity/bronze/chuck))</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/acme_w_fks.json</td><td>copy</td><td>success</td><td>File copied successfully.</td><td>2025-09-18T21:14:30.575023Z</td><td>List(List(result, True), List(modificationTime, 1745878932000), List(fileSize, 9936), List(source_path, dbfs:/Volumes/_eo_amperity/bronze/chuck/acme_w_fks.json), List(target_path, /Volumes/dr_eo_amperity/bronze/chuck))</td></tr><tr><td>DR_Volume_data_temp</td><td>file</td><td>dbfs:/Volumes/_eo_amperity/bronze/chuck/acme.json</td><td>copy</td><td>success</td><td>File copied successfully.</td><td>2025-09-18T21:14:28.798117Z</td><td>List(List(result, True), List(modificationTime, 1745882459000), List(fileSize, 9891), List(source_path, dbfs:/Volumes/_eo_amperity/bronze/chuck/acme.json), List(target_path, /Volumes/dr_eo_amperity/bronze/chuck))</td></tr><tr><td>DR_Volume_data_temp</td><td>schema</td><td>_eo_amperity.information_schema</td><td>skip</td><td>success</td><td>Skipping 'information_schema'.</td><td>2025-09-18T21:14:28.077174Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>schema</td><td>dr_eo_amperity.default</td><td>create</td><td>success</td><td>Schema `default` in catalog `dr_eo_amperity` created or already exists.</td><td>2025-09-18T21:14:26.535566Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>volume</td><td>dr_eo_amperity.bronze.chuck</td><td>create</td><td>success</td><td>Volume created or already exists.</td><td>2025-09-18T21:14:26.496356Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>schema</td><td>dr_eo_amperity.bronze</td><td>create</td><td>success</td><td>Schema `bronze` in catalog `dr_eo_amperity` created or already exists.</td><td>2025-09-18T21:14:25.107757Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>catalog</td><td>dr_eo_amperity</td><td>create</td><td>success</td><td>Catalog `dr_eo_amperity` created or already exists.</td><td>2025-09-18T21:14:23.76101Z</td><td>List()</td></tr><tr><td>DR_Volume_data_temp</td><td>info</td><td>N/A</td><td>start_sync</td><td>success</td><td>Starting Databricks Volume DR Sync.</td><td>2025-09-18T21:14:22.650586Z</td><td>List()</td></tr><tr><td>DR_Model_data</td><td>schema</td><td>`satya_dr_catalog`.`satyendranath_sure`</td><td>create</td><td>success</td><td>Schema `satya_dr_catalog`.`satyendranath_sure` created or already exists.</td><td>2025-08-30T15:08:52.504525Z</td><td>List()</td></tr><tr><td>DR_Model_data</td><td>catalog</td><td>`satya_dr_catalog`</td><td>create</td><td>success</td><td>Catalog `satya_dr_catalog` created or already exists.</td><td>2025-08-30T15:08:50.532938Z</td><td>List()</td></tr><tr><td>DR_Model_data</td><td>catalog_pair</td><td>Processing of test_not:dr_test_not</td><td>unhandled_error</td><td>error</td><td>An unhandled error occurred: [NO_SUCH_CATALOG_EXCEPTION] Catalog 'test_not' was not found. Please verify the catalog name and then retry the query or command again. SQLSTATE: 42704\n",
       "\n",
       "JVM stacktrace:\n",
       "org.apache.spark.sql.catalyst.analysis.NoSuchCatalogException\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$listSchemasProto$1(ManagedCatalogClientImpl.scala:1328)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$recordAndWrapExceptionBase$2(ManagedCatalogClientImpl.scala:7398)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$recordAndWrapExceptionBase$1(ManagedCatalogClientImpl.scala:7397)\n",
       "\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:66)\n",
       "\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:63)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:251)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapExceptionBase(ManagedCatalogClientImpl.scala:7378)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:7364)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.listSchemasProto(ManagedCatalogClientImpl.scala:1293)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.listSchemas(ManagedCatalogClientImpl.scala:1279)\n",
       "\tat com.databricks.sql.managedcatalog.ManagedCatalogCommon.listSchemas(ManagedCatalogCommon.scala:827)\n",
       "\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.$anonfun$listSchemas$1(ProfiledManagedCatalog.scala:168)\n",
       "\tat org.apache.spark.sql.catalyst.MetricKeyUtils$.measure(MetricKey.scala:1709)\n",
       "\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.$anonfun$profile$1(ProfiledManagedCatalog.scala:64)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.profile(ProfiledManagedCatalog.scala:63)\n",
       "\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.listSchemas(ProfiledManagedCatalog.scala:168)\n",
       "\tat com.databricks.sql.managedcatalog.ManagedCatalogSessionCatalog.listDatabasesWithCatalog(ManagedCatalogSessionCatalog.scala:908)\n",
       "\tat com.databricks.sql.managedcatalog.UnityCatalogV2Proxy.listNamespaces(UnityCatalogV2Proxy.scala:141)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.ShowNamespacesExec.run(ShowNamespacesExec.scala:42)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.$anonfun$result$2(V2CommandExec.scala:48)\n",
       "\tat org.apache.spark.sql.execution.SparkPlan.runCommandInAetherOrSpark(SparkPlan.scala:194)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.$anonfun$result$1(V2CommandExec.scala:48)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:47)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:45)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:56)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$5(QueryExecution.scala:469)\n",
       "\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$4(QueryExecution.scala:469)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:219)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$3(QueryExecution.scala:468)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$11(SQLExecution.scala:479)\n",
       "\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$10(SQLExecution.scala:415)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:779)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:345)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:214)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:716)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:464)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1342)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:460)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.withMVTagsIfNecessary(QueryExecution.scala:389)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:458)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:537)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:532)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:521)\n",
       "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:521)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:42)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:361)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:357)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:497)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$8(QueryExecution.scala:532)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:532)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:345)\n",
       "\tat scala.util.Try$.apply(Try.scala:213)\n",
       "\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)\n",
       "\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)\n",
       "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:350)\n",
       "\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:406)\n",
       "\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$3(Dataset.scala:150)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n",
       "\tat org.apache.spark.sql.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1469)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1469)\n",
       "\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:141)\n",
       "\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:1142)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n",
       "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:1094)\n",
       "\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.executeSQL(SparkConnectPlanner.scala:3606)\n",
       "\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleSqlCommand(SparkConnectPlanner.scala:3435)\n",
       "\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3370)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:413)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:312)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:233)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:464)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:464)\n",
       "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n",
       "\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:90)\n",
       "\tat org.apache.spark.util.Utils$.withContextClassLoader(Utils.scala:241)\n",
       "\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:89)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:463)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:233)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:139)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:614)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n",
       "\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n",
       "\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n",
       "\tat scala.util.Using$.resource(Using.scala:269)\n",
       "\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:614)\n",
       "Traceback (most recent call last):\n",
       "  File \"/home/spark-fc8c1a42-477f-476e-91f5-48/.ipykernel/1920/command-8748648698932923-2826314312\", line 295, in main\n",
       "    source_schemas_df = spark.sql(f\"SHOW SCHEMAS IN `{source_catalog}`\")\n",
       "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
       "  File \"/databricks/spark/python/pyspark/sql/connect/session.py\", line 821, in sql\n",
       "    data, properties, ei = self.client.execute_command(cmd.command(self._client))\n",
       "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
       "  File \"/databricks/spark/python/pyspark/sql/connect/client/core.py\", line 1481, in execute_command\n",
       "    data, _, metrics, observed_metrics, properties = self._execute_and_fetch(\n",
       "                                                     ^^^^^^^^^^^^^^^^^^^^^^^^\n",
       "  File \"/databricks/spark/python/pyspark/sql/connect/client/core.py\", line 1970, in _execute_and_fetch\n",
       "    for response in self._execute_and_fetch_as_iterator(\n",
       "  File \"/databricks/spark/python/pyspark/sql/connect/client/core.py\", line 1946, in _execute_and_fetch_as_iterator\n",
       "    self._handle_error(error)\n",
       "  File \"/databricks/spark/python/pyspark/sql/connect/client/core.py\", line 2266, in _handle_error\n",
       "    self._handle_rpc_error(error)\n",
       "  File \"/databricks/spark/python/pyspark/sql/connect/client/core.py\", line 2347, in _handle_rpc_error\n",
       "    raise convert_exception(\n",
       "pyspark.errors.exceptions.connect.AnalysisException: [NO_SUCH_CATALOG_EXCEPTION] Catalog 'test_not' was not found. Please verify the catalog name and then retry the query or command again. SQLSTATE: 42704\n",
       "\n",
       "JVM stacktrace:\n",
       "org.apache.spark.sql.catalyst.analysis.NoSuchCatalogException\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$listSchemasProto$1(ManagedCatalogClientImpl.scala:1328)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$recordAndWrapExceptionBase$2(ManagedCatalogClientImpl.scala:7398)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$recordAndWrapExceptionBase$1(ManagedCatalogClientImpl.scala:7397)\n",
       "\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:66)\n",
       "\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:63)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:251)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapExceptionBase(ManagedCatalogClientImpl.scala:7378)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:7364)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.listSchemasProto(ManagedCatalogClientImpl.scala:1293)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.listSchemas(ManagedCatalogClientImpl.scala:1279)\n",
       "\tat com.databricks.sql.managedcatalog.ManagedCatalogCommon.listSchemas(ManagedCatalogCommon.scala:827)\n",
       "\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.$anonfun$listSchemas$1(ProfiledManagedCatalog.scala:168)\n",
       "\tat org.apache.spark.sql.catalyst.MetricKeyUtils$.measure(MetricKey.scala:1709)\n",
       "\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.$anonfun$profile$1(ProfiledManagedCatalog.scala:64)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.profile(ProfiledManagedCatalog.scala:63)\n",
       "\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.listSchemas(ProfiledManagedCatalog.scala:168)\n",
       "\tat com.databricks.sql.managedcatalog.ManagedCatalogSessionCatalog.listDatabasesWithCatalog(ManagedCatalogSessionCatalog.scala:908)\n",
       "\tat com.databricks.sql.managedcatalog.UnityCatalogV2Proxy.listNamespaces(UnityCatalogV2Proxy.scala:141)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.ShowNamespacesExec.run(ShowNamespacesExec.scala:42)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.$anonfun$result$2(V2CommandExec.scala:48)\n",
       "\tat org.apache.spark.sql.execution.SparkPlan.runCommandInAetherOrSpark(SparkPlan.scala:194)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.$anonfun$result$1(V2CommandExec.scala:48)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:47)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:45)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:56)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$5(QueryExecution.scala:469)\n",
       "\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$4(QueryExecution.scala:469)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:219)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$3(QueryExecution.scala:468)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$11(SQLExecution.scala:479)\n",
       "\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$10(SQLExecution.scala:415)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:779)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:345)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:214)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:716)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:464)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1342)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:460)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.withMVTagsIfNecessary(QueryExecution.scala:389)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:458)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:537)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:532)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:521)\n",
       "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:521)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:42)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:361)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:357)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:497)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$8(QueryExecution.scala:532)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:532)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:345)\n",
       "\tat scala.util.Try$.apply(Try.scala:213)\n",
       "\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)\n",
       "\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)\n",
       "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:350)\n",
       "\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:406)\n",
       "\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$3(Dataset.scala:150)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n",
       "\tat org.apache.spark.sql.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1469)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1469)\n",
       "\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:141)\n",
       "\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:1142)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n",
       "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:1094)\n",
       "\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.executeSQL(SparkConnectPlanner.scala:3606)\n",
       "\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleSqlCommand(SparkConnectPlanner.scala:3435)\n",
       "\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3370)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:413)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:312)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:233)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:464)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:464)\n",
       "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n",
       "\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:90)\n",
       "\tat org.apache.spark.util.Utils$.withContextClassLoader(Utils.scala:241)\n",
       "\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:89)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:463)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:233)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:139)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:614)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n",
       "\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n",
       "\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n",
       "\tat scala.util.Using$.resource(Using.scala:269)\n",
       "\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:614)\n",
       "</td><td>2025-08-30T15:08:49.180449Z</td><td>List()</td></tr><tr><td>DR_Model_data</td><td>catalog</td><td>`dr_test_not`</td><td>create</td><td>success</td><td>Catalog `dr_test_not` created or already exists.</td><td>2025-08-30T15:08:47.839358Z</td><td>List()</td></tr><tr><td>DR_Model_data</td><td>schema</td><td>`dr_accuweather`.`forecast`</td><td>create</td><td>success</td><td>Schema `dr_accuweather`.`forecast` created or already exists.</td><td>2025-08-30T15:08:46.45627Z</td><td>List()</td></tr><tr><td>DR_Model_data</td><td>catalog</td><td>`dr_accuweather`</td><td>create</td><td>success</td><td>Catalog `dr_accuweather` created or already exists.</td><td>2025-08-30T15:08:33.118839Z</td><td>List()</td></tr><tr><td>DR_Model_data</td><td>schema</td><td>`dr_eo_amperity`.`silver`</td><td>create</td><td>success</td><td>Schema `dr_eo_amperity`.`silver` created or already exists.</td><td>2025-08-30T15:08:31.722105Z</td><td>List()</td></tr><tr><td>DR_Model_data</td><td>schema</td><td>`dr_eo_amperity`.`default`</td><td>create</td><td>success</td><td>Schema `dr_eo_amperity`.`default` created or already exists.</td><td>2025-08-30T15:08:30.330966Z</td><td>List()</td></tr><tr><td>DR_Model_data</td><td>schema</td><td>`dr_eo_amperity`.`bronze`</td><td>create</td><td>success</td><td>Schema `dr_eo_amperity`.`bronze` created or already exists.</td><td>2025-08-30T15:08:28.698182Z</td><td>List()</td></tr><tr><td>DR_Model_data</td><td>catalog</td><td>`dr_eo_amperity`</td><td>create</td><td>success</td><td>Catalog `dr_eo_amperity` created or already exists.</td><td>2025-08-30T15:08:04.983538Z</td><td>List()</td></tr><tr><td>DR_Model_data</td><td>schema</td><td>`satya_dr_catalog`.`satyendranath_sure`</td><td>create</td><td>success</td><td>Schema `satya_dr_catalog`.`satyendranath_sure` created or already exists.</td><td>2025-08-29T18:34:43.994454Z</td><td>List()</td></tr><tr><td>DR_Model_data</td><td>catalog</td><td>`satya_dr_catalog`</td><td>create</td><td>success</td><td>Catalog `satya_dr_catalog` created or already exists.</td><td>2025-08-29T18:34:41.963747Z</td><td>List()</td></tr><tr><td>DR_Model_data</td><td>catalog_pair</td><td>Processing of test_not:dr_test_not</td><td>unhandled_error</td><td>error</td><td>An unhandled error occurred: [NO_SUCH_CATALOG_EXCEPTION] Catalog 'test_not' was not found. Please verify the catalog name and then retry the query or command again. SQLSTATE: 42704\n",
       "\n",
       "JVM stacktrace:\n",
       "org.apache.spark.sql.catalyst.analysis.NoSuchCatalogException\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$listSchemasProto$1(ManagedCatalogClientImpl.scala:1328)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$recordAndWrapExceptionBase$2(ManagedCatalogClientImpl.scala:7398)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$recordAndWrapExceptionBase$1(ManagedCatalogClientImpl.scala:7397)\n",
       "\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:66)\n",
       "\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:63)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:251)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapExceptionBase(ManagedCatalogClientImpl.scala:7378)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:7364)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.listSchemasProto(ManagedCatalogClientImpl.scala:1293)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.listSchemas(ManagedCatalogClientImpl.scala:1279)\n",
       "\tat com.databricks.sql.managedcatalog.ManagedCatalogCommon.listSchemas(ManagedCatalogCommon.scala:827)\n",
       "\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.$anonfun$listSchemas$1(ProfiledManagedCatalog.scala:168)\n",
       "\tat org.apache.spark.sql.catalyst.MetricKeyUtils$.measure(MetricKey.scala:1709)\n",
       "\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.$anonfun$profile$1(ProfiledManagedCatalog.scala:64)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.profile(ProfiledManagedCatalog.scala:63)\n",
       "\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.listSchemas(ProfiledManagedCatalog.scala:168)\n",
       "\tat com.databricks.sql.managedcatalog.ManagedCatalogSessionCatalog.listDatabasesWithCatalog(ManagedCatalogSessionCatalog.scala:908)\n",
       "\tat com.databricks.sql.managedcatalog.UnityCatalogV2Proxy.listNamespaces(UnityCatalogV2Proxy.scala:141)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.ShowNamespacesExec.run(ShowNamespacesExec.scala:42)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.$anonfun$result$2(V2CommandExec.scala:48)\n",
       "\tat org.apache.spark.sql.execution.SparkPlan.runCommandInAetherOrSpark(SparkPlan.scala:194)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.$anonfun$result$1(V2CommandExec.scala:48)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:47)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:45)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:56)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$5(QueryExecution.scala:469)\n",
       "\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$4(QueryExecution.scala:469)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:219)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$3(QueryExecution.scala:468)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$11(SQLExecution.scala:479)\n",
       "\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$10(SQLExecution.scala:415)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:779)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:345)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:214)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:716)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:464)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1342)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:460)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.withMVTagsIfNecessary(QueryExecution.scala:389)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:458)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:537)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:532)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:521)\n",
       "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:521)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:42)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:361)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:357)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:497)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$8(QueryExecution.scala:532)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:532)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:345)\n",
       "\tat scala.util.Try$.apply(Try.scala:213)\n",
       "\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)\n",
       "\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)\n",
       "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:350)\n",
       "\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:406)\n",
       "\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$3(Dataset.scala:150)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n",
       "\tat org.apache.spark.sql.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1469)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1469)\n",
       "\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:141)\n",
       "\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:1142)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n",
       "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:1094)\n",
       "\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.executeSQL(SparkConnectPlanner.scala:3606)\n",
       "\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleSqlCommand(SparkConnectPlanner.scala:3435)\n",
       "\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3370)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:413)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:312)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:233)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:464)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:464)\n",
       "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n",
       "\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:90)\n",
       "\tat org.apache.spark.util.Utils$.withContextClassLoader(Utils.scala:241)\n",
       "\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:89)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:463)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:233)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:139)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:614)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n",
       "\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n",
       "\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n",
       "\tat scala.util.Using$.resource(Using.scala:269)\n",
       "\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:614)\n",
       "Traceback (most recent call last):\n",
       "  File \"/home/spark-45bdfff6-00b1-4911-af55-ac/.ipykernel/1943/command-8748648698932923-2826314312\", line 295, in main\n",
       "    source_schemas_df = spark.sql(f\"SHOW SCHEMAS IN `{source_catalog}`\")\n",
       "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
       "  File \"/databricks/spark/python/pyspark/sql/connect/session.py\", line 821, in sql\n",
       "    data, properties, ei = self.client.execute_command(cmd.command(self._client))\n",
       "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
       "  File \"/databricks/spark/python/pyspark/sql/connect/client/core.py\", line 1481, in execute_command\n",
       "    data, _, metrics, observed_metrics, properties = self._execute_and_fetch(\n",
       "                                                     ^^^^^^^^^^^^^^^^^^^^^^^^\n",
       "  File \"/databricks/spark/python/pyspark/sql/connect/client/core.py\", line 1970, in _execute_and_fetch\n",
       "    for response in self._execute_and_fetch_as_iterator(\n",
       "  File \"/databricks/spark/python/pyspark/sql/connect/client/core.py\", line 1946, in _execute_and_fetch_as_iterator\n",
       "    self._handle_error(error)\n",
       "  File \"/databricks/spark/python/pyspark/sql/connect/client/core.py\", line 2266, in _handle_error\n",
       "    self._handle_rpc_error(error)\n",
       "  File \"/databricks/spark/python/pyspark/sql/connect/client/core.py\", line 2347, in _handle_rpc_error\n",
       "    raise convert_exception(\n",
       "pyspark.errors.exceptions.connect.AnalysisException: [NO_SUCH_CATALOG_EXCEPTION] Catalog 'test_not' was not found. Please verify the catalog name and then retry the query or command again. SQLSTATE: 42704\n",
       "\n",
       "JVM stacktrace:\n",
       "org.apache.spark.sql.catalyst.analysis.NoSuchCatalogException\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$listSchemasProto$1(ManagedCatalogClientImpl.scala:1328)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$recordAndWrapExceptionBase$2(ManagedCatalogClientImpl.scala:7398)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$recordAndWrapExceptionBase$1(ManagedCatalogClientImpl.scala:7397)\n",
       "\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:66)\n",
       "\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:63)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:251)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapExceptionBase(ManagedCatalogClientImpl.scala:7378)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:7364)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.listSchemasProto(ManagedCatalogClientImpl.scala:1293)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.listSchemas(ManagedCatalogClientImpl.scala:1279)\n",
       "\tat com.databricks.sql.managedcatalog.ManagedCatalogCommon.listSchemas(ManagedCatalogCommon.scala:827)\n",
       "\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.$anonfun$listSchemas$1(ProfiledManagedCatalog.scala:168)\n",
       "\tat org.apache.spark.sql.catalyst.MetricKeyUtils$.measure(MetricKey.scala:1709)\n",
       "\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.$anonfun$profile$1(ProfiledManagedCatalog.scala:64)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.profile(ProfiledManagedCatalog.scala:63)\n",
       "\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.listSchemas(ProfiledManagedCatalog.scala:168)\n",
       "\tat com.databricks.sql.managedcatalog.ManagedCatalogSessionCatalog.listDatabasesWithCatalog(ManagedCatalogSessionCatalog.scala:908)\n",
       "\tat com.databricks.sql.managedcatalog.UnityCatalogV2Proxy.listNamespaces(UnityCatalogV2Proxy.scala:141)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.ShowNamespacesExec.run(ShowNamespacesExec.scala:42)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.$anonfun$result$2(V2CommandExec.scala:48)\n",
       "\tat org.apache.spark.sql.execution.SparkPlan.runCommandInAetherOrSpark(SparkPlan.scala:194)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.$anonfun$result$1(V2CommandExec.scala:48)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:47)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:45)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:56)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$5(QueryExecution.scala:469)\n",
       "\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$4(QueryExecution.scala:469)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:219)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$3(QueryExecution.scala:468)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$11(SQLExecution.scala:479)\n",
       "\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$10(SQLExecution.scala:415)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:779)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:345)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:214)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:716)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:464)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1342)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:460)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.withMVTagsIfNecessary(QueryExecution.scala:389)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:458)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:537)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:532)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:521)\n",
       "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:521)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:42)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:361)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:357)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:497)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$8(QueryExecution.scala:532)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:532)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:345)\n",
       "\tat scala.util.Try$.apply(Try.scala:213)\n",
       "\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)\n",
       "\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)\n",
       "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:350)\n",
       "\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:406)\n",
       "\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$3(Dataset.scala:150)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n",
       "\tat org.apache.spark.sql.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1469)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1469)\n",
       "\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:141)\n",
       "\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:1142)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n",
       "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:1094)\n",
       "\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.executeSQL(SparkConnectPlanner.scala:3606)\n",
       "\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleSqlCommand(SparkConnectPlanner.scala:3435)\n",
       "\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3370)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:413)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:312)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:233)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:464)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:464)\n",
       "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n",
       "\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:90)\n",
       "\tat org.apache.spark.util.Utils$.withContextClassLoader(Utils.scala:241)\n",
       "\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:89)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:463)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:233)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:139)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:614)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n",
       "\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n",
       "\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n",
       "\tat scala.util.Using$.resource(Using.scala:269)\n",
       "\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:614)\n",
       "</td><td>2025-08-29T18:34:40.618934Z</td><td>List()</td></tr><tr><td>DR_Model_data</td><td>catalog</td><td>`dr_test_not`</td><td>create</td><td>success</td><td>Catalog `dr_test_not` created or already exists.</td><td>2025-08-29T18:34:38.912022Z</td><td>List()</td></tr><tr><td>DR_Model_data</td><td>schema</td><td>`dr_accuweather`.`forecast`</td><td>create</td><td>success</td><td>Schema `dr_accuweather`.`forecast` created or already exists.</td><td>2025-08-29T18:34:37.40092Z</td><td>List()</td></tr><tr><td>DR_Model_data</td><td>catalog</td><td>`dr_accuweather`</td><td>create</td><td>success</td><td>Catalog `dr_accuweather` created or already exists.</td><td>2025-08-29T18:34:24.325473Z</td><td>List()</td></tr><tr><td>DR_Model_data</td><td>schema</td><td>`dr_eo_amperity`.`silver`</td><td>create</td><td>success</td><td>Schema `dr_eo_amperity`.`silver` created or already exists.</td><td>2025-08-29T18:34:22.782756Z</td><td>List()</td></tr><tr><td>DR_Model_data</td><td>schema</td><td>`dr_eo_amperity`.`default`</td><td>create</td><td>success</td><td>Schema `dr_eo_amperity`.`default` created or already exists.</td><td>2025-08-29T18:34:21.270727Z</td><td>List()</td></tr><tr><td>DR_Model_data</td><td>schema</td><td>`dr_eo_amperity`.`bronze`</td><td>create</td><td>success</td><td>Schema `dr_eo_amperity`.`bronze` created or already exists.</td><td>2025-08-29T18:34:19.658793Z</td><td>List()</td></tr><tr><td>DR_Model_data</td><td>catalog</td><td>`dr_eo_amperity`</td><td>create</td><td>success</td><td>Catalog `dr_eo_amperity` created or already exists.</td><td>2025-08-29T18:33:57.274619Z</td><td>List()</td></tr><tr><td>DR_Model_data</td><td>schema</td><td>`satya_dr_catalog`.`satyendranath_sure`</td><td>create</td><td>success</td><td>Schema `satya_dr_catalog`.`satyendranath_sure` created or already exists.</td><td>2025-08-20T15:48:14.916583Z</td><td>List()</td></tr><tr><td>DR_Model_data</td><td>catalog</td><td>`satya_dr_catalog`</td><td>create</td><td>success</td><td>Catalog `satya_dr_catalog` created or already exists.</td><td>2025-08-20T15:48:12.815245Z</td><td>List()</td></tr><tr><td>DR_Model_data</td><td>catalog_pair</td><td>Processing of test_not:dr_test_not</td><td>unhandled_error</td><td>error</td><td>An unhandled error occurred: [NO_SUCH_CATALOG_EXCEPTION] Catalog 'test_not' was not found. Please verify the catalog name and then retry the query or command again. SQLSTATE: 42704\n",
       "\n",
       "JVM stacktrace:\n",
       "org.apache.spark.sql.catalyst.analysis.NoSuchCatalogException\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$listSchemasProto$1(ManagedCatalogClientImpl.scala:1328)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$recordAndWrapExceptionBase$2(ManagedCatalogClientImpl.scala:7398)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$recordAndWrapExceptionBase$1(ManagedCatalogClientImpl.scala:7397)\n",
       "\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:66)\n",
       "\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:63)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:251)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapExceptionBase(ManagedCatalogClientImpl.scala:7378)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:7364)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.listSchemasProto(ManagedCatalogClientImpl.scala:1293)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.listSchemas(ManagedCatalogClientImpl.scala:1279)\n",
       "\tat com.databricks.sql.managedcatalog.ManagedCatalogCommon.listSchemas(ManagedCatalogCommon.scala:821)\n",
       "\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.$anonfun$listSchemas$1(ProfiledManagedCatalog.scala:168)\n",
       "\tat org.apache.spark.sql.catalyst.MetricKeyUtils$.measure(MetricKey.scala:1688)\n",
       "\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.$anonfun$profile$1(ProfiledManagedCatalog.scala:64)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.profile(ProfiledManagedCatalog.scala:63)\n",
       "\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.listSchemas(ProfiledManagedCatalog.scala:168)\n",
       "\tat com.databricks.sql.managedcatalog.ManagedCatalogSessionCatalog.listDatabasesWithCatalog(ManagedCatalogSessionCatalog.scala:908)\n",
       "\tat com.databricks.sql.managedcatalog.UnityCatalogV2Proxy.listNamespaces(UnityCatalogV2Proxy.scala:141)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.ShowNamespacesExec.run(ShowNamespacesExec.scala:42)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.$anonfun$result$2(V2CommandExec.scala:48)\n",
       "\tat org.apache.spark.sql.execution.SparkPlan.runCommandInAetherOrSpark(SparkPlan.scala:194)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.$anonfun$result$1(V2CommandExec.scala:48)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:47)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:45)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:56)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$5(QueryExecution.scala:450)\n",
       "\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$4(QueryExecution.scala:450)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:219)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$3(QueryExecution.scala:449)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$11(SQLExecution.scala:478)\n",
       "\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$10(SQLExecution.scala:414)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:777)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:344)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:213)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:714)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:445)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1307)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:441)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.withMVTagsIfNecessary(QueryExecution.scala:370)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:439)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:503)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:498)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:521)\n",
       "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:521)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:42)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:361)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:357)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:497)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$8(QueryExecution.scala:498)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:498)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:326)\n",
       "\tat scala.util.Try$.apply(Try.scala:213)\n",
       "\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)\n",
       "\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)\n",
       "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:331)\n",
       "\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:406)\n",
       "\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$3(Dataset.scala:150)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n",
       "\tat org.apache.spark.sql.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1469)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1469)\n",
       "\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:141)\n",
       "\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:1142)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n",
       "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:1094)\n",
       "\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.executeSQL(SparkConnectPlanner.scala:3606)\n",
       "\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleSqlCommand(SparkConnectPlanner.scala:3435)\n",
       "\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3370)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:413)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:312)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:233)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:464)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:464)\n",
       "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n",
       "\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:90)\n",
       "\tat org.apache.spark.util.Utils$.withContextClassLoader(Utils.scala:241)\n",
       "\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:89)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:463)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:233)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:139)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:614)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n",
       "\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n",
       "\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n",
       "\tat scala.util.Using$.resource(Using.scala:269)\n",
       "\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:614)\n",
       "Traceback (most recent call last):\n",
       "  File \"/home/spark-99074ce5-7219-4ba1-80ca-d5/.ipykernel/2410/command-8748648698932923-2826314312\", line 295, in main\n",
       "    source_schemas_df = spark.sql(f\"SHOW SCHEMAS IN `{source_catalog}`\")\n",
       "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
       "  File \"/databricks/spark/python/pyspark/sql/connect/session.py\", line 821, in sql\n",
       "    data, properties, ei = self.client.execute_command(cmd.command(self._client))\n",
       "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
       "  File \"/databricks/spark/python/pyspark/sql/connect/client/core.py\", line 1481, in execute_command\n",
       "    data, _, metrics, observed_metrics, properties = self._execute_and_fetch(\n",
       "                                                     ^^^^^^^^^^^^^^^^^^^^^^^^\n",
       "  File \"/databricks/spark/python/pyspark/sql/connect/client/core.py\", line 1970, in _execute_and_fetch\n",
       "    for response in self._execute_and_fetch_as_iterator(\n",
       "  File \"/databricks/spark/python/pyspark/sql/connect/client/core.py\", line 1946, in _execute_and_fetch_as_iterator\n",
       "    self._handle_error(error)\n",
       "  File \"/databricks/spark/python/pyspark/sql/connect/client/core.py\", line 2266, in _handle_error\n",
       "    self._handle_rpc_error(error)\n",
       "  File \"/databricks/spark/python/pyspark/sql/connect/client/core.py\", line 2377, in _handle_rpc_error\n",
       "    raise convert_exception(\n",
       "pyspark.errors.exceptions.connect.AnalysisException: [NO_SUCH_CATALOG_EXCEPTION] Catalog 'test_not' was not found. Please verify the catalog name and then retry the query or command again. SQLSTATE: 42704\n",
       "\n",
       "JVM stacktrace:\n",
       "org.apache.spark.sql.catalyst.analysis.NoSuchCatalogException\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$listSchemasProto$1(ManagedCatalogClientImpl.scala:1328)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$recordAndWrapExceptionBase$2(ManagedCatalogClientImpl.scala:7398)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$recordAndWrapExceptionBase$1(ManagedCatalogClientImpl.scala:7397)\n",
       "\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:66)\n",
       "\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:63)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:251)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapExceptionBase(ManagedCatalogClientImpl.scala:7378)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:7364)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.listSchemasProto(ManagedCatalogClientImpl.scala:1293)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.listSchemas(ManagedCatalogClientImpl.scala:1279)\n",
       "\tat com.databricks.sql.managedcatalog.ManagedCatalogCommon.listSchemas(ManagedCatalogCommon.scala:821)\n",
       "\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.$anonfun$listSchemas$1(ProfiledManagedCatalog.scala:168)\n",
       "\tat org.apache.spark.sql.catalyst.MetricKeyUtils$.measure(MetricKey.scala:1688)\n",
       "\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.$anonfun$profile$1(ProfiledManagedCatalog.scala:64)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.profile(ProfiledManagedCatalog.scala:63)\n",
       "\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.listSchemas(ProfiledManagedCatalog.scala:168)\n",
       "\tat com.databricks.sql.managedcatalog.ManagedCatalogSessionCatalog.listDatabasesWithCatalog(ManagedCatalogSessionCatalog.scala:908)\n",
       "\tat com.databricks.sql.managedcatalog.UnityCatalogV2Proxy.listNamespaces(UnityCatalogV2Proxy.scala:141)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.ShowNamespacesExec.run(ShowNamespacesExec.scala:42)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.$anonfun$result$2(V2CommandExec.scala:48)\n",
       "\tat org.apache.spark.sql.execution.SparkPlan.runCommandInAetherOrSpark(SparkPlan.scala:194)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.$anonfun$result$1(V2CommandExec.scala:48)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:47)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:45)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:56)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$5(QueryExecution.scala:450)\n",
       "\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$4(QueryExecution.scala:450)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:219)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$3(QueryExecution.scala:449)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$11(SQLExecution.scala:478)\n",
       "\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$10(SQLExecution.scala:414)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:777)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:344)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:213)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:714)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:445)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1307)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:441)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.withMVTagsIfNecessary(QueryExecution.scala:370)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:439)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:503)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:498)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:521)\n",
       "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:521)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:42)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:361)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:357)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:497)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$8(QueryExecution.scala:498)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:498)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:326)\n",
       "\tat scala.util.Try$.apply(Try.scala:213)\n",
       "\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)\n",
       "\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)\n",
       "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:331)\n",
       "\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:406)\n",
       "\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$3(Dataset.scala:150)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n",
       "\tat org.apache.spark.sql.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1469)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1469)\n",
       "\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:141)\n",
       "\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:1142)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n",
       "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:1094)\n",
       "\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.executeSQL(SparkConnectPlanner.scala:3606)\n",
       "\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleSqlCommand(SparkConnectPlanner.scala:3435)\n",
       "\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3370)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:413)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:312)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:233)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:464)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:464)\n",
       "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n",
       "\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:90)\n",
       "\tat org.apache.spark.util.Utils$.withContextClassLoader(Utils.scala:241)\n",
       "\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:89)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:463)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:233)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:139)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:614)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n",
       "\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n",
       "\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n",
       "\tat scala.util.Using$.resource(Using.scala:269)\n",
       "\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:614)\n",
       "</td><td>2025-08-20T15:48:11.398218Z</td><td>List()</td></tr><tr><td>DR_Model_data</td><td>catalog</td><td>`dr_test_not`</td><td>create</td><td>success</td><td>Catalog `dr_test_not` created or already exists.</td><td>2025-08-20T15:48:10.072018Z</td><td>List()</td></tr><tr><td>DR_Model_data</td><td>schema</td><td>`dr_accuweather`.`forecast`</td><td>create</td><td>success</td><td>Schema `dr_accuweather`.`forecast` created or already exists.</td><td>2025-08-20T15:48:08.592083Z</td><td>List()</td></tr><tr><td>DR_Model_data</td><td>catalog</td><td>`dr_accuweather`</td><td>create</td><td>success</td><td>Catalog `dr_accuweather` created or already exists.</td><td>2025-08-20T15:48:06.657797Z</td><td>List()</td></tr><tr><td>DR_Model_data</td><td>schema</td><td>`dr_eo_amperity`.`silver`</td><td>create</td><td>success</td><td>Schema `dr_eo_amperity`.`silver` created or already exists.</td><td>2025-08-20T15:48:05.019282Z</td><td>List()</td></tr><tr><td>DR_Model_data</td><td>schema</td><td>`dr_eo_amperity`.`default`</td><td>create</td><td>success</td><td>Schema `dr_eo_amperity`.`default` created or already exists.</td><td>2025-08-20T15:48:03.382008Z</td><td>List()</td></tr><tr><td>DR_Model_data</td><td>schema</td><td>`dr_eo_amperity`.`bronze`</td><td>create</td><td>success</td><td>Schema `dr_eo_amperity`.`bronze` created or already exists.</td><td>2025-08-20T15:48:01.785224Z</td><td>List()</td></tr><tr><td>DR_Model_data</td><td>catalog</td><td>`dr_eo_amperity`</td><td>create</td><td>success</td><td>Catalog `dr_eo_amperity` created or already exists.</td><td>2025-08-20T15:47:59.541252Z</td><td>List()</td></tr><tr><td>DR_Model_data</td><td>model_alias</td><td>satya_dr_catalog.satyendranath_sure.satya_model/2/ver2</td><td>set</td><td>success</td><td>Successfully set alias 'ver2' on model version 2.</td><td>2025-08-20T15:35:49.381423Z</td><td>List(List(model_name, satya_dr_catalog.satyendranath_sure.satya_model), List(version, 2), List(alias, ver2))</td></tr><tr><td>DR_Model_data</td><td>model_version</td><td>satya_dr_catalog.satyendranath_sure.satya_model/2</td><td>copy</td><td>success</td><td>Successfully copied model version 2 from satya_share.satyendranath_sure.satya_model to satya_dr_catalog.satyendranath_sure.satya_model.</td><td>2025-08-20T15:35:47.627569Z</td><td>List(List(copy_status, success), List(source_uri, models:/satya_share.satyendranath_sure.satya_model/2), List(target_uri, satya_dr_catalog.satyendranath_sure.satya_model), List(version_copied, 2), List(version_result, <ModelVersion: aliases=[], creation_timestamp=1755704146673, current_stage=None, description='Comment2', last_updated_timestamp=1755704147398, name='satya_dr_catalog.satyendranath_sure.satya_model', run_id='', run_link=None, source='models:/satya_share.satyendranath_sure.satya_model/2', status='READY', status_message='', tags={}, user_id='satyendranath.sure@databricks.com', version='2'>))</td></tr><tr><td>DR_Model_data</td><td>model_alias</td><td>satya_dr_catalog.satyendranath_sure.satya_model/1/ver1</td><td>set</td><td>success</td><td>Successfully set alias 'ver1' on model version 1.</td><td>2025-08-20T15:35:42.192633Z</td><td>List(List(model_name, satya_dr_catalog.satyendranath_sure.satya_model), List(version, 1), List(alias, ver1))</td></tr><tr><td>DR_Model_data</td><td>model_version</td><td>satya_dr_catalog.satyendranath_sure.satya_model/1</td><td>copy</td><td>success</td><td>Successfully copied model version 1 from satya_share.satyendranath_sure.satya_model to satya_dr_catalog.satyendranath_sure.satya_model.</td><td>2025-08-20T15:35:40.299971Z</td><td>List(List(copy_status, success), List(source_uri, models:/satya_share.satyendranath_sure.satya_model/1), List(target_uri, satya_dr_catalog.satyendranath_sure.satya_model), List(version_copied, 1), List(version_result, <ModelVersion: aliases=[], creation_timestamp=1755704138700, current_stage=None, description='Comment1', last_updated_timestamp=1755704140051, name='satya_dr_catalog.satyendranath_sure.satya_model', run_id='', run_link=None, source='models:/satya_share.satyendranath_sure.satya_model/1', status='READY', status_message='', tags={}, user_id='satyendranath.sure@databricks.com', version='1'>))</td></tr><tr><td>DR_Model_data</td><td>schema</td><td>`satya_dr_catalog`.`satyendranath_sure`</td><td>create</td><td>success</td><td>Schema `satya_dr_catalog`.`satyendranath_sure` created or already exists.</td><td>2025-08-20T15:35:32.68967Z</td><td>List()</td></tr><tr><td>DR_Model_data</td><td>catalog</td><td>`satya_dr_catalog`</td><td>create</td><td>success</td><td>Catalog `satya_dr_catalog` created or already exists.</td><td>2025-08-20T15:35:30.217706Z</td><td>List()</td></tr><tr><td>DR_Model_data</td><td>catalog_pair</td><td>Processing of test_not:dr_test_not</td><td>unhandled_error</td><td>error</td><td>An unhandled error occurred: [NO_SUCH_CATALOG_EXCEPTION] Catalog 'test_not' was not found. Please verify the catalog name and then retry the query or command again. SQLSTATE: 42704\n",
       "\n",
       "JVM stacktrace:\n",
       "org.apache.spark.sql.catalyst.analysis.NoSuchCatalogException\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$listSchemasProto$1(ManagedCatalogClientImpl.scala:1328)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$recordAndWrapExceptionBase$2(ManagedCatalogClientImpl.scala:7398)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$recordAndWrapExceptionBase$1(ManagedCatalogClientImpl.scala:7397)\n",
       "\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:66)\n",
       "\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:63)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:251)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapExceptionBase(ManagedCatalogClientImpl.scala:7378)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:7364)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.listSchemasProto(ManagedCatalogClientImpl.scala:1293)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.listSchemas(ManagedCatalogClientImpl.scala:1279)\n",
       "\tat com.databricks.sql.managedcatalog.ManagedCatalogCommon.listSchemas(ManagedCatalogCommon.scala:821)\n",
       "\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.$anonfun$listSchemas$1(ProfiledManagedCatalog.scala:168)\n",
       "\tat org.apache.spark.sql.catalyst.MetricKeyUtils$.measure(MetricKey.scala:1688)\n",
       "\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.$anonfun$profile$1(ProfiledManagedCatalog.scala:64)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.profile(ProfiledManagedCatalog.scala:63)\n",
       "\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.listSchemas(ProfiledManagedCatalog.scala:168)\n",
       "\tat com.databricks.sql.managedcatalog.ManagedCatalogSessionCatalog.listDatabasesWithCatalog(ManagedCatalogSessionCatalog.scala:908)\n",
       "\tat com.databricks.sql.managedcatalog.UnityCatalogV2Proxy.listNamespaces(UnityCatalogV2Proxy.scala:141)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.ShowNamespacesExec.run(ShowNamespacesExec.scala:42)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.$anonfun$result$2(V2CommandExec.scala:48)\n",
       "\tat org.apache.spark.sql.execution.SparkPlan.runCommandInAetherOrSpark(SparkPlan.scala:194)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.$anonfun$result$1(V2CommandExec.scala:48)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:47)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:45)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:56)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$5(QueryExecution.scala:450)\n",
       "\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$4(QueryExecution.scala:450)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:219)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$3(QueryExecution.scala:449)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$11(SQLExecution.scala:478)\n",
       "\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$10(SQLExecution.scala:414)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:777)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:344)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:213)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:714)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:445)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1307)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:441)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.withMVTagsIfNecessary(QueryExecution.scala:370)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:439)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:503)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:498)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:521)\n",
       "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:521)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:42)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:361)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:357)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:497)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$8(QueryExecution.scala:498)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:498)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:326)\n",
       "\tat scala.util.Try$.apply(Try.scala:213)\n",
       "\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)\n",
       "\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)\n",
       "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:331)\n",
       "\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:406)\n",
       "\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$3(Dataset.scala:150)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n",
       "\tat org.apache.spark.sql.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1469)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1469)\n",
       "\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:141)\n",
       "\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:1142)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n",
       "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:1094)\n",
       "\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.executeSQL(SparkConnectPlanner.scala:3606)\n",
       "\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleSqlCommand(SparkConnectPlanner.scala:3435)\n",
       "\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3370)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:413)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:312)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:233)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:464)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:464)\n",
       "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n",
       "\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:90)\n",
       "\tat org.apache.spark.util.Utils$.withContextClassLoader(Utils.scala:241)\n",
       "\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:89)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:463)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:233)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:139)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:614)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n",
       "\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n",
       "\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n",
       "\tat scala.util.Using$.resource(Using.scala:269)\n",
       "\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:614)\n",
       "Traceback (most recent call last):\n",
       "  File \"/home/spark-99074ce5-7219-4ba1-80ca-d5/.ipykernel/2410/command-8748648698932923-2826314312\", line 295, in main\n",
       "    source_schemas_df = spark.sql(f\"SHOW SCHEMAS IN `{source_catalog}`\")\n",
       "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
       "  File \"/databricks/spark/python/pyspark/sql/connect/session.py\", line 821, in sql\n",
       "    data, properties, ei = self.client.execute_command(cmd.command(self._client))\n",
       "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
       "  File \"/databricks/spark/python/pyspark/sql/connect/client/core.py\", line 1481, in execute_command\n",
       "    data, _, metrics, observed_metrics, properties = self._execute_and_fetch(\n",
       "                                                     ^^^^^^^^^^^^^^^^^^^^^^^^\n",
       "  File \"/databricks/spark/python/pyspark/sql/connect/client/core.py\", line 1970, in _execute_and_fetch\n",
       "    for response in self._execute_and_fetch_as_iterator(\n",
       "  File \"/databricks/spark/python/pyspark/sql/connect/client/core.py\", line 1946, in _execute_and_fetch_as_iterator\n",
       "    self._handle_error(error)\n",
       "  File \"/databricks/spark/python/pyspark/sql/connect/client/core.py\", line 2266, in _handle_error\n",
       "    self._handle_rpc_error(error)\n",
       "  File \"/databricks/spark/python/pyspark/sql/connect/client/core.py\", line 2377, in _handle_rpc_error\n",
       "    raise convert_exception(\n",
       "pyspark.errors.exceptions.connect.AnalysisException: [NO_SUCH_CATALOG_EXCEPTION] Catalog 'test_not' was not found. Please verify the catalog name and then retry the query or command again. SQLSTATE: 42704\n",
       "\n",
       "JVM stacktrace:\n",
       "org.apache.spark.sql.catalyst.analysis.NoSuchCatalogException\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$listSchemasProto$1(ManagedCatalogClientImpl.scala:1328)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$recordAndWrapExceptionBase$2(ManagedCatalogClientImpl.scala:7398)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$recordAndWrapExceptionBase$1(ManagedCatalogClientImpl.scala:7397)\n",
       "\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:66)\n",
       "\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:63)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:251)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapExceptionBase(ManagedCatalogClientImpl.scala:7378)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:7364)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.listSchemasProto(ManagedCatalogClientImpl.scala:1293)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.listSchemas(ManagedCatalogClientImpl.scala:1279)\n",
       "\tat com.databricks.sql.managedcatalog.ManagedCatalogCommon.listSchemas(ManagedCatalogCommon.scala:821)\n",
       "\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.$anonfun$listSchemas$1(ProfiledManagedCatalog.scala:168)\n",
       "\tat org.apache.spark.sql.catalyst.MetricKeyUtils$.measure(MetricKey.scala:1688)\n",
       "\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.$anonfun$profile$1(ProfiledManagedCatalog.scala:64)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.profile(ProfiledManagedCatalog.scala:63)\n",
       "\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.listSchemas(ProfiledManagedCatalog.scala:168)\n",
       "\tat com.databricks.sql.managedcatalog.ManagedCatalogSessionCatalog.listDatabasesWithCatalog(ManagedCatalogSessionCatalog.scala:908)\n",
       "\tat com.databricks.sql.managedcatalog.UnityCatalogV2Proxy.listNamespaces(UnityCatalogV2Proxy.scala:141)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.ShowNamespacesExec.run(ShowNamespacesExec.scala:42)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.$anonfun$result$2(V2CommandExec.scala:48)\n",
       "\tat org.apache.spark.sql.execution.SparkPlan.runCommandInAetherOrSpark(SparkPlan.scala:194)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.$anonfun$result$1(V2CommandExec.scala:48)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:47)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:45)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:56)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$5(QueryExecution.scala:450)\n",
       "\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$4(QueryExecution.scala:450)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:219)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$3(QueryExecution.scala:449)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$11(SQLExecution.scala:478)\n",
       "\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$10(SQLExecution.scala:414)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:777)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:344)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:213)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:714)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:445)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1307)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:441)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.withMVTagsIfNecessary(QueryExecution.scala:370)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:439)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:503)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:498)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:521)\n",
       "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:521)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:42)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:361)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:357)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:497)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$8(QueryExecution.scala:498)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:498)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:326)\n",
       "\tat scala.util.Try$.apply(Try.scala:213)\n",
       "\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)\n",
       "\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)\n",
       "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:331)\n",
       "\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:406)\n",
       "\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$3(Dataset.scala:150)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n",
       "\tat org.apache.spark.sql.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1469)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1469)\n",
       "\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:141)\n",
       "\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:1142)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n",
       "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:1094)\n",
       "\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.executeSQL(SparkConnectPlanner.scala:3606)\n",
       "\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleSqlCommand(SparkConnectPlanner.scala:3435)\n",
       "\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3370)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:413)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:312)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:233)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:464)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:464)\n",
       "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n",
       "\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:90)\n",
       "\tat org.apache.spark.util.Utils$.withContextClassLoader(Utils.scala:241)\n",
       "\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:89)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:463)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:233)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:139)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:614)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n",
       "\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n",
       "\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n",
       "\tat scala.util.Using$.resource(Using.scala:269)\n",
       "\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:614)\n",
       "</td><td>2025-08-20T15:35:28.566707Z</td><td>List()</td></tr><tr><td>DR_Model_data</td><td>catalog</td><td>`dr_test_not`</td><td>create</td><td>success</td><td>Catalog `dr_test_not` created or already exists.</td><td>2025-08-20T15:35:26.898056Z</td><td>List()</td></tr><tr><td>DR_Model_data</td><td>schema</td><td>`dr_accuweather`.`forecast`</td><td>create</td><td>success</td><td>Schema `dr_accuweather`.`forecast` created or already exists.</td><td>2025-08-20T15:35:25.247892Z</td><td>List()</td></tr><tr><td>DR_Model_data</td><td>catalog</td><td>`dr_accuweather`</td><td>create</td><td>success</td><td>Catalog `dr_accuweather` created or already exists.</td><td>2025-08-20T15:35:22.810636Z</td><td>List()</td></tr><tr><td>DR_Model_data</td><td>schema</td><td>`dr_eo_amperity`.`silver`</td><td>create</td><td>success</td><td>Schema `dr_eo_amperity`.`silver` created or already exists.</td><td>2025-08-20T15:35:20.827643Z</td><td>List()</td></tr><tr><td>DR_Model_data</td><td>schema</td><td>`dr_eo_amperity`.`default`</td><td>create</td><td>success</td><td>Schema `dr_eo_amperity`.`default` created or already exists.</td><td>2025-08-20T15:35:19.043524Z</td><td>List()</td></tr><tr><td>DR_Model_data</td><td>schema</td><td>`dr_eo_amperity`.`bronze`</td><td>create</td><td>success</td><td>Schema `dr_eo_amperity`.`bronze` created or already exists.</td><td>2025-08-20T15:35:14.099212Z</td><td>List()</td></tr><tr><td>DR_Model_data</td><td>catalog</td><td>`dr_eo_amperity`</td><td>create</td><td>success</td><td>Catalog `dr_eo_amperity` created or already exists.</td><td>2025-08-20T15:34:39.443726Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>table</td><td>`satya_dr_catalog`.`satyendranath_sure`.`satya_managed_table`</td><td>clone</td><td>success</td><td>Table `satya_dr_catalog`.`satyendranath_sure`.`satya_managed_table` cloned successfully.</td><td>2025-08-13T14:52:36.707876Z</td><td>List(List(source_table_size, 827), List(source_num_of_files, 1), List(num_of_synced_transactions, None), List(num_removed_files, 0), List(num_copied_files, 0), List(removed_files_size, 0), List(copied_files_size, 0))</td></tr><tr><td>DR_Clone_data_v2</td><td>table</td><td>`satya_dr_catalog`.`satyendranath_sure`.`satya_external_table`</td><td>clone</td><td>success</td><td>Table `satya_dr_catalog`.`satyendranath_sure`.`satya_external_table` cloned successfully.</td><td>2025-08-13T14:52:35.441812Z</td><td>List(List(source_table_size, 826), List(source_num_of_files, 1), List(num_of_synced_transactions, None), List(num_removed_files, 0), List(num_copied_files, 0), List(removed_files_size, 0), List(copied_files_size, 0))</td></tr><tr><td>DR_Clone_data_v2</td><td>table/view</td><td>`accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample`.`forecast`.`us_postal_hourly_metric`</td><td>clone/create</td><td>error</td><td>Failed to process table/view `accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample`.`forecast`.`us_postal_hourly_metric`. Error: An error occurred while calling o427.sql.\n",
       ": com.databricks.sql.managedcatalog.acl.UnauthorizedAccessException: PERMISSION_DENIED: User does not have USE CATALOG on Catalog 'accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample'.\n",
       "\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:87)\n",
       "\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:63)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:251)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapExceptionBase(ManagedCatalogClientImpl.scala:7378)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:7364)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.getTableCredentials(ManagedCatalogClientImpl.scala:3951)\n",
       "\tat com.databricks.sql.managedcatalog.ManagedCatalogClient.getTemporaryCredentials(ManagedCatalogClient.scala:2620)\n",
       "\tat com.databricks.sql.managedcatalog.ManagedCatalogClient.getTemporaryCredentials$(ManagedCatalogClient.scala:2605)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.getTemporaryCredentials(ManagedCatalogClientImpl.scala:251)\n",
       "\tat com.databricks.unity.TempCredCache.$anonfun$getInternal$7(TemporaryCredentials.scala:448)\n",
       "\tat com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4724)\n",
       "\tat com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3522)\n",
       "\tat com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2315)\n",
       "\tat com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2278)\n",
       "\tat com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2193)\n",
       "\tat com.google.common.cache.LocalCache.get(LocalCache.java:3932)\n",
       "\tat com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4721)\n",
       "\tat com.databricks.unity.TempCredCache.liftedTree1$1(TemporaryCredentials.scala:447)\n",
       "\tat com.databricks.unity.TempCredCache.getInternal(TemporaryCredentials.scala:446)\n",
       "\tat com.databricks.unity.TempCredCache.get(TemporaryCredentials.scala:373)\n",
       "\tat com.databricks.unity.UnityCredentialManager.getTemporaryCredentials(CredentialManager.scala:488)\n",
       "\tat com.databricks.unity.CredentialManager$.getTemporaryCredentials(CredentialManager.scala:881)\n",
       "\tat com.databricks.unity.SAM.setConfFromCred$1(SAM.scala:338)\n",
       "\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$8(SAM.scala:353)\n",
       "\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$8$adapted(SAM.scala:352)\n",
       "\tat com.databricks.unity.TempCredRequest$.$anonfun$withFallbackOnOverlappingForParentPath$3(TemporaryCredentials.scala:147)\n",
       "\tat scala.util.Try$.apply(Try.scala:213)\n",
       "\tat com.databricks.unity.TempCredRequest$.withFallbackOnOverlappingForParentPath(TemporaryCredentials.scala:147)\n",
       "\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$7(SAM.scala:352)\n",
       "\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$7$adapted(SAM.scala:350)\n",
       "\tat scala.Option.map(Option.scala:230)\n",
       "\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$6(SAM.scala:350)\n",
       "\tat scala.Option.flatMap(Option.scala:271)\n",
       "\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$1(SAM.scala:349)\n",
       "\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$1$adapted(SAM.scala:336)\n",
       "\tat scala.Option.foreach(Option.scala:407)\n",
       "\tat com.databricks.unity.SAM.setFileSystemConfigs(SAM.scala:336)\n",
       "\tat com.databricks.unity.SAM.createDelegate(SAM.scala:388)\n",
       "\tat com.databricks.unity.SAM.createDelegate$(SAM.scala:372)\n",
       "\tat com.databricks.unity.TableSAM.createDelegate(SAM.scala:695)\n",
       "\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.createDelegate(CredentialScopeFileSystem.scala:74)\n",
       "\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.$anonfun$setDelegates$1(CredentialScopeFileSystem.scala:127)\n",
       "\tat com.databricks.sql.acl.fs.Lazy.apply(DelegatingFileSystem.scala:310)\n",
       "\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.readDelegateMakeQualified(CredentialScopeFileSystem.scala:282)\n",
       "\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.optimizedMakeQualifiedPath(CredentialScopeFileSystem.scala:277)\n",
       "\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.makeQualified(CredentialScopeFileSystem.scala:294)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaLog$.apply(DeltaLog.scala:1284)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaLog$.forTable(DeltaLog.scala:1087)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$5(DeltaTableV2.scala:175)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$withAdditionalSnapshotInitializationUsageLogData$1(DeltaLog.scala:968)\n",
       "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
       "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
       "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaLog$.withAdditionalSnapshotInitializationUsageLogData(DeltaLog.scala:969)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$2(DeltaTableV2.scala:169)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:700)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$1(DeltaTableV2.scala:150)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:83)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.deltaLog$lzycompute(DeltaTableV2.scala:150)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.deltaLog(DeltaTableV2.scala:148)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$8(DeltaTableV2.scala:264)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$withAdditionalSnapshotInitializationUsageLogData$1(DeltaLog.scala:968)\n",
       "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
       "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
       "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaLog$.withAdditionalSnapshotInitializationUsageLogData(DeltaLog.scala:969)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$7(DeltaTableV2.scala:263)\n",
       "\tat scala.Option.orElse(Option.scala:447)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$5(DeltaTableV2.scala:263)\n",
       "\tat scala.Option.orElse(Option.scala:447)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$2(DeltaTableV2.scala:260)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:83)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$1(DeltaTableV2.scala:290)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:700)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot$lzycompute(DeltaTableV2.scala:240)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot(DeltaTableV2.scala:238)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$tableSchema$2(DeltaTableV2.scala:317)\n",
       "\tat scala.Option.getOrElse(Option.scala:189)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.tableSchema$lzycompute(DeltaTableV2.scala:317)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.tableSchema(DeltaTableV2.scala:315)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.schema(DeltaTableV2.scala:322)\n",
       "\tat org.apache.spark.sql.connector.catalog.Table.columns(Table.java:68)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.columns(DeltaTableV2.scala:83)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.ResolvedTable$.create(v2ResolutionPlans.scala:231)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$4(Analyzer.scala:1600)\n",
       "\tat scala.Option.map(Option.scala:230)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$3(Analyzer.scala:1592)\n",
       "\tat scala.Option.orElse(Option.scala:447)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableOrView(Analyzer.scala:1587)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1556)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1466)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:182)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:171)\n",
       "\tat scala.PartialFunction.applyOrElse(PartialFunction.scala:127)\n",
       "\tat scala.PartialFunction.applyOrElse$(PartialFunction.scala:126)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.applyOrElse(AnalysisHelper.scala:171)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)\n",
       "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:138)\n",
       "\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1330)\n",
       "\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1329)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.DescribeRelation.mapChildren(v2Commands.scala:726)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning(AnalysisHelper.scala:186)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning$(AnalysisHelper.scala:167)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithSubqueriesAndPruning(LogicalPlan.scala:42)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.resolveOperatorsUpWithSubqueriesAndPruningByConf(Analyzer.scala:1674)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply0(Analyzer.scala:1466)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1419)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1365)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:480)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:629)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:613)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:131)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:480)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:479)\n",
       "\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n",
       "\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n",
       "\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:475)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:452)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22(RuleExecutor.scala:585)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22$adapted(RuleExecutor.scala:585)\n",
       "\tat scala.collection.immutable.List.foreach(List.scala:431)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:585)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:349)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:498)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:491)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:397)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:491)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:416)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:341)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:219)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:341)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:252)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:96)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:131)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:87)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:478)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:478)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:294)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:548)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:668)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:151)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:668)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1307)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:661)\n",
       "\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:658)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:658)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:288)\n",
       "\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:287)\n",
       "\tat scala.util.Try$.apply(Try.scala:213)\n",
       "\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)\n",
       "\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)\n",
       "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:321)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:267)\n",
       "\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$3(Dataset.scala:149)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n",
       "\tat org.apache.spark.sql.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1469)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1469)\n",
       "\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:141)\n",
       "\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:1048)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n",
       "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:1001)\n",
       "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:1071)\n",
       "\tat jdk.internal.reflect.GeneratedMethodAccessor593.invoke(Unknown Source)\n",
       "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
       "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
       "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n",
       "\tat py4j.Gateway.invoke(Gateway.java:306)\n",
       "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
       "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:197)\n",
       "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:117)\n",
       "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
       "\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\n",
       "\t\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:87)\n",
       "\t\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:63)\n",
       "\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:251)\n",
       "\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapExceptionBase(ManagedCatalogClientImpl.scala:7378)\n",
       "\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:7364)\n",
       "\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.getTableCredentials(ManagedCatalogClientImpl.scala:3951)\n",
       "\t\tat com.databricks.sql.managedcatalog.ManagedCatalogClient.getTemporaryCredentials(ManagedCatalogClient.scala:2620)\n",
       "\t\tat com.databricks.sql.managedcatalog.ManagedCatalogClient.getTemporaryCredentials$(ManagedCatalogClient.scala:2605)\n",
       "\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.getTemporaryCredentials(ManagedCatalogClientImpl.scala:251)\n",
       "\t\tat com.databricks.unity.TempCredCache.$anonfun$getInternal$7(TemporaryCredentials.scala:448)\n",
       "\t\tat com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4724)\n",
       "\t\tat com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3522)\n",
       "\t\tat com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2315)\n",
       "\t\tat com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2278)\n",
       "\t\tat com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2193)\n",
       "\t\tat com.google.common.cache.LocalCache.get(LocalCache.java:3932)\n",
       "\t\tat com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4721)\n",
       "\t\tat com.databricks.unity.TempCredCache.liftedTree1$1(TemporaryCredentials.scala:447)\n",
       "\t\tat com.databricks.unity.TempCredCache.getInternal(TemporaryCredentials.scala:446)\n",
       "\t\tat com.databricks.unity.TempCredCache.get(TemporaryCredentials.scala:373)\n",
       "\t\tat com.databricks.unity.UnityCredentialManager.getTemporaryCredentials(CredentialManager.scala:488)\n",
       "\t\tat com.databricks.unity.CredentialManager$.getTemporaryCredentials(CredentialManager.scala:881)\n",
       "\t\tat com.databricks.unity.SAM.setConfFromCred$1(SAM.scala:338)\n",
       "\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$8(SAM.scala:353)\n",
       "\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$8$adapted(SAM.scala:352)\n",
       "\t\tat com.databricks.unity.TempCredRequest$.$anonfun$withFallbackOnOverlappingForParentPath$3(TemporaryCredentials.scala:147)\n",
       "\t\tat scala.util.Try$.apply(Try.scala:213)\n",
       "\t\tat com.databricks.unity.TempCredRequest$.withFallbackOnOverlappingForParentPath(TemporaryCredentials.scala:147)\n",
       "\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$7(SAM.scala:352)\n",
       "\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$7$adapted(SAM.scala:350)\n",
       "\t\tat scala.Option.map(Option.scala:230)\n",
       "\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$6(SAM.scala:350)\n",
       "\t\tat scala.Option.flatMap(Option.scala:271)\n",
       "\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$1(SAM.scala:349)\n",
       "\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$1$adapted(SAM.scala:336)\n",
       "\t\tat scala.Option.foreach(Option.scala:407)\n",
       "\t\tat com.databricks.unity.SAM.setFileSystemConfigs(SAM.scala:336)\n",
       "\t\tat com.databricks.unity.SAM.createDelegate(SAM.scala:388)\n",
       "\t\tat com.databricks.unity.SAM.createDelegate$(SAM.scala:372)\n",
       "\t\tat com.databricks.unity.TableSAM.createDelegate(SAM.scala:695)\n",
       "\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.createDelegate(CredentialScopeFileSystem.scala:74)\n",
       "\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.$anonfun$setDelegates$1(CredentialScopeFileSystem.scala:127)\n",
       "\t\tat com.databricks.sql.acl.fs.Lazy.apply(DelegatingFileSystem.scala:310)\n",
       "\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.readDelegateMakeQualified(CredentialScopeFileSystem.scala:282)\n",
       "\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.optimizedMakeQualifiedPath(CredentialScopeFileSystem.scala:277)\n",
       "\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.makeQualified(CredentialScopeFileSystem.scala:294)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.apply(DeltaLog.scala:1284)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.forTable(DeltaLog.scala:1087)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$5(DeltaTableV2.scala:175)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$withAdditionalSnapshotInitializationUsageLogData$1(DeltaLog.scala:968)\n",
       "\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
       "\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
       "\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.withAdditionalSnapshotInitializationUsageLogData(DeltaLog.scala:969)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$2(DeltaTableV2.scala:169)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:700)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$1(DeltaTableV2.scala:150)\n",
       "\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:83)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.deltaLog$lzycompute(DeltaTableV2.scala:150)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.deltaLog(DeltaTableV2.scala:148)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$8(DeltaTableV2.scala:264)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$withAdditionalSnapshotInitializationUsageLogData$1(DeltaLog.scala:968)\n",
       "\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
       "\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
       "\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.withAdditionalSnapshotInitializationUsageLogData(DeltaLog.scala:969)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$7(DeltaTableV2.scala:263)\n",
       "\t\tat scala.Option.orElse(Option.scala:447)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$5(DeltaTableV2.scala:263)\n",
       "\t\tat scala.Option.orElse(Option.scala:447)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$2(DeltaTableV2.scala:260)\n",
       "\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:83)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$1(DeltaTableV2.scala:290)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:700)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot$lzycompute(DeltaTableV2.scala:240)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot(DeltaTableV2.scala:238)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$tableSchema$2(DeltaTableV2.scala:317)\n",
       "\t\tat scala.Option.getOrElse(Option.scala:189)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.tableSchema$lzycompute(DeltaTableV2.scala:317)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.tableSchema(DeltaTableV2.scala:315)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.schema(DeltaTableV2.scala:322)\n",
       "\t\tat org.apache.spark.sql.connector.catalog.Table.columns(Table.java:68)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.columns(DeltaTableV2.scala:83)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.ResolvedTable$.create(v2ResolutionPlans.scala:231)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$4(Analyzer.scala:1600)\n",
       "\t\tat scala.Option.map(Option.scala:230)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$3(Analyzer.scala:1592)\n",
       "\t\tat scala.Option.orElse(Option.scala:447)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableOrView(Analyzer.scala:1587)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1556)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1466)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:182)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:171)\n",
       "\t\tat scala.PartialFunction.applyOrElse(PartialFunction.scala:127)\n",
       "\t\tat scala.PartialFunction.applyOrElse$(PartialFunction.scala:126)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.applyOrElse(AnalysisHelper.scala:171)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)\n",
       "\t\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:138)\n",
       "\t\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1330)\n",
       "\t\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1329)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.DescribeRelation.mapChildren(v2Commands.scala:726)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning(AnalysisHelper.scala:186)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning$(AnalysisHelper.scala:167)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithSubqueriesAndPruning(LogicalPlan.scala:42)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.resolveOperatorsUpWithSubqueriesAndPruningByConf(Analyzer.scala:1674)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply0(Analyzer.scala:1466)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1419)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1365)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:480)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:629)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:613)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:131)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:480)\n",
       "\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:479)\n",
       "\t\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n",
       "\t\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n",
       "\t\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:475)\n",
       "\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:452)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22(RuleExecutor.scala:585)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22$adapted(RuleExecutor.scala:585)\n",
       "\t\tat scala.collection.immutable.List.foreach(List.scala:431)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:585)\n",
       "\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:349)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:498)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:491)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:397)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:491)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:416)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:341)\n",
       "\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:219)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:341)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:252)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:96)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:131)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:87)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:478)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:478)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:294)\n",
       "\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:548)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:668)\n",
       "\t\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:151)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:668)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1307)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:661)\n",
       "\t\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:658)\n",
       "\t\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:658)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:288)\n",
       "\t\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:287)\n",
       "\t\tat scala.util.Try$.apply(Try.scala:213)\n",
       "\t\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)\n",
       "\t\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n",
       "\t\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n",
       "\t\t... 24 more\n",
       "</td><td>2025-08-13T14:52:13.322232Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>table/view</td><td>`accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample`.`forecast`.`us_postal_hourly_imperial`</td><td>clone/create</td><td>error</td><td>Failed to process table/view `accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample`.`forecast`.`us_postal_hourly_imperial`. Error: An error occurred while calling o427.sql.\n",
       ": com.databricks.sql.managedcatalog.acl.UnauthorizedAccessException: PERMISSION_DENIED: User does not have USE CATALOG on Catalog 'accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample'.\n",
       "\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:87)\n",
       "\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:63)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:251)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapExceptionBase(ManagedCatalogClientImpl.scala:7378)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:7364)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.getTableCredentials(ManagedCatalogClientImpl.scala:3951)\n",
       "\tat com.databricks.sql.managedcatalog.ManagedCatalogClient.getTemporaryCredentials(ManagedCatalogClient.scala:2620)\n",
       "\tat com.databricks.sql.managedcatalog.ManagedCatalogClient.getTemporaryCredentials$(ManagedCatalogClient.scala:2605)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.getTemporaryCredentials(ManagedCatalogClientImpl.scala:251)\n",
       "\tat com.databricks.unity.TempCredCache.$anonfun$getInternal$7(TemporaryCredentials.scala:448)\n",
       "\tat com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4724)\n",
       "\tat com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3522)\n",
       "\tat com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2315)\n",
       "\tat com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2278)\n",
       "\tat com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2193)\n",
       "\tat com.google.common.cache.LocalCache.get(LocalCache.java:3932)\n",
       "\tat com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4721)\n",
       "\tat com.databricks.unity.TempCredCache.liftedTree1$1(TemporaryCredentials.scala:447)\n",
       "\tat com.databricks.unity.TempCredCache.getInternal(TemporaryCredentials.scala:446)\n",
       "\tat com.databricks.unity.TempCredCache.get(TemporaryCredentials.scala:373)\n",
       "\tat com.databricks.unity.UnityCredentialManager.getTemporaryCredentials(CredentialManager.scala:488)\n",
       "\tat com.databricks.unity.CredentialManager$.getTemporaryCredentials(CredentialManager.scala:881)\n",
       "\tat com.databricks.unity.SAM.setConfFromCred$1(SAM.scala:338)\n",
       "\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$8(SAM.scala:353)\n",
       "\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$8$adapted(SAM.scala:352)\n",
       "\tat com.databricks.unity.TempCredRequest$.$anonfun$withFallbackOnOverlappingForParentPath$3(TemporaryCredentials.scala:147)\n",
       "\tat scala.util.Try$.apply(Try.scala:213)\n",
       "\tat com.databricks.unity.TempCredRequest$.withFallbackOnOverlappingForParentPath(TemporaryCredentials.scala:147)\n",
       "\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$7(SAM.scala:352)\n",
       "\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$7$adapted(SAM.scala:350)\n",
       "\tat scala.Option.map(Option.scala:230)\n",
       "\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$6(SAM.scala:350)\n",
       "\tat scala.Option.flatMap(Option.scala:271)\n",
       "\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$1(SAM.scala:349)\n",
       "\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$1$adapted(SAM.scala:336)\n",
       "\tat scala.Option.foreach(Option.scala:407)\n",
       "\tat com.databricks.unity.SAM.setFileSystemConfigs(SAM.scala:336)\n",
       "\tat com.databricks.unity.SAM.createDelegate(SAM.scala:388)\n",
       "\tat com.databricks.unity.SAM.createDelegate$(SAM.scala:372)\n",
       "\tat com.databricks.unity.TableSAM.createDelegate(SAM.scala:695)\n",
       "\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.createDelegate(CredentialScopeFileSystem.scala:74)\n",
       "\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.$anonfun$setDelegates$1(CredentialScopeFileSystem.scala:127)\n",
       "\tat com.databricks.sql.acl.fs.Lazy.apply(DelegatingFileSystem.scala:310)\n",
       "\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.readDelegateMakeQualified(CredentialScopeFileSystem.scala:282)\n",
       "\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.optimizedMakeQualifiedPath(CredentialScopeFileSystem.scala:277)\n",
       "\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.makeQualified(CredentialScopeFileSystem.scala:294)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaLog$.apply(DeltaLog.scala:1284)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaLog$.forTable(DeltaLog.scala:1087)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$5(DeltaTableV2.scala:175)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$withAdditionalSnapshotInitializationUsageLogData$1(DeltaLog.scala:968)\n",
       "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
       "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
       "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaLog$.withAdditionalSnapshotInitializationUsageLogData(DeltaLog.scala:969)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$2(DeltaTableV2.scala:169)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:700)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$1(DeltaTableV2.scala:150)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:83)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.deltaLog$lzycompute(DeltaTableV2.scala:150)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.deltaLog(DeltaTableV2.scala:148)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$8(DeltaTableV2.scala:264)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$withAdditionalSnapshotInitializationUsageLogData$1(DeltaLog.scala:968)\n",
       "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
       "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
       "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaLog$.withAdditionalSnapshotInitializationUsageLogData(DeltaLog.scala:969)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$7(DeltaTableV2.scala:263)\n",
       "\tat scala.Option.orElse(Option.scala:447)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$5(DeltaTableV2.scala:263)\n",
       "\tat scala.Option.orElse(Option.scala:447)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$2(DeltaTableV2.scala:260)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:83)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$1(DeltaTableV2.scala:290)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:700)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot$lzycompute(DeltaTableV2.scala:240)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot(DeltaTableV2.scala:238)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$tableSchema$2(DeltaTableV2.scala:317)\n",
       "\tat scala.Option.getOrElse(Option.scala:189)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.tableSchema$lzycompute(DeltaTableV2.scala:317)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.tableSchema(DeltaTableV2.scala:315)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.schema(DeltaTableV2.scala:322)\n",
       "\tat org.apache.spark.sql.connector.catalog.Table.columns(Table.java:68)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.columns(DeltaTableV2.scala:83)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.ResolvedTable$.create(v2ResolutionPlans.scala:231)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$4(Analyzer.scala:1600)\n",
       "\tat scala.Option.map(Option.scala:230)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$3(Analyzer.scala:1592)\n",
       "\tat scala.Option.orElse(Option.scala:447)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableOrView(Analyzer.scala:1587)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1556)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1466)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:182)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:171)\n",
       "\tat scala.PartialFunction.applyOrElse(PartialFunction.scala:127)\n",
       "\tat scala.PartialFunction.applyOrElse$(PartialFunction.scala:126)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.applyOrElse(AnalysisHelper.scala:171)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)\n",
       "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:138)\n",
       "\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1330)\n",
       "\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1329)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.DescribeRelation.mapChildren(v2Commands.scala:726)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning(AnalysisHelper.scala:186)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning$(AnalysisHelper.scala:167)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithSubqueriesAndPruning(LogicalPlan.scala:42)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.resolveOperatorsUpWithSubqueriesAndPruningByConf(Analyzer.scala:1674)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply0(Analyzer.scala:1466)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1419)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1365)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:480)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:629)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:613)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:131)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:480)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:479)\n",
       "\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n",
       "\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n",
       "\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:475)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:452)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22(RuleExecutor.scala:585)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22$adapted(RuleExecutor.scala:585)\n",
       "\tat scala.collection.immutable.List.foreach(List.scala:431)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:585)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:349)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:498)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:491)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:397)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:491)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:416)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:341)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:219)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:341)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:252)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:96)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:131)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:87)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:478)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:478)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:294)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:548)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:668)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:151)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:668)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1307)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:661)\n",
       "\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:658)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:658)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:288)\n",
       "\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:287)\n",
       "\tat scala.util.Try$.apply(Try.scala:213)\n",
       "\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)\n",
       "\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)\n",
       "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:321)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:267)\n",
       "\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$3(Dataset.scala:149)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n",
       "\tat org.apache.spark.sql.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1469)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1469)\n",
       "\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:141)\n",
       "\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:1048)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n",
       "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:1001)\n",
       "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:1071)\n",
       "\tat jdk.internal.reflect.GeneratedMethodAccessor593.invoke(Unknown Source)\n",
       "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
       "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
       "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n",
       "\tat py4j.Gateway.invoke(Gateway.java:306)\n",
       "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
       "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:197)\n",
       "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:117)\n",
       "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
       "\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\n",
       "\t\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:87)\n",
       "\t\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:63)\n",
       "\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:251)\n",
       "\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapExceptionBase(ManagedCatalogClientImpl.scala:7378)\n",
       "\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:7364)\n",
       "\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.getTableCredentials(ManagedCatalogClientImpl.scala:3951)\n",
       "\t\tat com.databricks.sql.managedcatalog.ManagedCatalogClient.getTemporaryCredentials(ManagedCatalogClient.scala:2620)\n",
       "\t\tat com.databricks.sql.managedcatalog.ManagedCatalogClient.getTemporaryCredentials$(ManagedCatalogClient.scala:2605)\n",
       "\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.getTemporaryCredentials(ManagedCatalogClientImpl.scala:251)\n",
       "\t\tat com.databricks.unity.TempCredCache.$anonfun$getInternal$7(TemporaryCredentials.scala:448)\n",
       "\t\tat com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4724)\n",
       "\t\tat com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3522)\n",
       "\t\tat com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2315)\n",
       "\t\tat com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2278)\n",
       "\t\tat com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2193)\n",
       "\t\tat com.google.common.cache.LocalCache.get(LocalCache.java:3932)\n",
       "\t\tat com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4721)\n",
       "\t\tat com.databricks.unity.TempCredCache.liftedTree1$1(TemporaryCredentials.scala:447)\n",
       "\t\tat com.databricks.unity.TempCredCache.getInternal(TemporaryCredentials.scala:446)\n",
       "\t\tat com.databricks.unity.TempCredCache.get(TemporaryCredentials.scala:373)\n",
       "\t\tat com.databricks.unity.UnityCredentialManager.getTemporaryCredentials(CredentialManager.scala:488)\n",
       "\t\tat com.databricks.unity.CredentialManager$.getTemporaryCredentials(CredentialManager.scala:881)\n",
       "\t\tat com.databricks.unity.SAM.setConfFromCred$1(SAM.scala:338)\n",
       "\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$8(SAM.scala:353)\n",
       "\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$8$adapted(SAM.scala:352)\n",
       "\t\tat com.databricks.unity.TempCredRequest$.$anonfun$withFallbackOnOverlappingForParentPath$3(TemporaryCredentials.scala:147)\n",
       "\t\tat scala.util.Try$.apply(Try.scala:213)\n",
       "\t\tat com.databricks.unity.TempCredRequest$.withFallbackOnOverlappingForParentPath(TemporaryCredentials.scala:147)\n",
       "\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$7(SAM.scala:352)\n",
       "\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$7$adapted(SAM.scala:350)\n",
       "\t\tat scala.Option.map(Option.scala:230)\n",
       "\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$6(SAM.scala:350)\n",
       "\t\tat scala.Option.flatMap(Option.scala:271)\n",
       "\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$1(SAM.scala:349)\n",
       "\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$1$adapted(SAM.scala:336)\n",
       "\t\tat scala.Option.foreach(Option.scala:407)\n",
       "\t\tat com.databricks.unity.SAM.setFileSystemConfigs(SAM.scala:336)\n",
       "\t\tat com.databricks.unity.SAM.createDelegate(SAM.scala:388)\n",
       "\t\tat com.databricks.unity.SAM.createDelegate$(SAM.scala:372)\n",
       "\t\tat com.databricks.unity.TableSAM.createDelegate(SAM.scala:695)\n",
       "\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.createDelegate(CredentialScopeFileSystem.scala:74)\n",
       "\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.$anonfun$setDelegates$1(CredentialScopeFileSystem.scala:127)\n",
       "\t\tat com.databricks.sql.acl.fs.Lazy.apply(DelegatingFileSystem.scala:310)\n",
       "\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.readDelegateMakeQualified(CredentialScopeFileSystem.scala:282)\n",
       "\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.optimizedMakeQualifiedPath(CredentialScopeFileSystem.scala:277)\n",
       "\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.makeQualified(CredentialScopeFileSystem.scala:294)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.apply(DeltaLog.scala:1284)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.forTable(DeltaLog.scala:1087)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$5(DeltaTableV2.scala:175)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$withAdditionalSnapshotInitializationUsageLogData$1(DeltaLog.scala:968)\n",
       "\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
       "\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
       "\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.withAdditionalSnapshotInitializationUsageLogData(DeltaLog.scala:969)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$2(DeltaTableV2.scala:169)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:700)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$1(DeltaTableV2.scala:150)\n",
       "\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:83)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.deltaLog$lzycompute(DeltaTableV2.scala:150)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.deltaLog(DeltaTableV2.scala:148)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$8(DeltaTableV2.scala:264)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$withAdditionalSnapshotInitializationUsageLogData$1(DeltaLog.scala:968)\n",
       "\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
       "\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
       "\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.withAdditionalSnapshotInitializationUsageLogData(DeltaLog.scala:969)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$7(DeltaTableV2.scala:263)\n",
       "\t\tat scala.Option.orElse(Option.scala:447)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$5(DeltaTableV2.scala:263)\n",
       "\t\tat scala.Option.orElse(Option.scala:447)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$2(DeltaTableV2.scala:260)\n",
       "\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:83)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$1(DeltaTableV2.scala:290)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:700)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot$lzycompute(DeltaTableV2.scala:240)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot(DeltaTableV2.scala:238)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$tableSchema$2(DeltaTableV2.scala:317)\n",
       "\t\tat scala.Option.getOrElse(Option.scala:189)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.tableSchema$lzycompute(DeltaTableV2.scala:317)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.tableSchema(DeltaTableV2.scala:315)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.schema(DeltaTableV2.scala:322)\n",
       "\t\tat org.apache.spark.sql.connector.catalog.Table.columns(Table.java:68)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.columns(DeltaTableV2.scala:83)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.ResolvedTable$.create(v2ResolutionPlans.scala:231)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$4(Analyzer.scala:1600)\n",
       "\t\tat scala.Option.map(Option.scala:230)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$3(Analyzer.scala:1592)\n",
       "\t\tat scala.Option.orElse(Option.scala:447)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableOrView(Analyzer.scala:1587)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1556)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1466)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:182)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:171)\n",
       "\t\tat scala.PartialFunction.applyOrElse(PartialFunction.scala:127)\n",
       "\t\tat scala.PartialFunction.applyOrElse$(PartialFunction.scala:126)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.applyOrElse(AnalysisHelper.scala:171)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)\n",
       "\t\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:138)\n",
       "\t\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1330)\n",
       "\t\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1329)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.DescribeRelation.mapChildren(v2Commands.scala:726)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning(AnalysisHelper.scala:186)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning$(AnalysisHelper.scala:167)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithSubqueriesAndPruning(LogicalPlan.scala:42)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.resolveOperatorsUpWithSubqueriesAndPruningByConf(Analyzer.scala:1674)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply0(Analyzer.scala:1466)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1419)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1365)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:480)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:629)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:613)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:131)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:480)\n",
       "\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:479)\n",
       "\t\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n",
       "\t\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n",
       "\t\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:475)\n",
       "\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:452)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22(RuleExecutor.scala:585)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22$adapted(RuleExecutor.scala:585)\n",
       "\t\tat scala.collection.immutable.List.foreach(List.scala:431)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:585)\n",
       "\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:349)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:498)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:491)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:397)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:491)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:416)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:341)\n",
       "\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:219)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:341)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:252)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:96)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:131)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:87)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:478)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:478)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:294)\n",
       "\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:548)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:668)\n",
       "\t\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:151)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:668)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1307)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:661)\n",
       "\t\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:658)\n",
       "\t\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:658)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:288)\n",
       "\t\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:287)\n",
       "\t\tat scala.util.Try$.apply(Try.scala:213)\n",
       "\t\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)\n",
       "\t\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n",
       "\t\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n",
       "\t\t... 24 more\n",
       "</td><td>2025-08-13T14:52:13.308379Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>table/view</td><td>`accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample`.`forecast`.`us_postal_daynight_metric`</td><td>clone/create</td><td>error</td><td>Failed to process table/view `accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample`.`forecast`.`us_postal_daynight_metric`. Error: An error occurred while calling o427.sql.\n",
       ": com.databricks.sql.managedcatalog.acl.UnauthorizedAccessException: PERMISSION_DENIED: User does not have USE CATALOG on Catalog 'accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample'.\n",
       "\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:87)\n",
       "\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:63)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:251)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapExceptionBase(ManagedCatalogClientImpl.scala:7378)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:7364)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.getTableCredentials(ManagedCatalogClientImpl.scala:3951)\n",
       "\tat com.databricks.sql.managedcatalog.ManagedCatalogClient.getTemporaryCredentials(ManagedCatalogClient.scala:2620)\n",
       "\tat com.databricks.sql.managedcatalog.ManagedCatalogClient.getTemporaryCredentials$(ManagedCatalogClient.scala:2605)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.getTemporaryCredentials(ManagedCatalogClientImpl.scala:251)\n",
       "\tat com.databricks.unity.TempCredCache.$anonfun$getInternal$7(TemporaryCredentials.scala:448)\n",
       "\tat com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4724)\n",
       "\tat com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3522)\n",
       "\tat com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2315)\n",
       "\tat com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2278)\n",
       "\tat com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2193)\n",
       "\tat com.google.common.cache.LocalCache.get(LocalCache.java:3932)\n",
       "\tat com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4721)\n",
       "\tat com.databricks.unity.TempCredCache.liftedTree1$1(TemporaryCredentials.scala:447)\n",
       "\tat com.databricks.unity.TempCredCache.getInternal(TemporaryCredentials.scala:446)\n",
       "\tat com.databricks.unity.TempCredCache.get(TemporaryCredentials.scala:373)\n",
       "\tat com.databricks.unity.UnityCredentialManager.getTemporaryCredentials(CredentialManager.scala:488)\n",
       "\tat com.databricks.unity.CredentialManager$.getTemporaryCredentials(CredentialManager.scala:881)\n",
       "\tat com.databricks.unity.SAM.setConfFromCred$1(SAM.scala:338)\n",
       "\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$8(SAM.scala:353)\n",
       "\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$8$adapted(SAM.scala:352)\n",
       "\tat com.databricks.unity.TempCredRequest$.$anonfun$withFallbackOnOverlappingForParentPath$3(TemporaryCredentials.scala:147)\n",
       "\tat scala.util.Try$.apply(Try.scala:213)\n",
       "\tat com.databricks.unity.TempCredRequest$.withFallbackOnOverlappingForParentPath(TemporaryCredentials.scala:147)\n",
       "\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$7(SAM.scala:352)\n",
       "\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$7$adapted(SAM.scala:350)\n",
       "\tat scala.Option.map(Option.scala:230)\n",
       "\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$6(SAM.scala:350)\n",
       "\tat scala.Option.flatMap(Option.scala:271)\n",
       "\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$1(SAM.scala:349)\n",
       "\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$1$adapted(SAM.scala:336)\n",
       "\tat scala.Option.foreach(Option.scala:407)\n",
       "\tat com.databricks.unity.SAM.setFileSystemConfigs(SAM.scala:336)\n",
       "\tat com.databricks.unity.SAM.createDelegate(SAM.scala:388)\n",
       "\tat com.databricks.unity.SAM.createDelegate$(SAM.scala:372)\n",
       "\tat com.databricks.unity.TableSAM.createDelegate(SAM.scala:695)\n",
       "\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.createDelegate(CredentialScopeFileSystem.scala:74)\n",
       "\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.$anonfun$setDelegates$1(CredentialScopeFileSystem.scala:127)\n",
       "\tat com.databricks.sql.acl.fs.Lazy.apply(DelegatingFileSystem.scala:310)\n",
       "\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.readDelegateMakeQualified(CredentialScopeFileSystem.scala:282)\n",
       "\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.optimizedMakeQualifiedPath(CredentialScopeFileSystem.scala:277)\n",
       "\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.makeQualified(CredentialScopeFileSystem.scala:294)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaLog$.apply(DeltaLog.scala:1284)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaLog$.forTable(DeltaLog.scala:1087)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$5(DeltaTableV2.scala:175)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$withAdditionalSnapshotInitializationUsageLogData$1(DeltaLog.scala:968)\n",
       "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
       "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
       "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaLog$.withAdditionalSnapshotInitializationUsageLogData(DeltaLog.scala:969)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$2(DeltaTableV2.scala:169)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:700)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$1(DeltaTableV2.scala:150)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:83)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.deltaLog$lzycompute(DeltaTableV2.scala:150)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.deltaLog(DeltaTableV2.scala:148)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$8(DeltaTableV2.scala:264)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$withAdditionalSnapshotInitializationUsageLogData$1(DeltaLog.scala:968)\n",
       "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
       "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
       "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaLog$.withAdditionalSnapshotInitializationUsageLogData(DeltaLog.scala:969)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$7(DeltaTableV2.scala:263)\n",
       "\tat scala.Option.orElse(Option.scala:447)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$5(DeltaTableV2.scala:263)\n",
       "\tat scala.Option.orElse(Option.scala:447)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$2(DeltaTableV2.scala:260)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:83)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$1(DeltaTableV2.scala:290)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:700)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot$lzycompute(DeltaTableV2.scala:240)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot(DeltaTableV2.scala:238)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$tableSchema$2(DeltaTableV2.scala:317)\n",
       "\tat scala.Option.getOrElse(Option.scala:189)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.tableSchema$lzycompute(DeltaTableV2.scala:317)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.tableSchema(DeltaTableV2.scala:315)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.schema(DeltaTableV2.scala:322)\n",
       "\tat org.apache.spark.sql.connector.catalog.Table.columns(Table.java:68)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.columns(DeltaTableV2.scala:83)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.ResolvedTable$.create(v2ResolutionPlans.scala:231)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$4(Analyzer.scala:1600)\n",
       "\tat scala.Option.map(Option.scala:230)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$3(Analyzer.scala:1592)\n",
       "\tat scala.Option.orElse(Option.scala:447)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableOrView(Analyzer.scala:1587)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1556)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1466)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:182)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:171)\n",
       "\tat scala.PartialFunction.applyOrElse(PartialFunction.scala:127)\n",
       "\tat scala.PartialFunction.applyOrElse$(PartialFunction.scala:126)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.applyOrElse(AnalysisHelper.scala:171)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)\n",
       "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:138)\n",
       "\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1330)\n",
       "\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1329)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.DescribeRelation.mapChildren(v2Commands.scala:726)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning(AnalysisHelper.scala:186)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning$(AnalysisHelper.scala:167)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithSubqueriesAndPruning(LogicalPlan.scala:42)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.resolveOperatorsUpWithSubqueriesAndPruningByConf(Analyzer.scala:1674)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply0(Analyzer.scala:1466)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1419)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1365)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:480)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:629)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:613)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:131)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:480)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:479)\n",
       "\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n",
       "\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n",
       "\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:475)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:452)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22(RuleExecutor.scala:585)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22$adapted(RuleExecutor.scala:585)\n",
       "\tat scala.collection.immutable.List.foreach(List.scala:431)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:585)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:349)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:498)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:491)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:397)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:491)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:416)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:341)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:219)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:341)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:252)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:96)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:131)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:87)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:478)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:478)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:294)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:548)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:668)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:151)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:668)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1307)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:661)\n",
       "\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:658)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:658)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:288)\n",
       "\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:287)\n",
       "\tat scala.util.Try$.apply(Try.scala:213)\n",
       "\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)\n",
       "\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)\n",
       "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:321)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:267)\n",
       "\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$3(Dataset.scala:149)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n",
       "\tat org.apache.spark.sql.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1469)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1469)\n",
       "\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:141)\n",
       "\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:1048)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n",
       "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:1001)\n",
       "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:1071)\n",
       "\tat jdk.internal.reflect.GeneratedMethodAccessor593.invoke(Unknown Source)\n",
       "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
       "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
       "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n",
       "\tat py4j.Gateway.invoke(Gateway.java:306)\n",
       "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
       "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:197)\n",
       "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:117)\n",
       "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
       "\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\n",
       "\t\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:87)\n",
       "\t\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:63)\n",
       "\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:251)\n",
       "\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapExceptionBase(ManagedCatalogClientImpl.scala:7378)\n",
       "\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:7364)\n",
       "\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.getTableCredentials(ManagedCatalogClientImpl.scala:3951)\n",
       "\t\tat com.databricks.sql.managedcatalog.ManagedCatalogClient.getTemporaryCredentials(ManagedCatalogClient.scala:2620)\n",
       "\t\tat com.databricks.sql.managedcatalog.ManagedCatalogClient.getTemporaryCredentials$(ManagedCatalogClient.scala:2605)\n",
       "\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.getTemporaryCredentials(ManagedCatalogClientImpl.scala:251)\n",
       "\t\tat com.databricks.unity.TempCredCache.$anonfun$getInternal$7(TemporaryCredentials.scala:448)\n",
       "\t\tat com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4724)\n",
       "\t\tat com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3522)\n",
       "\t\tat com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2315)\n",
       "\t\tat com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2278)\n",
       "\t\tat com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2193)\n",
       "\t\tat com.google.common.cache.LocalCache.get(LocalCache.java:3932)\n",
       "\t\tat com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4721)\n",
       "\t\tat com.databricks.unity.TempCredCache.liftedTree1$1(TemporaryCredentials.scala:447)\n",
       "\t\tat com.databricks.unity.TempCredCache.getInternal(TemporaryCredentials.scala:446)\n",
       "\t\tat com.databricks.unity.TempCredCache.get(TemporaryCredentials.scala:373)\n",
       "\t\tat com.databricks.unity.UnityCredentialManager.getTemporaryCredentials(CredentialManager.scala:488)\n",
       "\t\tat com.databricks.unity.CredentialManager$.getTemporaryCredentials(CredentialManager.scala:881)\n",
       "\t\tat com.databricks.unity.SAM.setConfFromCred$1(SAM.scala:338)\n",
       "\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$8(SAM.scala:353)\n",
       "\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$8$adapted(SAM.scala:352)\n",
       "\t\tat com.databricks.unity.TempCredRequest$.$anonfun$withFallbackOnOverlappingForParentPath$3(TemporaryCredentials.scala:147)\n",
       "\t\tat scala.util.Try$.apply(Try.scala:213)\n",
       "\t\tat com.databricks.unity.TempCredRequest$.withFallbackOnOverlappingForParentPath(TemporaryCredentials.scala:147)\n",
       "\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$7(SAM.scala:352)\n",
       "\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$7$adapted(SAM.scala:350)\n",
       "\t\tat scala.Option.map(Option.scala:230)\n",
       "\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$6(SAM.scala:350)\n",
       "\t\tat scala.Option.flatMap(Option.scala:271)\n",
       "\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$1(SAM.scala:349)\n",
       "\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$1$adapted(SAM.scala:336)\n",
       "\t\tat scala.Option.foreach(Option.scala:407)\n",
       "\t\tat com.databricks.unity.SAM.setFileSystemConfigs(SAM.scala:336)\n",
       "\t\tat com.databricks.unity.SAM.createDelegate(SAM.scala:388)\n",
       "\t\tat com.databricks.unity.SAM.createDelegate$(SAM.scala:372)\n",
       "\t\tat com.databricks.unity.TableSAM.createDelegate(SAM.scala:695)\n",
       "\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.createDelegate(CredentialScopeFileSystem.scala:74)\n",
       "\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.$anonfun$setDelegates$1(CredentialScopeFileSystem.scala:127)\n",
       "\t\tat com.databricks.sql.acl.fs.Lazy.apply(DelegatingFileSystem.scala:310)\n",
       "\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.readDelegateMakeQualified(CredentialScopeFileSystem.scala:282)\n",
       "\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.optimizedMakeQualifiedPath(CredentialScopeFileSystem.scala:277)\n",
       "\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.makeQualified(CredentialScopeFileSystem.scala:294)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.apply(DeltaLog.scala:1284)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.forTable(DeltaLog.scala:1087)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$5(DeltaTableV2.scala:175)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$withAdditionalSnapshotInitializationUsageLogData$1(DeltaLog.scala:968)\n",
       "\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
       "\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
       "\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.withAdditionalSnapshotInitializationUsageLogData(DeltaLog.scala:969)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$2(DeltaTableV2.scala:169)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:700)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$1(DeltaTableV2.scala:150)\n",
       "\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:83)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.deltaLog$lzycompute(DeltaTableV2.scala:150)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.deltaLog(DeltaTableV2.scala:148)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$8(DeltaTableV2.scala:264)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$withAdditionalSnapshotInitializationUsageLogData$1(DeltaLog.scala:968)\n",
       "\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
       "\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
       "\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.withAdditionalSnapshotInitializationUsageLogData(DeltaLog.scala:969)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$7(DeltaTableV2.scala:263)\n",
       "\t\tat scala.Option.orElse(Option.scala:447)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$5(DeltaTableV2.scala:263)\n",
       "\t\tat scala.Option.orElse(Option.scala:447)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$2(DeltaTableV2.scala:260)\n",
       "\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:83)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$1(DeltaTableV2.scala:290)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:700)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot$lzycompute(DeltaTableV2.scala:240)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot(DeltaTableV2.scala:238)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$tableSchema$2(DeltaTableV2.scala:317)\n",
       "\t\tat scala.Option.getOrElse(Option.scala:189)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.tableSchema$lzycompute(DeltaTableV2.scala:317)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.tableSchema(DeltaTableV2.scala:315)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.schema(DeltaTableV2.scala:322)\n",
       "\t\tat org.apache.spark.sql.connector.catalog.Table.columns(Table.java:68)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.columns(DeltaTableV2.scala:83)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.ResolvedTable$.create(v2ResolutionPlans.scala:231)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$4(Analyzer.scala:1600)\n",
       "\t\tat scala.Option.map(Option.scala:230)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$3(Analyzer.scala:1592)\n",
       "\t\tat scala.Option.orElse(Option.scala:447)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableOrView(Analyzer.scala:1587)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1556)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1466)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:182)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:171)\n",
       "\t\tat scala.PartialFunction.applyOrElse(PartialFunction.scala:127)\n",
       "\t\tat scala.PartialFunction.applyOrElse$(PartialFunction.scala:126)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.applyOrElse(AnalysisHelper.scala:171)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)\n",
       "\t\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:138)\n",
       "\t\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1330)\n",
       "\t\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1329)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.DescribeRelation.mapChildren(v2Commands.scala:726)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning(AnalysisHelper.scala:186)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning$(AnalysisHelper.scala:167)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithSubqueriesAndPruning(LogicalPlan.scala:42)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.resolveOperatorsUpWithSubqueriesAndPruningByConf(Analyzer.scala:1674)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply0(Analyzer.scala:1466)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1419)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1365)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:480)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:629)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:613)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:131)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:480)\n",
       "\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:479)\n",
       "\t\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n",
       "\t\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n",
       "\t\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:475)\n",
       "\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:452)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22(RuleExecutor.scala:585)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22$adapted(RuleExecutor.scala:585)\n",
       "\t\tat scala.collection.immutable.List.foreach(List.scala:431)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:585)\n",
       "\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:349)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:498)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:491)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:397)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:491)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:416)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:341)\n",
       "\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:219)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:341)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:252)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:96)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:131)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:87)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:478)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:478)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:294)\n",
       "\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:548)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:668)\n",
       "\t\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:151)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:668)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1307)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:661)\n",
       "\t\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:658)\n",
       "\t\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:658)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:288)\n",
       "\t\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:287)\n",
       "\t\tat scala.util.Try$.apply(Try.scala:213)\n",
       "\t\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)\n",
       "\t\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n",
       "\t\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n",
       "\t\t... 24 more\n",
       "</td><td>2025-08-13T14:52:13.280398Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>table/view</td><td>`accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample`.`forecast`.`us_postal_daynight_imperial`</td><td>clone/create</td><td>error</td><td>Failed to process table/view `accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample`.`forecast`.`us_postal_daynight_imperial`. Error: An error occurred while calling o427.sql.\n",
       ": com.databricks.sql.managedcatalog.acl.UnauthorizedAccessException: PERMISSION_DENIED: User does not have USE CATALOG on Catalog 'accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample'.\n",
       "\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:87)\n",
       "\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:63)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:251)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapExceptionBase(ManagedCatalogClientImpl.scala:7378)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:7364)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.getTableCredentials(ManagedCatalogClientImpl.scala:3951)\n",
       "\tat com.databricks.sql.managedcatalog.ManagedCatalogClient.getTemporaryCredentials(ManagedCatalogClient.scala:2620)\n",
       "\tat com.databricks.sql.managedcatalog.ManagedCatalogClient.getTemporaryCredentials$(ManagedCatalogClient.scala:2605)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.getTemporaryCredentials(ManagedCatalogClientImpl.scala:251)\n",
       "\tat com.databricks.unity.TempCredCache.$anonfun$getInternal$7(TemporaryCredentials.scala:448)\n",
       "\tat com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4724)\n",
       "\tat com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3522)\n",
       "\tat com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2315)\n",
       "\tat com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2278)\n",
       "\tat com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2193)\n",
       "\tat com.google.common.cache.LocalCache.get(LocalCache.java:3932)\n",
       "\tat com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4721)\n",
       "\tat com.databricks.unity.TempCredCache.liftedTree1$1(TemporaryCredentials.scala:447)\n",
       "\tat com.databricks.unity.TempCredCache.getInternal(TemporaryCredentials.scala:446)\n",
       "\tat com.databricks.unity.TempCredCache.get(TemporaryCredentials.scala:373)\n",
       "\tat com.databricks.unity.UnityCredentialManager.getTemporaryCredentials(CredentialManager.scala:488)\n",
       "\tat com.databricks.unity.CredentialManager$.getTemporaryCredentials(CredentialManager.scala:881)\n",
       "\tat com.databricks.unity.SAM.setConfFromCred$1(SAM.scala:338)\n",
       "\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$8(SAM.scala:353)\n",
       "\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$8$adapted(SAM.scala:352)\n",
       "\tat com.databricks.unity.TempCredRequest$.$anonfun$withFallbackOnOverlappingForParentPath$3(TemporaryCredentials.scala:147)\n",
       "\tat scala.util.Try$.apply(Try.scala:213)\n",
       "\tat com.databricks.unity.TempCredRequest$.withFallbackOnOverlappingForParentPath(TemporaryCredentials.scala:147)\n",
       "\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$7(SAM.scala:352)\n",
       "\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$7$adapted(SAM.scala:350)\n",
       "\tat scala.Option.map(Option.scala:230)\n",
       "\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$6(SAM.scala:350)\n",
       "\tat scala.Option.flatMap(Option.scala:271)\n",
       "\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$1(SAM.scala:349)\n",
       "\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$1$adapted(SAM.scala:336)\n",
       "\tat scala.Option.foreach(Option.scala:407)\n",
       "\tat com.databricks.unity.SAM.setFileSystemConfigs(SAM.scala:336)\n",
       "\tat com.databricks.unity.SAM.createDelegate(SAM.scala:388)\n",
       "\tat com.databricks.unity.SAM.createDelegate$(SAM.scala:372)\n",
       "\tat com.databricks.unity.TableSAM.createDelegate(SAM.scala:695)\n",
       "\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.createDelegate(CredentialScopeFileSystem.scala:74)\n",
       "\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.$anonfun$setDelegates$1(CredentialScopeFileSystem.scala:127)\n",
       "\tat com.databricks.sql.acl.fs.Lazy.apply(DelegatingFileSystem.scala:310)\n",
       "\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.readDelegateMakeQualified(CredentialScopeFileSystem.scala:282)\n",
       "\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.optimizedMakeQualifiedPath(CredentialScopeFileSystem.scala:277)\n",
       "\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.makeQualified(CredentialScopeFileSystem.scala:294)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaLog$.apply(DeltaLog.scala:1284)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaLog$.forTable(DeltaLog.scala:1087)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$5(DeltaTableV2.scala:175)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$withAdditionalSnapshotInitializationUsageLogData$1(DeltaLog.scala:968)\n",
       "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
       "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
       "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaLog$.withAdditionalSnapshotInitializationUsageLogData(DeltaLog.scala:969)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$2(DeltaTableV2.scala:169)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:700)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$1(DeltaTableV2.scala:150)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:83)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.deltaLog$lzycompute(DeltaTableV2.scala:150)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.deltaLog(DeltaTableV2.scala:148)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$8(DeltaTableV2.scala:264)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$withAdditionalSnapshotInitializationUsageLogData$1(DeltaLog.scala:968)\n",
       "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
       "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
       "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaLog$.withAdditionalSnapshotInitializationUsageLogData(DeltaLog.scala:969)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$7(DeltaTableV2.scala:263)\n",
       "\tat scala.Option.orElse(Option.scala:447)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$5(DeltaTableV2.scala:263)\n",
       "\tat scala.Option.orElse(Option.scala:447)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$2(DeltaTableV2.scala:260)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:83)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$1(DeltaTableV2.scala:290)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:700)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot$lzycompute(DeltaTableV2.scala:240)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot(DeltaTableV2.scala:238)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$tableSchema$2(DeltaTableV2.scala:317)\n",
       "\tat scala.Option.getOrElse(Option.scala:189)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.tableSchema$lzycompute(DeltaTableV2.scala:317)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.tableSchema(DeltaTableV2.scala:315)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.schema(DeltaTableV2.scala:322)\n",
       "\tat org.apache.spark.sql.connector.catalog.Table.columns(Table.java:68)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.columns(DeltaTableV2.scala:83)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.ResolvedTable$.create(v2ResolutionPlans.scala:231)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$4(Analyzer.scala:1600)\n",
       "\tat scala.Option.map(Option.scala:230)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$3(Analyzer.scala:1592)\n",
       "\tat scala.Option.orElse(Option.scala:447)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableOrView(Analyzer.scala:1587)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1556)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1466)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:182)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:171)\n",
       "\tat scala.PartialFunction.applyOrElse(PartialFunction.scala:127)\n",
       "\tat scala.PartialFunction.applyOrElse$(PartialFunction.scala:126)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.applyOrElse(AnalysisHelper.scala:171)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)\n",
       "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:138)\n",
       "\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1330)\n",
       "\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1329)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.DescribeRelation.mapChildren(v2Commands.scala:726)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning(AnalysisHelper.scala:186)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning$(AnalysisHelper.scala:167)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithSubqueriesAndPruning(LogicalPlan.scala:42)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.resolveOperatorsUpWithSubqueriesAndPruningByConf(Analyzer.scala:1674)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply0(Analyzer.scala:1466)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1419)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1365)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:480)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:629)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:613)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:131)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:480)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:479)\n",
       "\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n",
       "\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n",
       "\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:475)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:452)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22(RuleExecutor.scala:585)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22$adapted(RuleExecutor.scala:585)\n",
       "\tat scala.collection.immutable.List.foreach(List.scala:431)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:585)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:349)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:498)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:491)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:397)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:491)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:416)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:341)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:219)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:341)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:252)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:96)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:131)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:87)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:478)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:478)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:294)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:548)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:668)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:151)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:668)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1307)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:661)\n",
       "\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:658)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:658)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:288)\n",
       "\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:287)\n",
       "\tat scala.util.Try$.apply(Try.scala:213)\n",
       "\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)\n",
       "\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)\n",
       "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:321)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:267)\n",
       "\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$3(Dataset.scala:149)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n",
       "\tat org.apache.spark.sql.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1469)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1469)\n",
       "\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:141)\n",
       "\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:1048)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n",
       "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:1001)\n",
       "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:1071)\n",
       "\tat jdk.internal.reflect.GeneratedMethodAccessor593.invoke(Unknown Source)\n",
       "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
       "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
       "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n",
       "\tat py4j.Gateway.invoke(Gateway.java:306)\n",
       "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
       "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:197)\n",
       "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:117)\n",
       "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
       "\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\n",
       "\t\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:87)\n",
       "\t\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:63)\n",
       "\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:251)\n",
       "\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapExceptionBase(ManagedCatalogClientImpl.scala:7378)\n",
       "\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:7364)\n",
       "\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.getTableCredentials(ManagedCatalogClientImpl.scala:3951)\n",
       "\t\tat com.databricks.sql.managedcatalog.ManagedCatalogClient.getTemporaryCredentials(ManagedCatalogClient.scala:2620)\n",
       "\t\tat com.databricks.sql.managedcatalog.ManagedCatalogClient.getTemporaryCredentials$(ManagedCatalogClient.scala:2605)\n",
       "\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.getTemporaryCredentials(ManagedCatalogClientImpl.scala:251)\n",
       "\t\tat com.databricks.unity.TempCredCache.$anonfun$getInternal$7(TemporaryCredentials.scala:448)\n",
       "\t\tat com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4724)\n",
       "\t\tat com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3522)\n",
       "\t\tat com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2315)\n",
       "\t\tat com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2278)\n",
       "\t\tat com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2193)\n",
       "\t\tat com.google.common.cache.LocalCache.get(LocalCache.java:3932)\n",
       "\t\tat com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4721)\n",
       "\t\tat com.databricks.unity.TempCredCache.liftedTree1$1(TemporaryCredentials.scala:447)\n",
       "\t\tat com.databricks.unity.TempCredCache.getInternal(TemporaryCredentials.scala:446)\n",
       "\t\tat com.databricks.unity.TempCredCache.get(TemporaryCredentials.scala:373)\n",
       "\t\tat com.databricks.unity.UnityCredentialManager.getTemporaryCredentials(CredentialManager.scala:488)\n",
       "\t\tat com.databricks.unity.CredentialManager$.getTemporaryCredentials(CredentialManager.scala:881)\n",
       "\t\tat com.databricks.unity.SAM.setConfFromCred$1(SAM.scala:338)\n",
       "\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$8(SAM.scala:353)\n",
       "\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$8$adapted(SAM.scala:352)\n",
       "\t\tat com.databricks.unity.TempCredRequest$.$anonfun$withFallbackOnOverlappingForParentPath$3(TemporaryCredentials.scala:147)\n",
       "\t\tat scala.util.Try$.apply(Try.scala:213)\n",
       "\t\tat com.databricks.unity.TempCredRequest$.withFallbackOnOverlappingForParentPath(TemporaryCredentials.scala:147)\n",
       "\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$7(SAM.scala:352)\n",
       "\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$7$adapted(SAM.scala:350)\n",
       "\t\tat scala.Option.map(Option.scala:230)\n",
       "\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$6(SAM.scala:350)\n",
       "\t\tat scala.Option.flatMap(Option.scala:271)\n",
       "\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$1(SAM.scala:349)\n",
       "\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$1$adapted(SAM.scala:336)\n",
       "\t\tat scala.Option.foreach(Option.scala:407)\n",
       "\t\tat com.databricks.unity.SAM.setFileSystemConfigs(SAM.scala:336)\n",
       "\t\tat com.databricks.unity.SAM.createDelegate(SAM.scala:388)\n",
       "\t\tat com.databricks.unity.SAM.createDelegate$(SAM.scala:372)\n",
       "\t\tat com.databricks.unity.TableSAM.createDelegate(SAM.scala:695)\n",
       "\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.createDelegate(CredentialScopeFileSystem.scala:74)\n",
       "\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.$anonfun$setDelegates$1(CredentialScopeFileSystem.scala:127)\n",
       "\t\tat com.databricks.sql.acl.fs.Lazy.apply(DelegatingFileSystem.scala:310)\n",
       "\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.readDelegateMakeQualified(CredentialScopeFileSystem.scala:282)\n",
       "\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.optimizedMakeQualifiedPath(CredentialScopeFileSystem.scala:277)\n",
       "\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.makeQualified(CredentialScopeFileSystem.scala:294)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.apply(DeltaLog.scala:1284)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.forTable(DeltaLog.scala:1087)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$5(DeltaTableV2.scala:175)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$withAdditionalSnapshotInitializationUsageLogData$1(DeltaLog.scala:968)\n",
       "\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
       "\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
       "\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.withAdditionalSnapshotInitializationUsageLogData(DeltaLog.scala:969)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$2(DeltaTableV2.scala:169)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:700)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$1(DeltaTableV2.scala:150)\n",
       "\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:83)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.deltaLog$lzycompute(DeltaTableV2.scala:150)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.deltaLog(DeltaTableV2.scala:148)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$8(DeltaTableV2.scala:264)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$withAdditionalSnapshotInitializationUsageLogData$1(DeltaLog.scala:968)\n",
       "\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
       "\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
       "\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.withAdditionalSnapshotInitializationUsageLogData(DeltaLog.scala:969)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$7(DeltaTableV2.scala:263)\n",
       "\t\tat scala.Option.orElse(Option.scala:447)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$5(DeltaTableV2.scala:263)\n",
       "\t\tat scala.Option.orElse(Option.scala:447)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$2(DeltaTableV2.scala:260)\n",
       "\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:83)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$1(DeltaTableV2.scala:290)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:700)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot$lzycompute(DeltaTableV2.scala:240)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot(DeltaTableV2.scala:238)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$tableSchema$2(DeltaTableV2.scala:317)\n",
       "\t\tat scala.Option.getOrElse(Option.scala:189)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.tableSchema$lzycompute(DeltaTableV2.scala:317)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.tableSchema(DeltaTableV2.scala:315)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.schema(DeltaTableV2.scala:322)\n",
       "\t\tat org.apache.spark.sql.connector.catalog.Table.columns(Table.java:68)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.columns(DeltaTableV2.scala:83)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.ResolvedTable$.create(v2ResolutionPlans.scala:231)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$4(Analyzer.scala:1600)\n",
       "\t\tat scala.Option.map(Option.scala:230)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$3(Analyzer.scala:1592)\n",
       "\t\tat scala.Option.orElse(Option.scala:447)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableOrView(Analyzer.scala:1587)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1556)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1466)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:182)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:171)\n",
       "\t\tat scala.PartialFunction.applyOrElse(PartialFunction.scala:127)\n",
       "\t\tat scala.PartialFunction.applyOrElse$(PartialFunction.scala:126)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.applyOrElse(AnalysisHelper.scala:171)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)\n",
       "\t\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:138)\n",
       "\t\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1330)\n",
       "\t\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1329)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.DescribeRelation.mapChildren(v2Commands.scala:726)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning(AnalysisHelper.scala:186)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning$(AnalysisHelper.scala:167)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithSubqueriesAndPruning(LogicalPlan.scala:42)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.resolveOperatorsUpWithSubqueriesAndPruningByConf(Analyzer.scala:1674)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply0(Analyzer.scala:1466)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1419)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1365)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:480)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:629)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:613)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:131)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:480)\n",
       "\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:479)\n",
       "\t\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n",
       "\t\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n",
       "\t\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:475)\n",
       "\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:452)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22(RuleExecutor.scala:585)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22$adapted(RuleExecutor.scala:585)\n",
       "\t\tat scala.collection.immutable.List.foreach(List.scala:431)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:585)\n",
       "\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:349)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:498)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:491)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:397)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:491)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:416)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:341)\n",
       "\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:219)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:341)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:252)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:96)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:131)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:87)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:478)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:478)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:294)\n",
       "\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:548)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:668)\n",
       "\t\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:151)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:668)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1307)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:661)\n",
       "\t\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:658)\n",
       "\t\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:658)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:288)\n",
       "\t\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:287)\n",
       "\t\tat scala.util.Try$.apply(Try.scala:213)\n",
       "\t\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)\n",
       "\t\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n",
       "\t\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n",
       "\t\t... 24 more\n",
       "</td><td>2025-08-13T14:52:12.808418Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>table</td><td>`dr_eo_amperity`.`silver`.`unified_coalesced`</td><td>clone</td><td>success</td><td>Table `dr_eo_amperity`.`silver`.`unified_coalesced` cloned successfully.</td><td>2025-08-13T14:52:09.960439Z</td><td>List(List(source_table_size, 25008603680), List(source_num_of_files, 234), List(num_of_synced_transactions, None), List(num_removed_files, 0), List(num_copied_files, 0), List(removed_files_size, 0), List(copied_files_size, 0))</td></tr><tr><td>DR_Clone_data_v2</td><td>table</td><td>`dr_eo_amperity`.`silver`.`unified_transactions`</td><td>clone</td><td>error</td><td>Failed to clone table `dr_eo_amperity`.`silver`.`unified_transactions`.. Error: [DELTA_CLONE_UNSUPPORTED_SOURCE] Unsupported DEEP clone source '_eo_amperity.silver.unified_transactions', whose format is deltasharing.\n",
       "The supported formats are 'delta', 'iceberg' and 'parquet'.</td><td>2025-08-13T14:52:08.040833Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>table</td><td>`dr_eo_amperity`.`silver`.`unified_scores`</td><td>clone</td><td>error</td><td>Failed to clone table `dr_eo_amperity`.`silver`.`unified_scores`.. Error: [DELTA_CLONE_UNSUPPORTED_SOURCE] Unsupported DEEP clone source '_eo_amperity.silver.unified_scores', whose format is deltasharing.\n",
       "The supported formats are 'delta', 'iceberg' and 'parquet'.</td><td>2025-08-13T14:52:07.910126Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>table/view</td><td>`accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample`.`forecast`.`us_postal_daily_metric`</td><td>clone/create</td><td>error</td><td>Failed to process table/view `accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample`.`forecast`.`us_postal_daily_metric`. Error: An error occurred while calling o427.sql.\n",
       ": com.databricks.sql.managedcatalog.acl.UnauthorizedAccessException: PERMISSION_DENIED: User does not have USE CATALOG on Catalog 'accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample'.\n",
       "\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:87)\n",
       "\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:63)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:251)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapExceptionBase(ManagedCatalogClientImpl.scala:7378)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:7364)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.getTableCredentials(ManagedCatalogClientImpl.scala:3951)\n",
       "\tat com.databricks.sql.managedcatalog.ManagedCatalogClient.getTemporaryCredentials(ManagedCatalogClient.scala:2620)\n",
       "\tat com.databricks.sql.managedcatalog.ManagedCatalogClient.getTemporaryCredentials$(ManagedCatalogClient.scala:2605)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.getTemporaryCredentials(ManagedCatalogClientImpl.scala:251)\n",
       "\tat com.databricks.unity.TempCredCache.$anonfun$getInternal$7(TemporaryCredentials.scala:448)\n",
       "\tat com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4724)\n",
       "\tat com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3522)\n",
       "\tat com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2315)\n",
       "\tat com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2278)\n",
       "\tat com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2193)\n",
       "\tat com.google.common.cache.LocalCache.get(LocalCache.java:3932)\n",
       "\tat com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4721)\n",
       "\tat com.databricks.unity.TempCredCache.liftedTree1$1(TemporaryCredentials.scala:447)\n",
       "\tat com.databricks.unity.TempCredCache.getInternal(TemporaryCredentials.scala:446)\n",
       "\tat com.databricks.unity.TempCredCache.get(TemporaryCredentials.scala:373)\n",
       "\tat com.databricks.unity.UnityCredentialManager.getTemporaryCredentials(CredentialManager.scala:488)\n",
       "\tat com.databricks.unity.CredentialManager$.getTemporaryCredentials(CredentialManager.scala:881)\n",
       "\tat com.databricks.unity.SAM.setConfFromCred$1(SAM.scala:338)\n",
       "\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$8(SAM.scala:353)\n",
       "\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$8$adapted(SAM.scala:352)\n",
       "\tat com.databricks.unity.TempCredRequest$.$anonfun$withFallbackOnOverlappingForParentPath$3(TemporaryCredentials.scala:147)\n",
       "\tat scala.util.Try$.apply(Try.scala:213)\n",
       "\tat com.databricks.unity.TempCredRequest$.withFallbackOnOverlappingForParentPath(TemporaryCredentials.scala:147)\n",
       "\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$7(SAM.scala:352)\n",
       "\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$7$adapted(SAM.scala:350)\n",
       "\tat scala.Option.map(Option.scala:230)\n",
       "\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$6(SAM.scala:350)\n",
       "\tat scala.Option.flatMap(Option.scala:271)\n",
       "\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$1(SAM.scala:349)\n",
       "\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$1$adapted(SAM.scala:336)\n",
       "\tat scala.Option.foreach(Option.scala:407)\n",
       "\tat com.databricks.unity.SAM.setFileSystemConfigs(SAM.scala:336)\n",
       "\tat com.databricks.unity.SAM.createDelegate(SAM.scala:388)\n",
       "\tat com.databricks.unity.SAM.createDelegate$(SAM.scala:372)\n",
       "\tat com.databricks.unity.TableSAM.createDelegate(SAM.scala:695)\n",
       "\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.createDelegate(CredentialScopeFileSystem.scala:74)\n",
       "\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.$anonfun$setDelegates$1(CredentialScopeFileSystem.scala:127)\n",
       "\tat com.databricks.sql.acl.fs.Lazy.apply(DelegatingFileSystem.scala:310)\n",
       "\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.readDelegateMakeQualified(CredentialScopeFileSystem.scala:282)\n",
       "\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.optimizedMakeQualifiedPath(CredentialScopeFileSystem.scala:277)\n",
       "\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.makeQualified(CredentialScopeFileSystem.scala:294)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaLog$.apply(DeltaLog.scala:1284)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaLog$.forTable(DeltaLog.scala:1087)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$5(DeltaTableV2.scala:175)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$withAdditionalSnapshotInitializationUsageLogData$1(DeltaLog.scala:968)\n",
       "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
       "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
       "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaLog$.withAdditionalSnapshotInitializationUsageLogData(DeltaLog.scala:969)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$2(DeltaTableV2.scala:169)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:700)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$1(DeltaTableV2.scala:150)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:83)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.deltaLog$lzycompute(DeltaTableV2.scala:150)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.deltaLog(DeltaTableV2.scala:148)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$8(DeltaTableV2.scala:264)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$withAdditionalSnapshotInitializationUsageLogData$1(DeltaLog.scala:968)\n",
       "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
       "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
       "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaLog$.withAdditionalSnapshotInitializationUsageLogData(DeltaLog.scala:969)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$7(DeltaTableV2.scala:263)\n",
       "\tat scala.Option.orElse(Option.scala:447)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$5(DeltaTableV2.scala:263)\n",
       "\tat scala.Option.orElse(Option.scala:447)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$2(DeltaTableV2.scala:260)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:83)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$1(DeltaTableV2.scala:290)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:700)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot$lzycompute(DeltaTableV2.scala:240)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot(DeltaTableV2.scala:238)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$tableSchema$2(DeltaTableV2.scala:317)\n",
       "\tat scala.Option.getOrElse(Option.scala:189)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.tableSchema$lzycompute(DeltaTableV2.scala:317)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.tableSchema(DeltaTableV2.scala:315)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.schema(DeltaTableV2.scala:322)\n",
       "\tat org.apache.spark.sql.connector.catalog.Table.columns(Table.java:68)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.columns(DeltaTableV2.scala:83)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.ResolvedTable$.create(v2ResolutionPlans.scala:231)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$4(Analyzer.scala:1600)\n",
       "\tat scala.Option.map(Option.scala:230)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$3(Analyzer.scala:1592)\n",
       "\tat scala.Option.orElse(Option.scala:447)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableOrView(Analyzer.scala:1587)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1556)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1466)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:182)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:171)\n",
       "\tat scala.PartialFunction.applyOrElse(PartialFunction.scala:127)\n",
       "\tat scala.PartialFunction.applyOrElse$(PartialFunction.scala:126)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.applyOrElse(AnalysisHelper.scala:171)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)\n",
       "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:138)\n",
       "\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1330)\n",
       "\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1329)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.DescribeRelation.mapChildren(v2Commands.scala:726)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning(AnalysisHelper.scala:186)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning$(AnalysisHelper.scala:167)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithSubqueriesAndPruning(LogicalPlan.scala:42)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.resolveOperatorsUpWithSubqueriesAndPruningByConf(Analyzer.scala:1674)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply0(Analyzer.scala:1466)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1419)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1365)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:480)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:629)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:613)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:131)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:480)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:479)\n",
       "\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n",
       "\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n",
       "\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:475)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:452)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22(RuleExecutor.scala:585)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22$adapted(RuleExecutor.scala:585)\n",
       "\tat scala.collection.immutable.List.foreach(List.scala:431)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:585)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:349)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:498)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:491)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:397)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:491)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:416)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:341)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:219)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:341)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:252)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:96)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:131)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:87)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:478)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:478)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:294)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:548)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:668)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:151)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:668)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1307)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:661)\n",
       "\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:658)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:658)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:288)\n",
       "\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:287)\n",
       "\tat scala.util.Try$.apply(Try.scala:213)\n",
       "\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)\n",
       "\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)\n",
       "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:321)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:267)\n",
       "\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$3(Dataset.scala:149)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n",
       "\tat org.apache.spark.sql.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1469)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1469)\n",
       "\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:141)\n",
       "\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:1048)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n",
       "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:1001)\n",
       "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:1071)\n",
       "\tat jdk.internal.reflect.GeneratedMethodAccessor593.invoke(Unknown Source)\n",
       "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
       "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
       "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n",
       "\tat py4j.Gateway.invoke(Gateway.java:306)\n",
       "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
       "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:197)\n",
       "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:117)\n",
       "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
       "\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\n",
       "\t\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:87)\n",
       "\t\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:63)\n",
       "\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:251)\n",
       "\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapExceptionBase(ManagedCatalogClientImpl.scala:7378)\n",
       "\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:7364)\n",
       "\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.getTableCredentials(ManagedCatalogClientImpl.scala:3951)\n",
       "\t\tat com.databricks.sql.managedcatalog.ManagedCatalogClient.getTemporaryCredentials(ManagedCatalogClient.scala:2620)\n",
       "\t\tat com.databricks.sql.managedcatalog.ManagedCatalogClient.getTemporaryCredentials$(ManagedCatalogClient.scala:2605)\n",
       "\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.getTemporaryCredentials(ManagedCatalogClientImpl.scala:251)\n",
       "\t\tat com.databricks.unity.TempCredCache.$anonfun$getInternal$7(TemporaryCredentials.scala:448)\n",
       "\t\tat com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4724)\n",
       "\t\tat com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3522)\n",
       "\t\tat com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2315)\n",
       "\t\tat com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2278)\n",
       "\t\tat com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2193)\n",
       "\t\tat com.google.common.cache.LocalCache.get(LocalCache.java:3932)\n",
       "\t\tat com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4721)\n",
       "\t\tat com.databricks.unity.TempCredCache.liftedTree1$1(TemporaryCredentials.scala:447)\n",
       "\t\tat com.databricks.unity.TempCredCache.getInternal(TemporaryCredentials.scala:446)\n",
       "\t\tat com.databricks.unity.TempCredCache.get(TemporaryCredentials.scala:373)\n",
       "\t\tat com.databricks.unity.UnityCredentialManager.getTemporaryCredentials(CredentialManager.scala:488)\n",
       "\t\tat com.databricks.unity.CredentialManager$.getTemporaryCredentials(CredentialManager.scala:881)\n",
       "\t\tat com.databricks.unity.SAM.setConfFromCred$1(SAM.scala:338)\n",
       "\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$8(SAM.scala:353)\n",
       "\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$8$adapted(SAM.scala:352)\n",
       "\t\tat com.databricks.unity.TempCredRequest$.$anonfun$withFallbackOnOverlappingForParentPath$3(TemporaryCredentials.scala:147)\n",
       "\t\tat scala.util.Try$.apply(Try.scala:213)\n",
       "\t\tat com.databricks.unity.TempCredRequest$.withFallbackOnOverlappingForParentPath(TemporaryCredentials.scala:147)\n",
       "\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$7(SAM.scala:352)\n",
       "\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$7$adapted(SAM.scala:350)\n",
       "\t\tat scala.Option.map(Option.scala:230)\n",
       "\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$6(SAM.scala:350)\n",
       "\t\tat scala.Option.flatMap(Option.scala:271)\n",
       "\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$1(SAM.scala:349)\n",
       "\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$1$adapted(SAM.scala:336)\n",
       "\t\tat scala.Option.foreach(Option.scala:407)\n",
       "\t\tat com.databricks.unity.SAM.setFileSystemConfigs(SAM.scala:336)\n",
       "\t\tat com.databricks.unity.SAM.createDelegate(SAM.scala:388)\n",
       "\t\tat com.databricks.unity.SAM.createDelegate$(SAM.scala:372)\n",
       "\t\tat com.databricks.unity.TableSAM.createDelegate(SAM.scala:695)\n",
       "\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.createDelegate(CredentialScopeFileSystem.scala:74)\n",
       "\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.$anonfun$setDelegates$1(CredentialScopeFileSystem.scala:127)\n",
       "\t\tat com.databricks.sql.acl.fs.Lazy.apply(DelegatingFileSystem.scala:310)\n",
       "\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.readDelegateMakeQualified(CredentialScopeFileSystem.scala:282)\n",
       "\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.optimizedMakeQualifiedPath(CredentialScopeFileSystem.scala:277)\n",
       "\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.makeQualified(CredentialScopeFileSystem.scala:294)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.apply(DeltaLog.scala:1284)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.forTable(DeltaLog.scala:1087)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$5(DeltaTableV2.scala:175)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$withAdditionalSnapshotInitializationUsageLogData$1(DeltaLog.scala:968)\n",
       "\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
       "\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
       "\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.withAdditionalSnapshotInitializationUsageLogData(DeltaLog.scala:969)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$2(DeltaTableV2.scala:169)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:700)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$1(DeltaTableV2.scala:150)\n",
       "\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:83)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.deltaLog$lzycompute(DeltaTableV2.scala:150)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.deltaLog(DeltaTableV2.scala:148)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$8(DeltaTableV2.scala:264)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$withAdditionalSnapshotInitializationUsageLogData$1(DeltaLog.scala:968)\n",
       "\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
       "\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
       "\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.withAdditionalSnapshotInitializationUsageLogData(DeltaLog.scala:969)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$7(DeltaTableV2.scala:263)\n",
       "\t\tat scala.Option.orElse(Option.scala:447)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$5(DeltaTableV2.scala:263)\n",
       "\t\tat scala.Option.orElse(Option.scala:447)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$2(DeltaTableV2.scala:260)\n",
       "\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:83)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$1(DeltaTableV2.scala:290)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:700)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot$lzycompute(DeltaTableV2.scala:240)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot(DeltaTableV2.scala:238)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$tableSchema$2(DeltaTableV2.scala:317)\n",
       "\t\tat scala.Option.getOrElse(Option.scala:189)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.tableSchema$lzycompute(DeltaTableV2.scala:317)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.tableSchema(DeltaTableV2.scala:315)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.schema(DeltaTableV2.scala:322)\n",
       "\t\tat org.apache.spark.sql.connector.catalog.Table.columns(Table.java:68)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.columns(DeltaTableV2.scala:83)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.ResolvedTable$.create(v2ResolutionPlans.scala:231)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$4(Analyzer.scala:1600)\n",
       "\t\tat scala.Option.map(Option.scala:230)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$3(Analyzer.scala:1592)\n",
       "\t\tat scala.Option.orElse(Option.scala:447)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableOrView(Analyzer.scala:1587)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1556)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1466)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:182)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:171)\n",
       "\t\tat scala.PartialFunction.applyOrElse(PartialFunction.scala:127)\n",
       "\t\tat scala.PartialFunction.applyOrElse$(PartialFunction.scala:126)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.applyOrElse(AnalysisHelper.scala:171)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)\n",
       "\t\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:138)\n",
       "\t\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1330)\n",
       "\t\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1329)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.DescribeRelation.mapChildren(v2Commands.scala:726)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning(AnalysisHelper.scala:186)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning$(AnalysisHelper.scala:167)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithSubqueriesAndPruning(LogicalPlan.scala:42)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.resolveOperatorsUpWithSubqueriesAndPruningByConf(Analyzer.scala:1674)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply0(Analyzer.scala:1466)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1419)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1365)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:480)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:629)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:613)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:131)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:480)\n",
       "\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:479)\n",
       "\t\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n",
       "\t\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n",
       "\t\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:475)\n",
       "\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:452)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22(RuleExecutor.scala:585)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22$adapted(RuleExecutor.scala:585)\n",
       "\t\tat scala.collection.immutable.List.foreach(List.scala:431)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:585)\n",
       "\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:349)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:498)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:491)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:397)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:491)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:416)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:341)\n",
       "\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:219)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:341)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:252)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:96)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:131)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:87)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:478)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:478)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:294)\n",
       "\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:548)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:668)\n",
       "\t\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:151)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:668)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1307)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:661)\n",
       "\t\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:658)\n",
       "\t\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:658)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:288)\n",
       "\t\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:287)\n",
       "\t\tat scala.util.Try$.apply(Try.scala:213)\n",
       "\t\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)\n",
       "\t\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n",
       "\t\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n",
       "\t\t... 24 more\n",
       "</td><td>2025-08-13T14:52:07.592439Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>table/view</td><td>`accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample`.`forecast`.`us_postal_daily_imperial`</td><td>clone/create</td><td>error</td><td>Failed to process table/view `accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample`.`forecast`.`us_postal_daily_imperial`. Error: An error occurred while calling o427.sql.\n",
       ": com.databricks.sql.managedcatalog.acl.UnauthorizedAccessException: PERMISSION_DENIED: User does not have USE CATALOG on Catalog 'accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample'.\n",
       "\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:87)\n",
       "\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:63)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:251)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapExceptionBase(ManagedCatalogClientImpl.scala:7378)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:7364)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.getTableCredentials(ManagedCatalogClientImpl.scala:3951)\n",
       "\tat com.databricks.sql.managedcatalog.ManagedCatalogClient.getTemporaryCredentials(ManagedCatalogClient.scala:2620)\n",
       "\tat com.databricks.sql.managedcatalog.ManagedCatalogClient.getTemporaryCredentials$(ManagedCatalogClient.scala:2605)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.getTemporaryCredentials(ManagedCatalogClientImpl.scala:251)\n",
       "\tat com.databricks.unity.TempCredCache.$anonfun$getInternal$7(TemporaryCredentials.scala:448)\n",
       "\tat com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4724)\n",
       "\tat com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3522)\n",
       "\tat com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2315)\n",
       "\tat com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2278)\n",
       "\tat com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2193)\n",
       "\tat com.google.common.cache.LocalCache.get(LocalCache.java:3932)\n",
       "\tat com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4721)\n",
       "\tat com.databricks.unity.TempCredCache.liftedTree1$1(TemporaryCredentials.scala:447)\n",
       "\tat com.databricks.unity.TempCredCache.getInternal(TemporaryCredentials.scala:446)\n",
       "\tat com.databricks.unity.TempCredCache.get(TemporaryCredentials.scala:373)\n",
       "\tat com.databricks.unity.UnityCredentialManager.getTemporaryCredentials(CredentialManager.scala:488)\n",
       "\tat com.databricks.unity.CredentialManager$.getTemporaryCredentials(CredentialManager.scala:881)\n",
       "\tat com.databricks.unity.SAM.setConfFromCred$1(SAM.scala:338)\n",
       "\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$8(SAM.scala:353)\n",
       "\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$8$adapted(SAM.scala:352)\n",
       "\tat com.databricks.unity.TempCredRequest$.$anonfun$withFallbackOnOverlappingForParentPath$3(TemporaryCredentials.scala:147)\n",
       "\tat scala.util.Try$.apply(Try.scala:213)\n",
       "\tat com.databricks.unity.TempCredRequest$.withFallbackOnOverlappingForParentPath(TemporaryCredentials.scala:147)\n",
       "\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$7(SAM.scala:352)\n",
       "\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$7$adapted(SAM.scala:350)\n",
       "\tat scala.Option.map(Option.scala:230)\n",
       "\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$6(SAM.scala:350)\n",
       "\tat scala.Option.flatMap(Option.scala:271)\n",
       "\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$1(SAM.scala:349)\n",
       "\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$1$adapted(SAM.scala:336)\n",
       "\tat scala.Option.foreach(Option.scala:407)\n",
       "\tat com.databricks.unity.SAM.setFileSystemConfigs(SAM.scala:336)\n",
       "\tat com.databricks.unity.SAM.createDelegate(SAM.scala:388)\n",
       "\tat com.databricks.unity.SAM.createDelegate$(SAM.scala:372)\n",
       "\tat com.databricks.unity.TableSAM.createDelegate(SAM.scala:695)\n",
       "\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.createDelegate(CredentialScopeFileSystem.scala:74)\n",
       "\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.$anonfun$setDelegates$1(CredentialScopeFileSystem.scala:127)\n",
       "\tat com.databricks.sql.acl.fs.Lazy.apply(DelegatingFileSystem.scala:310)\n",
       "\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.readDelegateMakeQualified(CredentialScopeFileSystem.scala:282)\n",
       "\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.optimizedMakeQualifiedPath(CredentialScopeFileSystem.scala:277)\n",
       "\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.makeQualified(CredentialScopeFileSystem.scala:294)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaLog$.apply(DeltaLog.scala:1284)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaLog$.forTable(DeltaLog.scala:1087)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$5(DeltaTableV2.scala:175)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$withAdditionalSnapshotInitializationUsageLogData$1(DeltaLog.scala:968)\n",
       "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
       "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
       "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaLog$.withAdditionalSnapshotInitializationUsageLogData(DeltaLog.scala:969)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$2(DeltaTableV2.scala:169)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:700)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$1(DeltaTableV2.scala:150)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:83)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.deltaLog$lzycompute(DeltaTableV2.scala:150)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.deltaLog(DeltaTableV2.scala:148)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$8(DeltaTableV2.scala:264)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$withAdditionalSnapshotInitializationUsageLogData$1(DeltaLog.scala:968)\n",
       "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
       "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
       "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaLog$.withAdditionalSnapshotInitializationUsageLogData(DeltaLog.scala:969)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$7(DeltaTableV2.scala:263)\n",
       "\tat scala.Option.orElse(Option.scala:447)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$5(DeltaTableV2.scala:263)\n",
       "\tat scala.Option.orElse(Option.scala:447)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$2(DeltaTableV2.scala:260)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:83)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$1(DeltaTableV2.scala:290)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:700)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot$lzycompute(DeltaTableV2.scala:240)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot(DeltaTableV2.scala:238)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$tableSchema$2(DeltaTableV2.scala:317)\n",
       "\tat scala.Option.getOrElse(Option.scala:189)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.tableSchema$lzycompute(DeltaTableV2.scala:317)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.tableSchema(DeltaTableV2.scala:315)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.schema(DeltaTableV2.scala:322)\n",
       "\tat org.apache.spark.sql.connector.catalog.Table.columns(Table.java:68)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.columns(DeltaTableV2.scala:83)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.ResolvedTable$.create(v2ResolutionPlans.scala:231)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$4(Analyzer.scala:1600)\n",
       "\tat scala.Option.map(Option.scala:230)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$3(Analyzer.scala:1592)\n",
       "\tat scala.Option.orElse(Option.scala:447)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableOrView(Analyzer.scala:1587)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1556)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1466)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:182)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:171)\n",
       "\tat scala.PartialFunction.applyOrElse(PartialFunction.scala:127)\n",
       "\tat scala.PartialFunction.applyOrElse$(PartialFunction.scala:126)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.applyOrElse(AnalysisHelper.scala:171)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)\n",
       "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:138)\n",
       "\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1330)\n",
       "\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1329)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.DescribeRelation.mapChildren(v2Commands.scala:726)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning(AnalysisHelper.scala:186)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning$(AnalysisHelper.scala:167)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithSubqueriesAndPruning(LogicalPlan.scala:42)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.resolveOperatorsUpWithSubqueriesAndPruningByConf(Analyzer.scala:1674)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply0(Analyzer.scala:1466)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1419)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1365)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:480)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:629)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:613)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:131)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:480)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:479)\n",
       "\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n",
       "\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n",
       "\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:475)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:452)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22(RuleExecutor.scala:585)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22$adapted(RuleExecutor.scala:585)\n",
       "\tat scala.collection.immutable.List.foreach(List.scala:431)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:585)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:349)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:498)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:491)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:397)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:491)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:416)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:341)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:219)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:341)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:252)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:96)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:131)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:87)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:478)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:478)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:294)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:548)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:668)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:151)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:668)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1307)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:661)\n",
       "\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:658)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:658)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:288)\n",
       "\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:287)\n",
       "\tat scala.util.Try$.apply(Try.scala:213)\n",
       "\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)\n",
       "\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)\n",
       "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:321)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:267)\n",
       "\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$3(Dataset.scala:149)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n",
       "\tat org.apache.spark.sql.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1469)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1469)\n",
       "\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:141)\n",
       "\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:1048)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n",
       "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:1001)\n",
       "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:1071)\n",
       "\tat jdk.internal.reflect.GeneratedMethodAccessor593.invoke(Unknown Source)\n",
       "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
       "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
       "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n",
       "\tat py4j.Gateway.invoke(Gateway.java:306)\n",
       "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
       "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:197)\n",
       "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:117)\n",
       "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
       "\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\n",
       "\t\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:87)\n",
       "\t\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:63)\n",
       "\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:251)\n",
       "\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapExceptionBase(ManagedCatalogClientImpl.scala:7378)\n",
       "\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:7364)\n",
       "\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.getTableCredentials(ManagedCatalogClientImpl.scala:3951)\n",
       "\t\tat com.databricks.sql.managedcatalog.ManagedCatalogClient.getTemporaryCredentials(ManagedCatalogClient.scala:2620)\n",
       "\t\tat com.databricks.sql.managedcatalog.ManagedCatalogClient.getTemporaryCredentials$(ManagedCatalogClient.scala:2605)\n",
       "\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.getTemporaryCredentials(ManagedCatalogClientImpl.scala:251)\n",
       "\t\tat com.databricks.unity.TempCredCache.$anonfun$getInternal$7(TemporaryCredentials.scala:448)\n",
       "\t\tat com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4724)\n",
       "\t\tat com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3522)\n",
       "\t\tat com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2315)\n",
       "\t\tat com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2278)\n",
       "\t\tat com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2193)\n",
       "\t\tat com.google.common.cache.LocalCache.get(LocalCache.java:3932)\n",
       "\t\tat com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4721)\n",
       "\t\tat com.databricks.unity.TempCredCache.liftedTree1$1(TemporaryCredentials.scala:447)\n",
       "\t\tat com.databricks.unity.TempCredCache.getInternal(TemporaryCredentials.scala:446)\n",
       "\t\tat com.databricks.unity.TempCredCache.get(TemporaryCredentials.scala:373)\n",
       "\t\tat com.databricks.unity.UnityCredentialManager.getTemporaryCredentials(CredentialManager.scala:488)\n",
       "\t\tat com.databricks.unity.CredentialManager$.getTemporaryCredentials(CredentialManager.scala:881)\n",
       "\t\tat com.databricks.unity.SAM.setConfFromCred$1(SAM.scala:338)\n",
       "\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$8(SAM.scala:353)\n",
       "\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$8$adapted(SAM.scala:352)\n",
       "\t\tat com.databricks.unity.TempCredRequest$.$anonfun$withFallbackOnOverlappingForParentPath$3(TemporaryCredentials.scala:147)\n",
       "\t\tat scala.util.Try$.apply(Try.scala:213)\n",
       "\t\tat com.databricks.unity.TempCredRequest$.withFallbackOnOverlappingForParentPath(TemporaryCredentials.scala:147)\n",
       "\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$7(SAM.scala:352)\n",
       "\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$7$adapted(SAM.scala:350)\n",
       "\t\tat scala.Option.map(Option.scala:230)\n",
       "\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$6(SAM.scala:350)\n",
       "\t\tat scala.Option.flatMap(Option.scala:271)\n",
       "\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$1(SAM.scala:349)\n",
       "\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$1$adapted(SAM.scala:336)\n",
       "\t\tat scala.Option.foreach(Option.scala:407)\n",
       "\t\tat com.databricks.unity.SAM.setFileSystemConfigs(SAM.scala:336)\n",
       "\t\tat com.databricks.unity.SAM.createDelegate(SAM.scala:388)\n",
       "\t\tat com.databricks.unity.SAM.createDelegate$(SAM.scala:372)\n",
       "\t\tat com.databricks.unity.TableSAM.createDelegate(SAM.scala:695)\n",
       "\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.createDelegate(CredentialScopeFileSystem.scala:74)\n",
       "\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.$anonfun$setDelegates$1(CredentialScopeFileSystem.scala:127)\n",
       "\t\tat com.databricks.sql.acl.fs.Lazy.apply(DelegatingFileSystem.scala:310)\n",
       "\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.readDelegateMakeQualified(CredentialScopeFileSystem.scala:282)\n",
       "\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.optimizedMakeQualifiedPath(CredentialScopeFileSystem.scala:277)\n",
       "\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.makeQualified(CredentialScopeFileSystem.scala:294)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.apply(DeltaLog.scala:1284)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.forTable(DeltaLog.scala:1087)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$5(DeltaTableV2.scala:175)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$withAdditionalSnapshotInitializationUsageLogData$1(DeltaLog.scala:968)\n",
       "\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
       "\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
       "\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.withAdditionalSnapshotInitializationUsageLogData(DeltaLog.scala:969)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$2(DeltaTableV2.scala:169)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:700)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$1(DeltaTableV2.scala:150)\n",
       "\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:83)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.deltaLog$lzycompute(DeltaTableV2.scala:150)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.deltaLog(DeltaTableV2.scala:148)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$8(DeltaTableV2.scala:264)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$withAdditionalSnapshotInitializationUsageLogData$1(DeltaLog.scala:968)\n",
       "\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
       "\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
       "\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.withAdditionalSnapshotInitializationUsageLogData(DeltaLog.scala:969)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$7(DeltaTableV2.scala:263)\n",
       "\t\tat scala.Option.orElse(Option.scala:447)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$5(DeltaTableV2.scala:263)\n",
       "\t\tat scala.Option.orElse(Option.scala:447)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$2(DeltaTableV2.scala:260)\n",
       "\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:83)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$1(DeltaTableV2.scala:290)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:700)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot$lzycompute(DeltaTableV2.scala:240)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot(DeltaTableV2.scala:238)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$tableSchema$2(DeltaTableV2.scala:317)\n",
       "\t\tat scala.Option.getOrElse(Option.scala:189)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.tableSchema$lzycompute(DeltaTableV2.scala:317)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.tableSchema(DeltaTableV2.scala:315)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.schema(DeltaTableV2.scala:322)\n",
       "\t\tat org.apache.spark.sql.connector.catalog.Table.columns(Table.java:68)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.columns(DeltaTableV2.scala:83)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.ResolvedTable$.create(v2ResolutionPlans.scala:231)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$4(Analyzer.scala:1600)\n",
       "\t\tat scala.Option.map(Option.scala:230)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$3(Analyzer.scala:1592)\n",
       "\t\tat scala.Option.orElse(Option.scala:447)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableOrView(Analyzer.scala:1587)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1556)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1466)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:182)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:171)\n",
       "\t\tat scala.PartialFunction.applyOrElse(PartialFunction.scala:127)\n",
       "\t\tat scala.PartialFunction.applyOrElse$(PartialFunction.scala:126)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.applyOrElse(AnalysisHelper.scala:171)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)\n",
       "\t\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:138)\n",
       "\t\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1330)\n",
       "\t\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1329)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.DescribeRelation.mapChildren(v2Commands.scala:726)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning(AnalysisHelper.scala:186)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning$(AnalysisHelper.scala:167)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithSubqueriesAndPruning(LogicalPlan.scala:42)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.resolveOperatorsUpWithSubqueriesAndPruningByConf(Analyzer.scala:1674)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply0(Analyzer.scala:1466)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1419)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1365)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:480)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:629)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:613)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:131)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:480)\n",
       "\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:479)\n",
       "\t\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n",
       "\t\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n",
       "\t\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:475)\n",
       "\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:452)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22(RuleExecutor.scala:585)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22$adapted(RuleExecutor.scala:585)\n",
       "\t\tat scala.collection.immutable.List.foreach(List.scala:431)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:585)\n",
       "\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:349)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:498)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:491)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:397)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:491)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:416)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:341)\n",
       "\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:219)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:341)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:252)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:96)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:131)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:87)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:478)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:478)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:294)\n",
       "\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:548)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:668)\n",
       "\t\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:151)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:668)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1307)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:661)\n",
       "\t\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:658)\n",
       "\t\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:658)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:288)\n",
       "\t\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:287)\n",
       "\t\tat scala.util.Try$.apply(Try.scala:213)\n",
       "\t\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)\n",
       "\t\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n",
       "\t\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n",
       "\t\t... 24 more\n",
       "</td><td>2025-08-13T14:52:07.57649Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>table</td><td>`dr_eo_amperity`.`silver`.`unified_merged`</td><td>clone</td><td>error</td><td>Failed to clone table `dr_eo_amperity`.`silver`.`unified_merged`.. Error: [DELTA_CLONE_UNSUPPORTED_SOURCE] Unsupported DEEP clone source '_eo_amperity.silver.unified_merged', whose format is deltasharing.\n",
       "The supported formats are 'delta', 'iceberg' and 'parquet'.</td><td>2025-08-13T14:51:51.756756Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>table</td><td>`dr_eo_amperity`.`silver`.`unified_loyalty_events`</td><td>clone</td><td>error</td><td>Failed to clone table `dr_eo_amperity`.`silver`.`unified_loyalty_events`.. Error: [DELTA_CLONE_UNSUPPORTED_SOURCE] Unsupported DEEP clone source '_eo_amperity.silver.unified_loyalty_events', whose format is deltasharing.\n",
       "The supported formats are 'delta', 'iceberg' and 'parquet'.</td><td>2025-08-13T14:51:51.697674Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>table</td><td>`dr_eo_amperity`.`silver`.`unified_paid_media`</td><td>clone</td><td>error</td><td>Failed to clone table `dr_eo_amperity`.`silver`.`unified_paid_media`.. Error: [DELTA_CLONE_UNSUPPORTED_SOURCE] Unsupported DEEP clone source '_eo_amperity.silver.unified_paid_media', whose format is deltasharing.\n",
       "The supported formats are 'delta', 'iceberg' and 'parquet'.</td><td>2025-08-13T14:51:51.683968Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>table</td><td>`dr_eo_amperity`.`silver`.`unified_loyalty`</td><td>clone</td><td>error</td><td>Failed to clone table `dr_eo_amperity`.`silver`.`unified_loyalty`.. Error: [DELTA_CLONE_UNSUPPORTED_SOURCE] Unsupported DEEP clone source '_eo_amperity.silver.unified_loyalty', whose format is deltasharing.\n",
       "The supported formats are 'delta', 'iceberg' and 'parquet'.</td><td>2025-08-13T14:51:51.367173Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>table</td><td>`dr_eo_amperity`.`silver`.`unified_itemized_transactions`</td><td>clone</td><td>error</td><td>Failed to clone table `dr_eo_amperity`.`silver`.`unified_itemized_transactions`.. Error: [DELTA_CLONE_UNSUPPORTED_SOURCE] Unsupported DEEP clone source '_eo_amperity.silver.unified_itemized_transactions', whose format is deltasharing.\n",
       "The supported formats are 'delta', 'iceberg' and 'parquet'.</td><td>2025-08-13T14:51:47.305991Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>table</td><td>`dr_eo_amperity`.`silver`.`unified_household`</td><td>clone</td><td>error</td><td>Failed to clone table `dr_eo_amperity`.`silver`.`unified_household`.. Error: [DELTA_CLONE_UNSUPPORTED_SOURCE] Unsupported DEEP clone source '_eo_amperity.silver.unified_household', whose format is deltasharing.\n",
       "The supported formats are 'delta', 'iceberg' and 'parquet'.</td><td>2025-08-13T14:51:47.113467Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>table</td><td>`dr_eo_amperity`.`silver`.`unified_customer`</td><td>clone</td><td>error</td><td>Failed to clone table `dr_eo_amperity`.`silver`.`unified_customer`.. Error: [DELTA_CLONE_UNSUPPORTED_SOURCE] Unsupported DEEP clone source '_eo_amperity.silver.unified_customer', whose format is deltasharing.\n",
       "The supported formats are 'delta', 'iceberg' and 'parquet'.</td><td>2025-08-13T14:51:46.924336Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>table</td><td>`dr_eo_amperity`.`silver`.`unified_compliance`</td><td>clone</td><td>error</td><td>Failed to clone table `dr_eo_amperity`.`silver`.`unified_compliance`.. Error: [DELTA_CLONE_UNSUPPORTED_SOURCE] Unsupported DEEP clone source '_eo_amperity.silver.unified_compliance', whose format is deltasharing.\n",
       "The supported formats are 'delta', 'iceberg' and 'parquet'.</td><td>2025-08-13T14:51:46.836848Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>table</td><td>`dr_eo_amperity`.`silver`.`fiscal_calendar`</td><td>clone</td><td>error</td><td>Failed to clone table `dr_eo_amperity`.`silver`.`fiscal_calendar`.. Error: [DELTA_CLONE_UNSUPPORTED_SOURCE] Unsupported DEEP clone source '_eo_amperity.silver.fiscal_calendar', whose format is deltasharing.\n",
       "The supported formats are 'delta', 'iceberg' and 'parquet'.</td><td>2025-08-13T14:51:42.681901Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>table</td><td>`dr_eo_amperity`.`default`.`transaction_attributes_extended`</td><td>clone</td><td>error</td><td>Failed to clone table `dr_eo_amperity`.`default`.`transaction_attributes_extended`.. Error: [DELTA_CLONE_UNSUPPORTED_SOURCE] Unsupported DEEP clone source '_eo_amperity.default.transaction_attributes_extended', whose format is deltasharing.\n",
       "The supported formats are 'delta', 'iceberg' and 'parquet'.</td><td>2025-08-13T14:51:42.606026Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>table</td><td>`dr_eo_amperity`.`default`.`unified_transactions_reporting`</td><td>clone</td><td>error</td><td>Failed to clone table `dr_eo_amperity`.`default`.`unified_transactions_reporting`.. Error: [DELTA_CLONE_UNSUPPORTED_SOURCE] Unsupported DEEP clone source '_eo_amperity.default.unified_transactions_reporting', whose format is deltasharing.\n",
       "The supported formats are 'delta', 'iceberg' and 'parquet'.</td><td>2025-08-13T14:51:42.545915Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>table</td><td>`dr_eo_amperity`.`default`.`transaction_attributes`</td><td>clone</td><td>error</td><td>Failed to clone table `dr_eo_amperity`.`default`.`transaction_attributes`.. Error: [DELTA_CLONE_UNSUPPORTED_SOURCE] Unsupported DEEP clone source '_eo_amperity.default.transaction_attributes', whose format is deltasharing.\n",
       "The supported formats are 'delta', 'iceberg' and 'parquet'.</td><td>2025-08-13T14:51:42.521191Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>table</td><td>`dr_eo_amperity`.`default`.`predicted_clv_attributes`</td><td>clone</td><td>error</td><td>Failed to clone table `dr_eo_amperity`.`default`.`predicted_clv_attributes`.. Error: [DELTA_CLONE_UNSUPPORTED_SOURCE] Unsupported DEEP clone source '_eo_amperity.default.predicted_clv_attributes', whose format is deltasharing.\n",
       "The supported formats are 'delta', 'iceberg' and 'parquet'.</td><td>2025-08-13T14:51:42.271169Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>table</td><td>`dr_eo_amperity`.`bronze`.`wifi_connections`</td><td>clone</td><td>success</td><td>Table `dr_eo_amperity`.`bronze`.`wifi_connections` cloned successfully.</td><td>2025-08-13T14:51:39.568689Z</td><td>List(List(source_table_size, 7882742359), List(source_num_of_files, 52), List(num_of_synced_transactions, None), List(num_removed_files, 0), List(num_copied_files, 0), List(removed_files_size, 0), List(copied_files_size, 0))</td></tr><tr><td>DR_Clone_data_v2</td><td>table</td><td>`dr_eo_amperity`.`bronze`.`web_visitors`</td><td>clone</td><td>success</td><td>Table `dr_eo_amperity`.`bronze`.`web_visitors` cloned successfully.</td><td>2025-08-13T14:51:39.481785Z</td><td>List(List(source_table_size, 334349516), List(source_num_of_files, 7), List(num_of_synced_transactions, None), List(num_removed_files, 0), List(num_copied_files, 0), List(removed_files_size, 0), List(copied_files_size, 0))</td></tr><tr><td>DR_Clone_data_v2</td><td>table</td><td>`dr_eo_amperity`.`default`.`email_engagement_summary`</td><td>clone</td><td>error</td><td>Failed to clone table `dr_eo_amperity`.`default`.`email_engagement_summary`.. Error: [DELTA_CLONE_UNSUPPORTED_SOURCE] Unsupported DEEP clone source '_eo_amperity.default.email_engagement_summary', whose format is deltasharing.\n",
       "The supported formats are 'delta', 'iceberg' and 'parquet'.</td><td>2025-08-13T14:51:35.794341Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>table</td><td>`dr_eo_amperity`.`default`.`predicted_affinity`</td><td>clone</td><td>error</td><td>Failed to clone table `dr_eo_amperity`.`default`.`predicted_affinity`.. Error: [DELTA_CLONE_UNSUPPORTED_SOURCE] Unsupported DEEP clone source '_eo_amperity.default.predicted_affinity', whose format is deltasharing.\n",
       "The supported formats are 'delta', 'iceberg' and 'parquet'.</td><td>2025-08-13T14:51:35.79349Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>table</td><td>`dr_eo_amperity`.`default`.`email_engagement_attributes`</td><td>clone</td><td>error</td><td>Failed to clone table `dr_eo_amperity`.`default`.`email_engagement_attributes`.. Error: [DELTA_CLONE_UNSUPPORTED_SOURCE] Unsupported DEEP clone source '_eo_amperity.default.email_engagement_attributes', whose format is deltasharing.\n",
       "The supported formats are 'delta', 'iceberg' and 'parquet'.</td><td>2025-08-13T14:51:35.52174Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>table</td><td>`dr_eo_amperity`.`default`.`customer_attributes`</td><td>clone</td><td>error</td><td>Failed to clone table `dr_eo_amperity`.`default`.`customer_attributes`.. Error: [DELTA_CLONE_UNSUPPORTED_SOURCE] Unsupported DEEP clone source '_eo_amperity.default.customer_attributes', whose format is deltasharing.\n",
       "The supported formats are 'delta', 'iceberg' and 'parquet'.</td><td>2025-08-13T14:51:00.347108Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>table</td><td>`dr_eo_amperity`.`default`.`customer_360`</td><td>clone</td><td>error</td><td>Failed to clone table `dr_eo_amperity`.`default`.`customer_360`.. Error: [DELTA_CLONE_UNSUPPORTED_SOURCE] Unsupported DEEP clone source '_eo_amperity.default.customer_360', whose format is deltasharing.\n",
       "The supported formats are 'delta', 'iceberg' and 'parquet'.</td><td>2025-08-13T14:51:00.340051Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>table</td><td>`dr_eo_amperity`.`default`.`analytics_tae`</td><td>clone</td><td>error</td><td>Failed to clone table `dr_eo_amperity`.`default`.`analytics_tae`.. Error: [DELTA_CLONE_UNSUPPORTED_SOURCE] Unsupported DEEP clone source '_eo_amperity.default.analytics_tae', whose format is deltasharing.\n",
       "The supported formats are 'delta', 'iceberg' and 'parquet'.</td><td>2025-08-13T14:51:00.195134Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>table</td><td>`dr_eo_amperity`.`bronze`.`loyalty_members`</td><td>clone</td><td>error</td><td>Failed to clone table `dr_eo_amperity`.`bronze`.`loyalty_members`.. Error: [DELTA_CLONE_UNSUPPORTED_SOURCE] Unsupported DEEP clone source '_eo_amperity.bronze.loyalty_members', whose format is deltasharing.\n",
       "The supported formats are 'delta', 'iceberg' and 'parquet'.</td><td>2025-08-13T14:50:56.183075Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>table</td><td>`dr_eo_amperity`.`bronze`.`pos_customers`</td><td>clone</td><td>error</td><td>Failed to clone table `dr_eo_amperity`.`bronze`.`pos_customers`.. Error: [DELTA_CLONE_UNSUPPORTED_SOURCE] Unsupported DEEP clone source '_eo_amperity.bronze.pos_customers', whose format is deltasharing.\n",
       "The supported formats are 'delta', 'iceberg' and 'parquet'.</td><td>2025-08-13T14:50:56.0049Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>table</td><td>`dr_eo_amperity`.`bronze`.`e_commerce_customers`</td><td>clone</td><td>error</td><td>Failed to clone table `dr_eo_amperity`.`bronze`.`e_commerce_customers`.. Error: [DELTA_CLONE_UNSUPPORTED_SOURCE] Unsupported DEEP clone source '_eo_amperity.bronze.e_commerce_customers', whose format is deltasharing.\n",
       "The supported formats are 'delta', 'iceberg' and 'parquet'.</td><td>2025-08-13T14:50:55.962848Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>catalog_pair</td><td>satya_share -> satya_dr_catalog</td><td>process</td><td>sequential_complete</td><td>Finished sequential processing for catalog pair: satya_share -> satya_dr_catalog</td><td>2025-08-13T14:50:53.422799Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>schema</td><td>`satya_dr_catalog`.`satyendranath_sure`</td><td>change_ownership</td><td>skipped</td><td>No ownership mapping found for this schema. Skipping ownership change.</td><td>2025-08-13T14:50:51.832389Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>schema</td><td>`satya_dr_catalog`.`satyendranath_sure`</td><td>create</td><td>success</td><td>Schema `satya_dr_catalog`.`satyendranath_sure` created or already exists.</td><td>2025-08-13T14:50:50.430502Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>schema</td><td>`satya_share`.`information_schema`</td><td>skip</td><td>success</td><td>Skipping `information_schema`.</td><td>2025-08-13T14:50:48.987385Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>catalog</td><td>`satya_dr_catalog`</td><td>create</td><td>success</td><td>Catalog `satya_dr_catalog` created or already exists.</td><td>2025-08-13T14:50:46.482613Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>catalog_pair</td><td>satya_share -> satya_dr_catalog</td><td>process</td><td>start</td><td>Starting processing for catalog pair: satya_share -> satya_dr_catalog</td><td>2025-08-13T14:50:44.845345Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>catalog</td><td>`test_not`</td><td>get_schemas</td><td>error</td><td>Failed to get schemas from source catalog `test_not`. Error: [NO_SUCH_CATALOG_EXCEPTION] Catalog 'test_not' was not found. Please verify the catalog name and then retry the query or command again. SQLSTATE: 42704</td><td>2025-08-13T14:50:42.940357Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>catalog</td><td>`dr_test_not`</td><td>create</td><td>success</td><td>Catalog `dr_test_not` created or already exists.</td><td>2025-08-13T14:50:41.400151Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>catalog_pair</td><td>test_not -> dr_test_not</td><td>process</td><td>start</td><td>Starting processing for catalog pair: test_not -> dr_test_not</td><td>2025-08-13T14:50:39.749677Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>catalog_pair</td><td>accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample -> dr_accuweather</td><td>process</td><td>sequential_complete</td><td>Finished sequential processing for catalog pair: accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample -> dr_accuweather</td><td>2025-08-13T14:50:38.223289Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>schema</td><td>`accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample`.`information_schema`</td><td>skip</td><td>success</td><td>Skipping `information_schema`.</td><td>2025-08-13T14:50:36.711376Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>schema</td><td>`dr_accuweather`.`forecast`</td><td>change_ownership</td><td>skipped</td><td>No ownership mapping found for this schema. Skipping ownership change.</td><td>2025-08-13T14:50:34.051606Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>schema</td><td>`dr_accuweather`.`forecast`</td><td>create</td><td>success</td><td>Schema `dr_accuweather`.`forecast` created or already exists.</td><td>2025-08-13T14:50:32.192943Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>catalog</td><td>`dr_accuweather`</td><td>create</td><td>success</td><td>Catalog `dr_accuweather` created or already exists.</td><td>2025-08-13T14:50:29.233097Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>catalog_pair</td><td>accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample -> dr_accuweather</td><td>process</td><td>start</td><td>Starting processing for catalog pair: accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample -> dr_accuweather</td><td>2025-08-13T14:50:27.452586Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>catalog_pair</td><td>_eo_amperity -> dr_eo_amperity</td><td>process</td><td>sequential_complete</td><td>Finished sequential processing for catalog pair: _eo_amperity -> dr_eo_amperity</td><td>2025-08-13T14:50:26.026816Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>schema</td><td>`dr_eo_amperity`.`silver`</td><td>change_ownership</td><td>success</td><td>Ownership of schema `dr_eo_amperity`.`silver` changed to `grp_test_clt_usa`.</td><td>2025-08-13T14:50:24.429794Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>schema</td><td>`dr_eo_amperity`.`silver`</td><td>create</td><td>success</td><td>Schema `dr_eo_amperity`.`silver` created or already exists.</td><td>2025-08-13T14:50:22.368961Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>schema</td><td>`_eo_amperity`.`information_schema`</td><td>skip</td><td>success</td><td>Skipping `information_schema`.</td><td>2025-08-13T14:50:20.451951Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>schema</td><td>`dr_eo_amperity`.`default`</td><td>change_ownership</td><td>success</td><td>Ownership of schema `dr_eo_amperity`.`default` changed to `grp_test_clt_usa`.</td><td>2025-08-13T14:50:18.733137Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>schema</td><td>`dr_eo_amperity`.`default`</td><td>create</td><td>success</td><td>Schema `dr_eo_amperity`.`default` created or already exists.</td><td>2025-08-13T14:50:16.860986Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>schema</td><td>`dr_eo_amperity`.`bronze`</td><td>change_ownership</td><td>success</td><td>Ownership of schema `dr_eo_amperity`.`bronze` changed to `grp_test_clt_usa`.</td><td>2025-08-13T14:50:14.866714Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>schema</td><td>`dr_eo_amperity`.`bronze`</td><td>create</td><td>success</td><td>Schema `dr_eo_amperity`.`bronze` created or already exists.</td><td>2025-08-13T14:50:12.872357Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>catalog</td><td>`dr_eo_amperity`</td><td>create</td><td>success</td><td>Catalog `dr_eo_amperity` created or already exists.</td><td>2025-08-13T14:50:10.517933Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>catalog_pair</td><td>_eo_amperity -> dr_eo_amperity</td><td>process</td><td>start</td><td>Starting processing for catalog pair: _eo_amperity -> dr_eo_amperity</td><td>2025-08-13T14:49:55.847726Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>table</td><td>`satya_dr_catalog`.`satyendranath_sure`.`satya_managed_table`</td><td>clone</td><td>success</td><td>Table `satya_dr_catalog`.`satyendranath_sure`.`satya_managed_table` cloned successfully.</td><td>2025-08-13T14:38:23.717714Z</td><td>List(List(source_table_size, 827), List(source_num_of_files, 1), List(num_of_synced_transactions, None), List(num_removed_files, 0), List(num_copied_files, 0), List(removed_files_size, 0), List(copied_files_size, 0))</td></tr><tr><td>DR_Clone_data_v2</td><td>table</td><td>`satya_dr_catalog`.`satyendranath_sure`.`satya_external_table`</td><td>clone</td><td>success</td><td>Table `satya_dr_catalog`.`satyendranath_sure`.`satya_external_table` cloned successfully.</td><td>2025-08-13T14:38:22.387917Z</td><td>List(List(source_table_size, 826), List(source_num_of_files, 1), List(num_of_synced_transactions, None), List(num_removed_files, 0), List(num_copied_files, 0), List(removed_files_size, 0), List(copied_files_size, 0))</td></tr><tr><td>DR_Clone_data_v2</td><td>table/view</td><td>`accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample`.`forecast`.`us_postal_daynight_imperial`</td><td>clone/create</td><td>error</td><td>Failed to process table/view `accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample`.`forecast`.`us_postal_daynight_imperial`. Error: An error occurred while calling o427.sql.\n",
       ": com.databricks.sql.managedcatalog.acl.UnauthorizedAccessException: PERMISSION_DENIED: User does not have USE CATALOG on Catalog 'accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample'.\n",
       "\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:87)\n",
       "\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:63)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:251)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapExceptionBase(ManagedCatalogClientImpl.scala:7378)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:7364)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.getTableCredentials(ManagedCatalogClientImpl.scala:3951)\n",
       "\tat com.databricks.sql.managedcatalog.ManagedCatalogClient.getTemporaryCredentials(ManagedCatalogClient.scala:2620)\n",
       "\tat com.databricks.sql.managedcatalog.ManagedCatalogClient.getTemporaryCredentials$(ManagedCatalogClient.scala:2605)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.getTemporaryCredentials(ManagedCatalogClientImpl.scala:251)\n",
       "\tat com.databricks.unity.TempCredCache.$anonfun$getInternal$7(TemporaryCredentials.scala:448)\n",
       "\tat com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4724)\n",
       "\tat com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3522)\n",
       "\tat com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2315)\n",
       "\tat com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2278)\n",
       "\tat com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2193)\n",
       "\tat com.google.common.cache.LocalCache.get(LocalCache.java:3932)\n",
       "\tat com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4721)\n",
       "\tat com.databricks.unity.TempCredCache.liftedTree1$1(TemporaryCredentials.scala:447)\n",
       "\tat com.databricks.unity.TempCredCache.getInternal(TemporaryCredentials.scala:446)\n",
       "\tat com.databricks.unity.TempCredCache.get(TemporaryCredentials.scala:373)\n",
       "\tat com.databricks.unity.UnityCredentialManager.getTemporaryCredentials(CredentialManager.scala:488)\n",
       "\tat com.databricks.unity.CredentialManager$.getTemporaryCredentials(CredentialManager.scala:881)\n",
       "\tat com.databricks.unity.SAM.setConfFromCred$1(SAM.scala:338)\n",
       "\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$8(SAM.scala:353)\n",
       "\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$8$adapted(SAM.scala:352)\n",
       "\tat com.databricks.unity.TempCredRequest$.$anonfun$withFallbackOnOverlappingForParentPath$3(TemporaryCredentials.scala:147)\n",
       "\tat scala.util.Try$.apply(Try.scala:213)\n",
       "\tat com.databricks.unity.TempCredRequest$.withFallbackOnOverlappingForParentPath(TemporaryCredentials.scala:147)\n",
       "\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$7(SAM.scala:352)\n",
       "\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$7$adapted(SAM.scala:350)\n",
       "\tat scala.Option.map(Option.scala:230)\n",
       "\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$6(SAM.scala:350)\n",
       "\tat scala.Option.flatMap(Option.scala:271)\n",
       "\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$1(SAM.scala:349)\n",
       "\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$1$adapted(SAM.scala:336)\n",
       "\tat scala.Option.foreach(Option.scala:407)\n",
       "\tat com.databricks.unity.SAM.setFileSystemConfigs(SAM.scala:336)\n",
       "\tat com.databricks.unity.SAM.createDelegate(SAM.scala:388)\n",
       "\tat com.databricks.unity.SAM.createDelegate$(SAM.scala:372)\n",
       "\tat com.databricks.unity.TableSAM.createDelegate(SAM.scala:695)\n",
       "\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.createDelegate(CredentialScopeFileSystem.scala:74)\n",
       "\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.$anonfun$setDelegates$1(CredentialScopeFileSystem.scala:127)\n",
       "\tat com.databricks.sql.acl.fs.Lazy.apply(DelegatingFileSystem.scala:310)\n",
       "\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.readDelegateMakeQualified(CredentialScopeFileSystem.scala:282)\n",
       "\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.optimizedMakeQualifiedPath(CredentialScopeFileSystem.scala:277)\n",
       "\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.makeQualified(CredentialScopeFileSystem.scala:294)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaLog$.apply(DeltaLog.scala:1284)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaLog$.forTable(DeltaLog.scala:1087)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$5(DeltaTableV2.scala:175)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$withAdditionalSnapshotInitializationUsageLogData$1(DeltaLog.scala:968)\n",
       "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
       "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
       "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaLog$.withAdditionalSnapshotInitializationUsageLogData(DeltaLog.scala:969)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$2(DeltaTableV2.scala:169)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:700)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$1(DeltaTableV2.scala:150)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:83)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.deltaLog$lzycompute(DeltaTableV2.scala:150)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.deltaLog(DeltaTableV2.scala:148)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$8(DeltaTableV2.scala:264)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$withAdditionalSnapshotInitializationUsageLogData$1(DeltaLog.scala:968)\n",
       "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
       "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
       "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaLog$.withAdditionalSnapshotInitializationUsageLogData(DeltaLog.scala:969)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$7(DeltaTableV2.scala:263)\n",
       "\tat scala.Option.orElse(Option.scala:447)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$5(DeltaTableV2.scala:263)\n",
       "\tat scala.Option.orElse(Option.scala:447)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$2(DeltaTableV2.scala:260)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:83)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$1(DeltaTableV2.scala:290)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:700)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot$lzycompute(DeltaTableV2.scala:240)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot(DeltaTableV2.scala:238)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$tableSchema$2(DeltaTableV2.scala:317)\n",
       "\tat scala.Option.getOrElse(Option.scala:189)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.tableSchema$lzycompute(DeltaTableV2.scala:317)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.tableSchema(DeltaTableV2.scala:315)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.schema(DeltaTableV2.scala:322)\n",
       "\tat org.apache.spark.sql.connector.catalog.Table.columns(Table.java:68)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.columns(DeltaTableV2.scala:83)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.ResolvedTable$.create(v2ResolutionPlans.scala:231)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$4(Analyzer.scala:1600)\n",
       "\tat scala.Option.map(Option.scala:230)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$3(Analyzer.scala:1592)\n",
       "\tat scala.Option.orElse(Option.scala:447)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableOrView(Analyzer.scala:1587)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1556)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1466)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:182)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:171)\n",
       "\tat scala.PartialFunction.applyOrElse(PartialFunction.scala:127)\n",
       "\tat scala.PartialFunction.applyOrElse$(PartialFunction.scala:126)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.applyOrElse(AnalysisHelper.scala:171)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)\n",
       "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:138)\n",
       "\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1330)\n",
       "\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1329)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.DescribeRelation.mapChildren(v2Commands.scala:726)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning(AnalysisHelper.scala:186)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning$(AnalysisHelper.scala:167)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithSubqueriesAndPruning(LogicalPlan.scala:42)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.resolveOperatorsUpWithSubqueriesAndPruningByConf(Analyzer.scala:1674)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply0(Analyzer.scala:1466)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1419)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1365)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:480)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:629)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:613)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:131)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:480)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:479)\n",
       "\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n",
       "\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n",
       "\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:475)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:452)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22(RuleExecutor.scala:585)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22$adapted(RuleExecutor.scala:585)\n",
       "\tat scala.collection.immutable.List.foreach(List.scala:431)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:585)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:349)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:498)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:491)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:397)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:491)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:416)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:341)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:219)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:341)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:252)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:96)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:131)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:87)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:478)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:478)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:294)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:548)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:668)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:151)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:668)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1307)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:661)\n",
       "\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:658)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:658)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:288)\n",
       "\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:287)\n",
       "\tat scala.util.Try$.apply(Try.scala:213)\n",
       "\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)\n",
       "\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)\n",
       "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:321)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:267)\n",
       "\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$3(Dataset.scala:149)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n",
       "\tat org.apache.spark.sql.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1469)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1469)\n",
       "\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:141)\n",
       "\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:1048)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n",
       "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:1001)\n",
       "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:1071)\n",
       "\tat jdk.internal.reflect.GeneratedMethodAccessor593.invoke(Unknown Source)\n",
       "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
       "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
       "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n",
       "\tat py4j.Gateway.invoke(Gateway.java:306)\n",
       "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
       "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:197)\n",
       "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:117)\n",
       "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
       "\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\n",
       "\t\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:87)\n",
       "\t\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:63)\n",
       "\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:251)\n",
       "\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapExceptionBase(ManagedCatalogClientImpl.scala:7378)\n",
       "\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:7364)\n",
       "\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.getTableCredentials(ManagedCatalogClientImpl.scala:3951)\n",
       "\t\tat com.databricks.sql.managedcatalog.ManagedCatalogClient.getTemporaryCredentials(ManagedCatalogClient.scala:2620)\n",
       "\t\tat com.databricks.sql.managedcatalog.ManagedCatalogClient.getTemporaryCredentials$(ManagedCatalogClient.scala:2605)\n",
       "\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.getTemporaryCredentials(ManagedCatalogClientImpl.scala:251)\n",
       "\t\tat com.databricks.unity.TempCredCache.$anonfun$getInternal$7(TemporaryCredentials.scala:448)\n",
       "\t\tat com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4724)\n",
       "\t\tat com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3522)\n",
       "\t\tat com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2315)\n",
       "\t\tat com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2278)\n",
       "\t\tat com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2193)\n",
       "\t\tat com.google.common.cache.LocalCache.get(LocalCache.java:3932)\n",
       "\t\tat com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4721)\n",
       "\t\tat com.databricks.unity.TempCredCache.liftedTree1$1(TemporaryCredentials.scala:447)\n",
       "\t\tat com.databricks.unity.TempCredCache.getInternal(TemporaryCredentials.scala:446)\n",
       "\t\tat com.databricks.unity.TempCredCache.get(TemporaryCredentials.scala:373)\n",
       "\t\tat com.databricks.unity.UnityCredentialManager.getTemporaryCredentials(CredentialManager.scala:488)\n",
       "\t\tat com.databricks.unity.CredentialManager$.getTemporaryCredentials(CredentialManager.scala:881)\n",
       "\t\tat com.databricks.unity.SAM.setConfFromCred$1(SAM.scala:338)\n",
       "\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$8(SAM.scala:353)\n",
       "\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$8$adapted(SAM.scala:352)\n",
       "\t\tat com.databricks.unity.TempCredRequest$.$anonfun$withFallbackOnOverlappingForParentPath$3(TemporaryCredentials.scala:147)\n",
       "\t\tat scala.util.Try$.apply(Try.scala:213)\n",
       "\t\tat com.databricks.unity.TempCredRequest$.withFallbackOnOverlappingForParentPath(TemporaryCredentials.scala:147)\n",
       "\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$7(SAM.scala:352)\n",
       "\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$7$adapted(SAM.scala:350)\n",
       "\t\tat scala.Option.map(Option.scala:230)\n",
       "\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$6(SAM.scala:350)\n",
       "\t\tat scala.Option.flatMap(Option.scala:271)\n",
       "\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$1(SAM.scala:349)\n",
       "\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$1$adapted(SAM.scala:336)\n",
       "\t\tat scala.Option.foreach(Option.scala:407)\n",
       "\t\tat com.databricks.unity.SAM.setFileSystemConfigs(SAM.scala:336)\n",
       "\t\tat com.databricks.unity.SAM.createDelegate(SAM.scala:388)\n",
       "\t\tat com.databricks.unity.SAM.createDelegate$(SAM.scala:372)\n",
       "\t\tat com.databricks.unity.TableSAM.createDelegate(SAM.scala:695)\n",
       "\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.createDelegate(CredentialScopeFileSystem.scala:74)\n",
       "\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.$anonfun$setDelegates$1(CredentialScopeFileSystem.scala:127)\n",
       "\t\tat com.databricks.sql.acl.fs.Lazy.apply(DelegatingFileSystem.scala:310)\n",
       "\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.readDelegateMakeQualified(CredentialScopeFileSystem.scala:282)\n",
       "\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.optimizedMakeQualifiedPath(CredentialScopeFileSystem.scala:277)\n",
       "\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.makeQualified(CredentialScopeFileSystem.scala:294)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.apply(DeltaLog.scala:1284)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.forTable(DeltaLog.scala:1087)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$5(DeltaTableV2.scala:175)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$withAdditionalSnapshotInitializationUsageLogData$1(DeltaLog.scala:968)\n",
       "\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
       "\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
       "\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.withAdditionalSnapshotInitializationUsageLogData(DeltaLog.scala:969)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$2(DeltaTableV2.scala:169)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:700)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$1(DeltaTableV2.scala:150)\n",
       "\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:83)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.deltaLog$lzycompute(DeltaTableV2.scala:150)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.deltaLog(DeltaTableV2.scala:148)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$8(DeltaTableV2.scala:264)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$withAdditionalSnapshotInitializationUsageLogData$1(DeltaLog.scala:968)\n",
       "\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
       "\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
       "\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.withAdditionalSnapshotInitializationUsageLogData(DeltaLog.scala:969)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$7(DeltaTableV2.scala:263)\n",
       "\t\tat scala.Option.orElse(Option.scala:447)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$5(DeltaTableV2.scala:263)\n",
       "\t\tat scala.Option.orElse(Option.scala:447)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$2(DeltaTableV2.scala:260)\n",
       "\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:83)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$1(DeltaTableV2.scala:290)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:700)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot$lzycompute(DeltaTableV2.scala:240)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot(DeltaTableV2.scala:238)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$tableSchema$2(DeltaTableV2.scala:317)\n",
       "\t\tat scala.Option.getOrElse(Option.scala:189)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.tableSchema$lzycompute(DeltaTableV2.scala:317)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.tableSchema(DeltaTableV2.scala:315)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.schema(DeltaTableV2.scala:322)\n",
       "\t\tat org.apache.spark.sql.connector.catalog.Table.columns(Table.java:68)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.columns(DeltaTableV2.scala:83)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.ResolvedTable$.create(v2ResolutionPlans.scala:231)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$4(Analyzer.scala:1600)\n",
       "\t\tat scala.Option.map(Option.scala:230)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$3(Analyzer.scala:1592)\n",
       "\t\tat scala.Option.orElse(Option.scala:447)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableOrView(Analyzer.scala:1587)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1556)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1466)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:182)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:171)\n",
       "\t\tat scala.PartialFunction.applyOrElse(PartialFunction.scala:127)\n",
       "\t\tat scala.PartialFunction.applyOrElse$(PartialFunction.scala:126)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.applyOrElse(AnalysisHelper.scala:171)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)\n",
       "\t\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:138)\n",
       "\t\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1330)\n",
       "\t\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1329)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.DescribeRelation.mapChildren(v2Commands.scala:726)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning(AnalysisHelper.scala:186)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning$(AnalysisHelper.scala:167)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithSubqueriesAndPruning(LogicalPlan.scala:42)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.resolveOperatorsUpWithSubqueriesAndPruningByConf(Analyzer.scala:1674)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply0(Analyzer.scala:1466)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1419)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1365)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:480)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:629)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:613)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:131)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:480)\n",
       "\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:479)\n",
       "\t\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n",
       "\t\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n",
       "\t\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:475)\n",
       "\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:452)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22(RuleExecutor.scala:585)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22$adapted(RuleExecutor.scala:585)\n",
       "\t\tat scala.collection.immutable.List.foreach(List.scala:431)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:585)\n",
       "\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:349)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:498)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:491)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:397)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:491)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:416)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:341)\n",
       "\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:219)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:341)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:252)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:96)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:131)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:87)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:478)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:478)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:294)\n",
       "\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:548)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:668)\n",
       "\t\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:151)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:668)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1307)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:661)\n",
       "\t\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:658)\n",
       "\t\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:658)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:288)\n",
       "\t\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:287)\n",
       "\t\tat scala.util.Try$.apply(Try.scala:213)\n",
       "\t\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)\n",
       "\t\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n",
       "\t\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n",
       "\t\t... 24 more\n",
       "</td><td>2025-08-13T14:38:00.764356Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>table/view</td><td>`accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample`.`forecast`.`us_postal_hourly_imperial`</td><td>clone/create</td><td>error</td><td>Failed to process table/view `accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample`.`forecast`.`us_postal_hourly_imperial`. Error: An error occurred while calling o427.sql.\n",
       ": com.databricks.sql.managedcatalog.acl.UnauthorizedAccessException: PERMISSION_DENIED: User does not have USE CATALOG on Catalog 'accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample'.\n",
       "\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:87)\n",
       "\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:63)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:251)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapExceptionBase(ManagedCatalogClientImpl.scala:7378)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:7364)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.getTableCredentials(ManagedCatalogClientImpl.scala:3951)\n",
       "\tat com.databricks.sql.managedcatalog.ManagedCatalogClient.getTemporaryCredentials(ManagedCatalogClient.scala:2620)\n",
       "\tat com.databricks.sql.managedcatalog.ManagedCatalogClient.getTemporaryCredentials$(ManagedCatalogClient.scala:2605)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.getTemporaryCredentials(ManagedCatalogClientImpl.scala:251)\n",
       "\tat com.databricks.unity.TempCredCache.$anonfun$getInternal$7(TemporaryCredentials.scala:448)\n",
       "\tat com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4724)\n",
       "\tat com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3522)\n",
       "\tat com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2315)\n",
       "\tat com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2278)\n",
       "\tat com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2193)\n",
       "\tat com.google.common.cache.LocalCache.get(LocalCache.java:3932)\n",
       "\tat com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4721)\n",
       "\tat com.databricks.unity.TempCredCache.liftedTree1$1(TemporaryCredentials.scala:447)\n",
       "\tat com.databricks.unity.TempCredCache.getInternal(TemporaryCredentials.scala:446)\n",
       "\tat com.databricks.unity.TempCredCache.get(TemporaryCredentials.scala:373)\n",
       "\tat com.databricks.unity.UnityCredentialManager.getTemporaryCredentials(CredentialManager.scala:488)\n",
       "\tat com.databricks.unity.CredentialManager$.getTemporaryCredentials(CredentialManager.scala:881)\n",
       "\tat com.databricks.unity.SAM.setConfFromCred$1(SAM.scala:338)\n",
       "\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$8(SAM.scala:353)\n",
       "\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$8$adapted(SAM.scala:352)\n",
       "\tat com.databricks.unity.TempCredRequest$.$anonfun$withFallbackOnOverlappingForParentPath$3(TemporaryCredentials.scala:147)\n",
       "\tat scala.util.Try$.apply(Try.scala:213)\n",
       "\tat com.databricks.unity.TempCredRequest$.withFallbackOnOverlappingForParentPath(TemporaryCredentials.scala:147)\n",
       "\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$7(SAM.scala:352)\n",
       "\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$7$adapted(SAM.scala:350)\n",
       "\tat scala.Option.map(Option.scala:230)\n",
       "\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$6(SAM.scala:350)\n",
       "\tat scala.Option.flatMap(Option.scala:271)\n",
       "\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$1(SAM.scala:349)\n",
       "\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$1$adapted(SAM.scala:336)\n",
       "\tat scala.Option.foreach(Option.scala:407)\n",
       "\tat com.databricks.unity.SAM.setFileSystemConfigs(SAM.scala:336)\n",
       "\tat com.databricks.unity.SAM.createDelegate(SAM.scala:388)\n",
       "\tat com.databricks.unity.SAM.createDelegate$(SAM.scala:372)\n",
       "\tat com.databricks.unity.TableSAM.createDelegate(SAM.scala:695)\n",
       "\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.createDelegate(CredentialScopeFileSystem.scala:74)\n",
       "\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.$anonfun$setDelegates$1(CredentialScopeFileSystem.scala:127)\n",
       "\tat com.databricks.sql.acl.fs.Lazy.apply(DelegatingFileSystem.scala:310)\n",
       "\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.readDelegateMakeQualified(CredentialScopeFileSystem.scala:282)\n",
       "\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.optimizedMakeQualifiedPath(CredentialScopeFileSystem.scala:277)\n",
       "\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.makeQualified(CredentialScopeFileSystem.scala:294)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaLog$.apply(DeltaLog.scala:1284)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaLog$.forTable(DeltaLog.scala:1087)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$5(DeltaTableV2.scala:175)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$withAdditionalSnapshotInitializationUsageLogData$1(DeltaLog.scala:968)\n",
       "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
       "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
       "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaLog$.withAdditionalSnapshotInitializationUsageLogData(DeltaLog.scala:969)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$2(DeltaTableV2.scala:169)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:700)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$1(DeltaTableV2.scala:150)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:83)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.deltaLog$lzycompute(DeltaTableV2.scala:150)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.deltaLog(DeltaTableV2.scala:148)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$8(DeltaTableV2.scala:264)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$withAdditionalSnapshotInitializationUsageLogData$1(DeltaLog.scala:968)\n",
       "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
       "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
       "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaLog$.withAdditionalSnapshotInitializationUsageLogData(DeltaLog.scala:969)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$7(DeltaTableV2.scala:263)\n",
       "\tat scala.Option.orElse(Option.scala:447)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$5(DeltaTableV2.scala:263)\n",
       "\tat scala.Option.orElse(Option.scala:447)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$2(DeltaTableV2.scala:260)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:83)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$1(DeltaTableV2.scala:290)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:700)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot$lzycompute(DeltaTableV2.scala:240)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot(DeltaTableV2.scala:238)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$tableSchema$2(DeltaTableV2.scala:317)\n",
       "\tat scala.Option.getOrElse(Option.scala:189)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.tableSchema$lzycompute(DeltaTableV2.scala:317)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.tableSchema(DeltaTableV2.scala:315)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.schema(DeltaTableV2.scala:322)\n",
       "\tat org.apache.spark.sql.connector.catalog.Table.columns(Table.java:68)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.columns(DeltaTableV2.scala:83)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.ResolvedTable$.create(v2ResolutionPlans.scala:231)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$4(Analyzer.scala:1600)\n",
       "\tat scala.Option.map(Option.scala:230)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$3(Analyzer.scala:1592)\n",
       "\tat scala.Option.orElse(Option.scala:447)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableOrView(Analyzer.scala:1587)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1556)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1466)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:182)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:171)\n",
       "\tat scala.PartialFunction.applyOrElse(PartialFunction.scala:127)\n",
       "\tat scala.PartialFunction.applyOrElse$(PartialFunction.scala:126)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.applyOrElse(AnalysisHelper.scala:171)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)\n",
       "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:138)\n",
       "\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1330)\n",
       "\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1329)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.DescribeRelation.mapChildren(v2Commands.scala:726)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning(AnalysisHelper.scala:186)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning$(AnalysisHelper.scala:167)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithSubqueriesAndPruning(LogicalPlan.scala:42)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.resolveOperatorsUpWithSubqueriesAndPruningByConf(Analyzer.scala:1674)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply0(Analyzer.scala:1466)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1419)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1365)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:480)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:629)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:613)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:131)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:480)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:479)\n",
       "\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n",
       "\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n",
       "\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:475)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:452)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22(RuleExecutor.scala:585)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22$adapted(RuleExecutor.scala:585)\n",
       "\tat scala.collection.immutable.List.foreach(List.scala:431)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:585)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:349)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:498)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:491)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:397)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:491)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:416)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:341)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:219)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:341)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:252)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:96)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:131)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:87)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:478)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:478)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:294)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:548)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:668)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:151)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:668)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1307)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:661)\n",
       "\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:658)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:658)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:288)\n",
       "\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:287)\n",
       "\tat scala.util.Try$.apply(Try.scala:213)\n",
       "\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)\n",
       "\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)\n",
       "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:321)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:267)\n",
       "\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$3(Dataset.scala:149)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n",
       "\tat org.apache.spark.sql.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1469)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1469)\n",
       "\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:141)\n",
       "\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:1048)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n",
       "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:1001)\n",
       "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:1071)\n",
       "\tat jdk.internal.reflect.GeneratedMethodAccessor593.invoke(Unknown Source)\n",
       "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
       "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
       "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n",
       "\tat py4j.Gateway.invoke(Gateway.java:306)\n",
       "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
       "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:197)\n",
       "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:117)\n",
       "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
       "\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\n",
       "\t\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:87)\n",
       "\t\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:63)\n",
       "\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:251)\n",
       "\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapExceptionBase(ManagedCatalogClientImpl.scala:7378)\n",
       "\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:7364)\n",
       "\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.getTableCredentials(ManagedCatalogClientImpl.scala:3951)\n",
       "\t\tat com.databricks.sql.managedcatalog.ManagedCatalogClient.getTemporaryCredentials(ManagedCatalogClient.scala:2620)\n",
       "\t\tat com.databricks.sql.managedcatalog.ManagedCatalogClient.getTemporaryCredentials$(ManagedCatalogClient.scala:2605)\n",
       "\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.getTemporaryCredentials(ManagedCatalogClientImpl.scala:251)\n",
       "\t\tat com.databricks.unity.TempCredCache.$anonfun$getInternal$7(TemporaryCredentials.scala:448)\n",
       "\t\tat com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4724)\n",
       "\t\tat com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3522)\n",
       "\t\tat com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2315)\n",
       "\t\tat com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2278)\n",
       "\t\tat com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2193)\n",
       "\t\tat com.google.common.cache.LocalCache.get(LocalCache.java:3932)\n",
       "\t\tat com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4721)\n",
       "\t\tat com.databricks.unity.TempCredCache.liftedTree1$1(TemporaryCredentials.scala:447)\n",
       "\t\tat com.databricks.unity.TempCredCache.getInternal(TemporaryCredentials.scala:446)\n",
       "\t\tat com.databricks.unity.TempCredCache.get(TemporaryCredentials.scala:373)\n",
       "\t\tat com.databricks.unity.UnityCredentialManager.getTemporaryCredentials(CredentialManager.scala:488)\n",
       "\t\tat com.databricks.unity.CredentialManager$.getTemporaryCredentials(CredentialManager.scala:881)\n",
       "\t\tat com.databricks.unity.SAM.setConfFromCred$1(SAM.scala:338)\n",
       "\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$8(SAM.scala:353)\n",
       "\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$8$adapted(SAM.scala:352)\n",
       "\t\tat com.databricks.unity.TempCredRequest$.$anonfun$withFallbackOnOverlappingForParentPath$3(TemporaryCredentials.scala:147)\n",
       "\t\tat scala.util.Try$.apply(Try.scala:213)\n",
       "\t\tat com.databricks.unity.TempCredRequest$.withFallbackOnOverlappingForParentPath(TemporaryCredentials.scala:147)\n",
       "\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$7(SAM.scala:352)\n",
       "\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$7$adapted(SAM.scala:350)\n",
       "\t\tat scala.Option.map(Option.scala:230)\n",
       "\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$6(SAM.scala:350)\n",
       "\t\tat scala.Option.flatMap(Option.scala:271)\n",
       "\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$1(SAM.scala:349)\n",
       "\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$1$adapted(SAM.scala:336)\n",
       "\t\tat scala.Option.foreach(Option.scala:407)\n",
       "\t\tat com.databricks.unity.SAM.setFileSystemConfigs(SAM.scala:336)\n",
       "\t\tat com.databricks.unity.SAM.createDelegate(SAM.scala:388)\n",
       "\t\tat com.databricks.unity.SAM.createDelegate$(SAM.scala:372)\n",
       "\t\tat com.databricks.unity.TableSAM.createDelegate(SAM.scala:695)\n",
       "\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.createDelegate(CredentialScopeFileSystem.scala:74)\n",
       "\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.$anonfun$setDelegates$1(CredentialScopeFileSystem.scala:127)\n",
       "\t\tat com.databricks.sql.acl.fs.Lazy.apply(DelegatingFileSystem.scala:310)\n",
       "\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.readDelegateMakeQualified(CredentialScopeFileSystem.scala:282)\n",
       "\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.optimizedMakeQualifiedPath(CredentialScopeFileSystem.scala:277)\n",
       "\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.makeQualified(CredentialScopeFileSystem.scala:294)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.apply(DeltaLog.scala:1284)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.forTable(DeltaLog.scala:1087)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$5(DeltaTableV2.scala:175)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$withAdditionalSnapshotInitializationUsageLogData$1(DeltaLog.scala:968)\n",
       "\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
       "\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
       "\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.withAdditionalSnapshotInitializationUsageLogData(DeltaLog.scala:969)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$2(DeltaTableV2.scala:169)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:700)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$1(DeltaTableV2.scala:150)\n",
       "\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:83)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.deltaLog$lzycompute(DeltaTableV2.scala:150)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.deltaLog(DeltaTableV2.scala:148)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$8(DeltaTableV2.scala:264)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$withAdditionalSnapshotInitializationUsageLogData$1(DeltaLog.scala:968)\n",
       "\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
       "\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
       "\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.withAdditionalSnapshotInitializationUsageLogData(DeltaLog.scala:969)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$7(DeltaTableV2.scala:263)\n",
       "\t\tat scala.Option.orElse(Option.scala:447)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$5(DeltaTableV2.scala:263)\n",
       "\t\tat scala.Option.orElse(Option.scala:447)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$2(DeltaTableV2.scala:260)\n",
       "\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:83)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$1(DeltaTableV2.scala:290)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:700)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot$lzycompute(DeltaTableV2.scala:240)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot(DeltaTableV2.scala:238)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$tableSchema$2(DeltaTableV2.scala:317)\n",
       "\t\tat scala.Option.getOrElse(Option.scala:189)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.tableSchema$lzycompute(DeltaTableV2.scala:317)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.tableSchema(DeltaTableV2.scala:315)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.schema(DeltaTableV2.scala:322)\n",
       "\t\tat org.apache.spark.sql.connector.catalog.Table.columns(Table.java:68)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.columns(DeltaTableV2.scala:83)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.ResolvedTable$.create(v2ResolutionPlans.scala:231)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$4(Analyzer.scala:1600)\n",
       "\t\tat scala.Option.map(Option.scala:230)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$3(Analyzer.scala:1592)\n",
       "\t\tat scala.Option.orElse(Option.scala:447)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableOrView(Analyzer.scala:1587)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1556)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1466)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:182)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:171)\n",
       "\t\tat scala.PartialFunction.applyOrElse(PartialFunction.scala:127)\n",
       "\t\tat scala.PartialFunction.applyOrElse$(PartialFunction.scala:126)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.applyOrElse(AnalysisHelper.scala:171)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)\n",
       "\t\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:138)\n",
       "\t\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1330)\n",
       "\t\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1329)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.DescribeRelation.mapChildren(v2Commands.scala:726)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning(AnalysisHelper.scala:186)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning$(AnalysisHelper.scala:167)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithSubqueriesAndPruning(LogicalPlan.scala:42)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.resolveOperatorsUpWithSubqueriesAndPruningByConf(Analyzer.scala:1674)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply0(Analyzer.scala:1466)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1419)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1365)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:480)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:629)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:613)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:131)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:480)\n",
       "\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:479)\n",
       "\t\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n",
       "\t\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n",
       "\t\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:475)\n",
       "\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:452)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22(RuleExecutor.scala:585)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22$adapted(RuleExecutor.scala:585)\n",
       "\t\tat scala.collection.immutable.List.foreach(List.scala:431)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:585)\n",
       "\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:349)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:498)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:491)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:397)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:491)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:416)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:341)\n",
       "\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:219)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:341)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:252)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:96)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:131)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:87)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:478)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:478)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:294)\n",
       "\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:548)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:668)\n",
       "\t\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:151)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:668)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1307)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:661)\n",
       "\t\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:658)\n",
       "\t\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:658)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:288)\n",
       "\t\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:287)\n",
       "\t\tat scala.util.Try$.apply(Try.scala:213)\n",
       "\t\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)\n",
       "\t\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n",
       "\t\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n",
       "\t\t... 24 more\n",
       "</td><td>2025-08-13T14:38:00.744419Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>table/view</td><td>`accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample`.`forecast`.`us_postal_daynight_metric`</td><td>clone/create</td><td>error</td><td>Failed to process table/view `accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample`.`forecast`.`us_postal_daynight_metric`. Error: An error occurred while calling o427.sql.\n",
       ": com.databricks.sql.managedcatalog.acl.UnauthorizedAccessException: PERMISSION_DENIED: User does not have USE CATALOG on Catalog 'accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample'.\n",
       "\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:87)\n",
       "\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:63)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:251)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapExceptionBase(ManagedCatalogClientImpl.scala:7378)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:7364)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.getTableCredentials(ManagedCatalogClientImpl.scala:3951)\n",
       "\tat com.databricks.sql.managedcatalog.ManagedCatalogClient.getTemporaryCredentials(ManagedCatalogClient.scala:2620)\n",
       "\tat com.databricks.sql.managedcatalog.ManagedCatalogClient.getTemporaryCredentials$(ManagedCatalogClient.scala:2605)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.getTemporaryCredentials(ManagedCatalogClientImpl.scala:251)\n",
       "\tat com.databricks.unity.TempCredCache.$anonfun$getInternal$7(TemporaryCredentials.scala:448)\n",
       "\tat com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4724)\n",
       "\tat com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3522)\n",
       "\tat com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2315)\n",
       "\tat com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2278)\n",
       "\tat com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2193)\n",
       "\tat com.google.common.cache.LocalCache.get(LocalCache.java:3932)\n",
       "\tat com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4721)\n",
       "\tat com.databricks.unity.TempCredCache.liftedTree1$1(TemporaryCredentials.scala:447)\n",
       "\tat com.databricks.unity.TempCredCache.getInternal(TemporaryCredentials.scala:446)\n",
       "\tat com.databricks.unity.TempCredCache.get(TemporaryCredentials.scala:373)\n",
       "\tat com.databricks.unity.UnityCredentialManager.getTemporaryCredentials(CredentialManager.scala:488)\n",
       "\tat com.databricks.unity.CredentialManager$.getTemporaryCredentials(CredentialManager.scala:881)\n",
       "\tat com.databricks.unity.SAM.setConfFromCred$1(SAM.scala:338)\n",
       "\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$8(SAM.scala:353)\n",
       "\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$8$adapted(SAM.scala:352)\n",
       "\tat com.databricks.unity.TempCredRequest$.$anonfun$withFallbackOnOverlappingForParentPath$3(TemporaryCredentials.scala:147)\n",
       "\tat scala.util.Try$.apply(Try.scala:213)\n",
       "\tat com.databricks.unity.TempCredRequest$.withFallbackOnOverlappingForParentPath(TemporaryCredentials.scala:147)\n",
       "\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$7(SAM.scala:352)\n",
       "\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$7$adapted(SAM.scala:350)\n",
       "\tat scala.Option.map(Option.scala:230)\n",
       "\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$6(SAM.scala:350)\n",
       "\tat scala.Option.flatMap(Option.scala:271)\n",
       "\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$1(SAM.scala:349)\n",
       "\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$1$adapted(SAM.scala:336)\n",
       "\tat scala.Option.foreach(Option.scala:407)\n",
       "\tat com.databricks.unity.SAM.setFileSystemConfigs(SAM.scala:336)\n",
       "\tat com.databricks.unity.SAM.createDelegate(SAM.scala:388)\n",
       "\tat com.databricks.unity.SAM.createDelegate$(SAM.scala:372)\n",
       "\tat com.databricks.unity.TableSAM.createDelegate(SAM.scala:695)\n",
       "\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.createDelegate(CredentialScopeFileSystem.scala:74)\n",
       "\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.$anonfun$setDelegates$1(CredentialScopeFileSystem.scala:127)\n",
       "\tat com.databricks.sql.acl.fs.Lazy.apply(DelegatingFileSystem.scala:310)\n",
       "\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.readDelegateMakeQualified(CredentialScopeFileSystem.scala:282)\n",
       "\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.optimizedMakeQualifiedPath(CredentialScopeFileSystem.scala:277)\n",
       "\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.makeQualified(CredentialScopeFileSystem.scala:294)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaLog$.apply(DeltaLog.scala:1284)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaLog$.forTable(DeltaLog.scala:1087)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$5(DeltaTableV2.scala:175)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$withAdditionalSnapshotInitializationUsageLogData$1(DeltaLog.scala:968)\n",
       "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
       "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
       "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaLog$.withAdditionalSnapshotInitializationUsageLogData(DeltaLog.scala:969)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$2(DeltaTableV2.scala:169)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:700)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$1(DeltaTableV2.scala:150)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:83)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.deltaLog$lzycompute(DeltaTableV2.scala:150)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.deltaLog(DeltaTableV2.scala:148)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$8(DeltaTableV2.scala:264)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$withAdditionalSnapshotInitializationUsageLogData$1(DeltaLog.scala:968)\n",
       "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
       "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
       "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaLog$.withAdditionalSnapshotInitializationUsageLogData(DeltaLog.scala:969)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$7(DeltaTableV2.scala:263)\n",
       "\tat scala.Option.orElse(Option.scala:447)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$5(DeltaTableV2.scala:263)\n",
       "\tat scala.Option.orElse(Option.scala:447)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$2(DeltaTableV2.scala:260)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:83)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$1(DeltaTableV2.scala:290)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:700)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot$lzycompute(DeltaTableV2.scala:240)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot(DeltaTableV2.scala:238)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$tableSchema$2(DeltaTableV2.scala:317)\n",
       "\tat scala.Option.getOrElse(Option.scala:189)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.tableSchema$lzycompute(DeltaTableV2.scala:317)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.tableSchema(DeltaTableV2.scala:315)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.schema(DeltaTableV2.scala:322)\n",
       "\tat org.apache.spark.sql.connector.catalog.Table.columns(Table.java:68)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.columns(DeltaTableV2.scala:83)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.ResolvedTable$.create(v2ResolutionPlans.scala:231)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$4(Analyzer.scala:1600)\n",
       "\tat scala.Option.map(Option.scala:230)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$3(Analyzer.scala:1592)\n",
       "\tat scala.Option.orElse(Option.scala:447)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableOrView(Analyzer.scala:1587)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1556)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1466)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:182)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:171)\n",
       "\tat scala.PartialFunction.applyOrElse(PartialFunction.scala:127)\n",
       "\tat scala.PartialFunction.applyOrElse$(PartialFunction.scala:126)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.applyOrElse(AnalysisHelper.scala:171)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)\n",
       "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:138)\n",
       "\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1330)\n",
       "\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1329)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.DescribeRelation.mapChildren(v2Commands.scala:726)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning(AnalysisHelper.scala:186)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning$(AnalysisHelper.scala:167)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithSubqueriesAndPruning(LogicalPlan.scala:42)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.resolveOperatorsUpWithSubqueriesAndPruningByConf(Analyzer.scala:1674)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply0(Analyzer.scala:1466)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1419)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1365)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:480)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:629)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:613)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:131)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:480)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:479)\n",
       "\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n",
       "\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n",
       "\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:475)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:452)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22(RuleExecutor.scala:585)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22$adapted(RuleExecutor.scala:585)\n",
       "\tat scala.collection.immutable.List.foreach(List.scala:431)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:585)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:349)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:498)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:491)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:397)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:491)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:416)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:341)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:219)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:341)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:252)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:96)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:131)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:87)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:478)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:478)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:294)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:548)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:668)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:151)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:668)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1307)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:661)\n",
       "\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:658)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:658)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:288)\n",
       "\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:287)\n",
       "\tat scala.util.Try$.apply(Try.scala:213)\n",
       "\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)\n",
       "\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)\n",
       "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:321)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:267)\n",
       "\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$3(Dataset.scala:149)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n",
       "\tat org.apache.spark.sql.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1469)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1469)\n",
       "\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:141)\n",
       "\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:1048)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n",
       "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:1001)\n",
       "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:1071)\n",
       "\tat jdk.internal.reflect.GeneratedMethodAccessor593.invoke(Unknown Source)\n",
       "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
       "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
       "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n",
       "\tat py4j.Gateway.invoke(Gateway.java:306)\n",
       "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
       "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:197)\n",
       "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:117)\n",
       "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
       "\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\n",
       "\t\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:87)\n",
       "\t\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:63)\n",
       "\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:251)\n",
       "\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapExceptionBase(ManagedCatalogClientImpl.scala:7378)\n",
       "\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:7364)\n",
       "\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.getTableCredentials(ManagedCatalogClientImpl.scala:3951)\n",
       "\t\tat com.databricks.sql.managedcatalog.ManagedCatalogClient.getTemporaryCredentials(ManagedCatalogClient.scala:2620)\n",
       "\t\tat com.databricks.sql.managedcatalog.ManagedCatalogClient.getTemporaryCredentials$(ManagedCatalogClient.scala:2605)\n",
       "\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.getTemporaryCredentials(ManagedCatalogClientImpl.scala:251)\n",
       "\t\tat com.databricks.unity.TempCredCache.$anonfun$getInternal$7(TemporaryCredentials.scala:448)\n",
       "\t\tat com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4724)\n",
       "\t\tat com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3522)\n",
       "\t\tat com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2315)\n",
       "\t\tat com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2278)\n",
       "\t\tat com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2193)\n",
       "\t\tat com.google.common.cache.LocalCache.get(LocalCache.java:3932)\n",
       "\t\tat com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4721)\n",
       "\t\tat com.databricks.unity.TempCredCache.liftedTree1$1(TemporaryCredentials.scala:447)\n",
       "\t\tat com.databricks.unity.TempCredCache.getInternal(TemporaryCredentials.scala:446)\n",
       "\t\tat com.databricks.unity.TempCredCache.get(TemporaryCredentials.scala:373)\n",
       "\t\tat com.databricks.unity.UnityCredentialManager.getTemporaryCredentials(CredentialManager.scala:488)\n",
       "\t\tat com.databricks.unity.CredentialManager$.getTemporaryCredentials(CredentialManager.scala:881)\n",
       "\t\tat com.databricks.unity.SAM.setConfFromCred$1(SAM.scala:338)\n",
       "\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$8(SAM.scala:353)\n",
       "\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$8$adapted(SAM.scala:352)\n",
       "\t\tat com.databricks.unity.TempCredRequest$.$anonfun$withFallbackOnOverlappingForParentPath$3(TemporaryCredentials.scala:147)\n",
       "\t\tat scala.util.Try$.apply(Try.scala:213)\n",
       "\t\tat com.databricks.unity.TempCredRequest$.withFallbackOnOverlappingForParentPath(TemporaryCredentials.scala:147)\n",
       "\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$7(SAM.scala:352)\n",
       "\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$7$adapted(SAM.scala:350)\n",
       "\t\tat scala.Option.map(Option.scala:230)\n",
       "\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$6(SAM.scala:350)\n",
       "\t\tat scala.Option.flatMap(Option.scala:271)\n",
       "\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$1(SAM.scala:349)\n",
       "\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$1$adapted(SAM.scala:336)\n",
       "\t\tat scala.Option.foreach(Option.scala:407)\n",
       "\t\tat com.databricks.unity.SAM.setFileSystemConfigs(SAM.scala:336)\n",
       "\t\tat com.databricks.unity.SAM.createDelegate(SAM.scala:388)\n",
       "\t\tat com.databricks.unity.SAM.createDelegate$(SAM.scala:372)\n",
       "\t\tat com.databricks.unity.TableSAM.createDelegate(SAM.scala:695)\n",
       "\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.createDelegate(CredentialScopeFileSystem.scala:74)\n",
       "\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.$anonfun$setDelegates$1(CredentialScopeFileSystem.scala:127)\n",
       "\t\tat com.databricks.sql.acl.fs.Lazy.apply(DelegatingFileSystem.scala:310)\n",
       "\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.readDelegateMakeQualified(CredentialScopeFileSystem.scala:282)\n",
       "\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.optimizedMakeQualifiedPath(CredentialScopeFileSystem.scala:277)\n",
       "\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.makeQualified(CredentialScopeFileSystem.scala:294)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.apply(DeltaLog.scala:1284)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.forTable(DeltaLog.scala:1087)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$5(DeltaTableV2.scala:175)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$withAdditionalSnapshotInitializationUsageLogData$1(DeltaLog.scala:968)\n",
       "\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
       "\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
       "\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.withAdditionalSnapshotInitializationUsageLogData(DeltaLog.scala:969)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$2(DeltaTableV2.scala:169)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:700)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$1(DeltaTableV2.scala:150)\n",
       "\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:83)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.deltaLog$lzycompute(DeltaTableV2.scala:150)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.deltaLog(DeltaTableV2.scala:148)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$8(DeltaTableV2.scala:264)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$withAdditionalSnapshotInitializationUsageLogData$1(DeltaLog.scala:968)\n",
       "\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
       "\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
       "\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.withAdditionalSnapshotInitializationUsageLogData(DeltaLog.scala:969)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$7(DeltaTableV2.scala:263)\n",
       "\t\tat scala.Option.orElse(Option.scala:447)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$5(DeltaTableV2.scala:263)\n",
       "\t\tat scala.Option.orElse(Option.scala:447)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$2(DeltaTableV2.scala:260)\n",
       "\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:83)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$1(DeltaTableV2.scala:290)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:700)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot$lzycompute(DeltaTableV2.scala:240)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot(DeltaTableV2.scala:238)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$tableSchema$2(DeltaTableV2.scala:317)\n",
       "\t\tat scala.Option.getOrElse(Option.scala:189)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.tableSchema$lzycompute(DeltaTableV2.scala:317)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.tableSchema(DeltaTableV2.scala:315)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.schema(DeltaTableV2.scala:322)\n",
       "\t\tat org.apache.spark.sql.connector.catalog.Table.columns(Table.java:68)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.columns(DeltaTableV2.scala:83)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.ResolvedTable$.create(v2ResolutionPlans.scala:231)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$4(Analyzer.scala:1600)\n",
       "\t\tat scala.Option.map(Option.scala:230)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$3(Analyzer.scala:1592)\n",
       "\t\tat scala.Option.orElse(Option.scala:447)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableOrView(Analyzer.scala:1587)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1556)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1466)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:182)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:171)\n",
       "\t\tat scala.PartialFunction.applyOrElse(PartialFunction.scala:127)\n",
       "\t\tat scala.PartialFunction.applyOrElse$(PartialFunction.scala:126)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.applyOrElse(AnalysisHelper.scala:171)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)\n",
       "\t\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:138)\n",
       "\t\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1330)\n",
       "\t\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1329)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.DescribeRelation.mapChildren(v2Commands.scala:726)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning(AnalysisHelper.scala:186)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning$(AnalysisHelper.scala:167)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithSubqueriesAndPruning(LogicalPlan.scala:42)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.resolveOperatorsUpWithSubqueriesAndPruningByConf(Analyzer.scala:1674)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply0(Analyzer.scala:1466)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1419)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1365)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:480)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:629)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:613)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:131)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:480)\n",
       "\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:479)\n",
       "\t\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n",
       "\t\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n",
       "\t\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:475)\n",
       "\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:452)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22(RuleExecutor.scala:585)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22$adapted(RuleExecutor.scala:585)\n",
       "\t\tat scala.collection.immutable.List.foreach(List.scala:431)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:585)\n",
       "\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:349)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:498)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:491)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:397)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:491)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:416)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:341)\n",
       "\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:219)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:341)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:252)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:96)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:131)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:87)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:478)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:478)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:294)\n",
       "\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:548)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:668)\n",
       "\t\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:151)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:668)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1307)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:661)\n",
       "\t\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:658)\n",
       "\t\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:658)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:288)\n",
       "\t\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:287)\n",
       "\t\tat scala.util.Try$.apply(Try.scala:213)\n",
       "\t\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)\n",
       "\t\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n",
       "\t\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n",
       "\t\t... 24 more\n",
       "</td><td>2025-08-13T14:38:00.684353Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>table/view</td><td>`accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample`.`forecast`.`us_postal_hourly_metric`</td><td>clone/create</td><td>error</td><td>Failed to process table/view `accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample`.`forecast`.`us_postal_hourly_metric`. Error: An error occurred while calling o427.sql.\n",
       ": com.databricks.sql.managedcatalog.acl.UnauthorizedAccessException: PERMISSION_DENIED: User does not have USE CATALOG on Catalog 'accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample'.\n",
       "\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:87)\n",
       "\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:63)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:251)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapExceptionBase(ManagedCatalogClientImpl.scala:7378)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:7364)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.getTableCredentials(ManagedCatalogClientImpl.scala:3951)\n",
       "\tat com.databricks.sql.managedcatalog.ManagedCatalogClient.getTemporaryCredentials(ManagedCatalogClient.scala:2620)\n",
       "\tat com.databricks.sql.managedcatalog.ManagedCatalogClient.getTemporaryCredentials$(ManagedCatalogClient.scala:2605)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.getTemporaryCredentials(ManagedCatalogClientImpl.scala:251)\n",
       "\tat com.databricks.unity.TempCredCache.$anonfun$getInternal$7(TemporaryCredentials.scala:448)\n",
       "\tat com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4724)\n",
       "\tat com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3522)\n",
       "\tat com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2315)\n",
       "\tat com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2278)\n",
       "\tat com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2193)\n",
       "\tat com.google.common.cache.LocalCache.get(LocalCache.java:3932)\n",
       "\tat com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4721)\n",
       "\tat com.databricks.unity.TempCredCache.liftedTree1$1(TemporaryCredentials.scala:447)\n",
       "\tat com.databricks.unity.TempCredCache.getInternal(TemporaryCredentials.scala:446)\n",
       "\tat com.databricks.unity.TempCredCache.get(TemporaryCredentials.scala:373)\n",
       "\tat com.databricks.unity.UnityCredentialManager.getTemporaryCredentials(CredentialManager.scala:488)\n",
       "\tat com.databricks.unity.CredentialManager$.getTemporaryCredentials(CredentialManager.scala:881)\n",
       "\tat com.databricks.unity.SAM.setConfFromCred$1(SAM.scala:338)\n",
       "\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$8(SAM.scala:353)\n",
       "\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$8$adapted(SAM.scala:352)\n",
       "\tat com.databricks.unity.TempCredRequest$.$anonfun$withFallbackOnOverlappingForParentPath$3(TemporaryCredentials.scala:147)\n",
       "\tat scala.util.Try$.apply(Try.scala:213)\n",
       "\tat com.databricks.unity.TempCredRequest$.withFallbackOnOverlappingForParentPath(TemporaryCredentials.scala:147)\n",
       "\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$7(SAM.scala:352)\n",
       "\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$7$adapted(SAM.scala:350)\n",
       "\tat scala.Option.map(Option.scala:230)\n",
       "\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$6(SAM.scala:350)\n",
       "\tat scala.Option.flatMap(Option.scala:271)\n",
       "\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$1(SAM.scala:349)\n",
       "\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$1$adapted(SAM.scala:336)\n",
       "\tat scala.Option.foreach(Option.scala:407)\n",
       "\tat com.databricks.unity.SAM.setFileSystemConfigs(SAM.scala:336)\n",
       "\tat com.databricks.unity.SAM.createDelegate(SAM.scala:388)\n",
       "\tat com.databricks.unity.SAM.createDelegate$(SAM.scala:372)\n",
       "\tat com.databricks.unity.TableSAM.createDelegate(SAM.scala:695)\n",
       "\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.createDelegate(CredentialScopeFileSystem.scala:74)\n",
       "\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.$anonfun$setDelegates$1(CredentialScopeFileSystem.scala:127)\n",
       "\tat com.databricks.sql.acl.fs.Lazy.apply(DelegatingFileSystem.scala:310)\n",
       "\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.readDelegateMakeQualified(CredentialScopeFileSystem.scala:282)\n",
       "\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.optimizedMakeQualifiedPath(CredentialScopeFileSystem.scala:277)\n",
       "\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.makeQualified(CredentialScopeFileSystem.scala:294)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaLog$.apply(DeltaLog.scala:1284)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaLog$.forTable(DeltaLog.scala:1087)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$5(DeltaTableV2.scala:175)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$withAdditionalSnapshotInitializationUsageLogData$1(DeltaLog.scala:968)\n",
       "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
       "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
       "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaLog$.withAdditionalSnapshotInitializationUsageLogData(DeltaLog.scala:969)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$2(DeltaTableV2.scala:169)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:700)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$1(DeltaTableV2.scala:150)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:83)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.deltaLog$lzycompute(DeltaTableV2.scala:150)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.deltaLog(DeltaTableV2.scala:148)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$8(DeltaTableV2.scala:264)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$withAdditionalSnapshotInitializationUsageLogData$1(DeltaLog.scala:968)\n",
       "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
       "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
       "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaLog$.withAdditionalSnapshotInitializationUsageLogData(DeltaLog.scala:969)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$7(DeltaTableV2.scala:263)\n",
       "\tat scala.Option.orElse(Option.scala:447)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$5(DeltaTableV2.scala:263)\n",
       "\tat scala.Option.orElse(Option.scala:447)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$2(DeltaTableV2.scala:260)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:83)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$1(DeltaTableV2.scala:290)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:700)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot$lzycompute(DeltaTableV2.scala:240)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot(DeltaTableV2.scala:238)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$tableSchema$2(DeltaTableV2.scala:317)\n",
       "\tat scala.Option.getOrElse(Option.scala:189)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.tableSchema$lzycompute(DeltaTableV2.scala:317)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.tableSchema(DeltaTableV2.scala:315)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.schema(DeltaTableV2.scala:322)\n",
       "\tat org.apache.spark.sql.connector.catalog.Table.columns(Table.java:68)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.columns(DeltaTableV2.scala:83)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.ResolvedTable$.create(v2ResolutionPlans.scala:231)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$4(Analyzer.scala:1600)\n",
       "\tat scala.Option.map(Option.scala:230)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$3(Analyzer.scala:1592)\n",
       "\tat scala.Option.orElse(Option.scala:447)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableOrView(Analyzer.scala:1587)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1556)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1466)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:182)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:171)\n",
       "\tat scala.PartialFunction.applyOrElse(PartialFunction.scala:127)\n",
       "\tat scala.PartialFunction.applyOrElse$(PartialFunction.scala:126)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.applyOrElse(AnalysisHelper.scala:171)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)\n",
       "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:138)\n",
       "\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1330)\n",
       "\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1329)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.DescribeRelation.mapChildren(v2Commands.scala:726)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning(AnalysisHelper.scala:186)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning$(AnalysisHelper.scala:167)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithSubqueriesAndPruning(LogicalPlan.scala:42)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.resolveOperatorsUpWithSubqueriesAndPruningByConf(Analyzer.scala:1674)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply0(Analyzer.scala:1466)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1419)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1365)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:480)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:629)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:613)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:131)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:480)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:479)\n",
       "\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n",
       "\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n",
       "\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:475)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:452)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22(RuleExecutor.scala:585)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22$adapted(RuleExecutor.scala:585)\n",
       "\tat scala.collection.immutable.List.foreach(List.scala:431)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:585)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:349)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:498)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:491)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:397)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:491)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:416)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:341)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:219)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:341)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:252)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:96)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:131)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:87)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:478)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:478)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:294)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:548)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:668)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:151)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:668)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1307)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:661)\n",
       "\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:658)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:658)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:288)\n",
       "\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:287)\n",
       "\tat scala.util.Try$.apply(Try.scala:213)\n",
       "\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)\n",
       "\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)\n",
       "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:321)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:267)\n",
       "\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$3(Dataset.scala:149)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n",
       "\tat org.apache.spark.sql.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1469)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1469)\n",
       "\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:141)\n",
       "\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:1048)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n",
       "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:1001)\n",
       "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:1071)\n",
       "\tat jdk.internal.reflect.GeneratedMethodAccessor593.invoke(Unknown Source)\n",
       "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
       "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
       "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n",
       "\tat py4j.Gateway.invoke(Gateway.java:306)\n",
       "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
       "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:197)\n",
       "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:117)\n",
       "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
       "\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\n",
       "\t\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:87)\n",
       "\t\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:63)\n",
       "\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:251)\n",
       "\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapExceptionBase(ManagedCatalogClientImpl.scala:7378)\n",
       "\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:7364)\n",
       "\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.getTableCredentials(ManagedCatalogClientImpl.scala:3951)\n",
       "\t\tat com.databricks.sql.managedcatalog.ManagedCatalogClient.getTemporaryCredentials(ManagedCatalogClient.scala:2620)\n",
       "\t\tat com.databricks.sql.managedcatalog.ManagedCatalogClient.getTemporaryCredentials$(ManagedCatalogClient.scala:2605)\n",
       "\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.getTemporaryCredentials(ManagedCatalogClientImpl.scala:251)\n",
       "\t\tat com.databricks.unity.TempCredCache.$anonfun$getInternal$7(TemporaryCredentials.scala:448)\n",
       "\t\tat com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4724)\n",
       "\t\tat com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3522)\n",
       "\t\tat com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2315)\n",
       "\t\tat com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2278)\n",
       "\t\tat com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2193)\n",
       "\t\tat com.google.common.cache.LocalCache.get(LocalCache.java:3932)\n",
       "\t\tat com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4721)\n",
       "\t\tat com.databricks.unity.TempCredCache.liftedTree1$1(TemporaryCredentials.scala:447)\n",
       "\t\tat com.databricks.unity.TempCredCache.getInternal(TemporaryCredentials.scala:446)\n",
       "\t\tat com.databricks.unity.TempCredCache.get(TemporaryCredentials.scala:373)\n",
       "\t\tat com.databricks.unity.UnityCredentialManager.getTemporaryCredentials(CredentialManager.scala:488)\n",
       "\t\tat com.databricks.unity.CredentialManager$.getTemporaryCredentials(CredentialManager.scala:881)\n",
       "\t\tat com.databricks.unity.SAM.setConfFromCred$1(SAM.scala:338)\n",
       "\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$8(SAM.scala:353)\n",
       "\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$8$adapted(SAM.scala:352)\n",
       "\t\tat com.databricks.unity.TempCredRequest$.$anonfun$withFallbackOnOverlappingForParentPath$3(TemporaryCredentials.scala:147)\n",
       "\t\tat scala.util.Try$.apply(Try.scala:213)\n",
       "\t\tat com.databricks.unity.TempCredRequest$.withFallbackOnOverlappingForParentPath(TemporaryCredentials.scala:147)\n",
       "\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$7(SAM.scala:352)\n",
       "\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$7$adapted(SAM.scala:350)\n",
       "\t\tat scala.Option.map(Option.scala:230)\n",
       "\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$6(SAM.scala:350)\n",
       "\t\tat scala.Option.flatMap(Option.scala:271)\n",
       "\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$1(SAM.scala:349)\n",
       "\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$1$adapted(SAM.scala:336)\n",
       "\t\tat scala.Option.foreach(Option.scala:407)\n",
       "\t\tat com.databricks.unity.SAM.setFileSystemConfigs(SAM.scala:336)\n",
       "\t\tat com.databricks.unity.SAM.createDelegate(SAM.scala:388)\n",
       "\t\tat com.databricks.unity.SAM.createDelegate$(SAM.scala:372)\n",
       "\t\tat com.databricks.unity.TableSAM.createDelegate(SAM.scala:695)\n",
       "\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.createDelegate(CredentialScopeFileSystem.scala:74)\n",
       "\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.$anonfun$setDelegates$1(CredentialScopeFileSystem.scala:127)\n",
       "\t\tat com.databricks.sql.acl.fs.Lazy.apply(DelegatingFileSystem.scala:310)\n",
       "\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.readDelegateMakeQualified(CredentialScopeFileSystem.scala:282)\n",
       "\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.optimizedMakeQualifiedPath(CredentialScopeFileSystem.scala:277)\n",
       "\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.makeQualified(CredentialScopeFileSystem.scala:294)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.apply(DeltaLog.scala:1284)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.forTable(DeltaLog.scala:1087)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$5(DeltaTableV2.scala:175)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$withAdditionalSnapshotInitializationUsageLogData$1(DeltaLog.scala:968)\n",
       "\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
       "\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
       "\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.withAdditionalSnapshotInitializationUsageLogData(DeltaLog.scala:969)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$2(DeltaTableV2.scala:169)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:700)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$1(DeltaTableV2.scala:150)\n",
       "\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:83)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.deltaLog$lzycompute(DeltaTableV2.scala:150)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.deltaLog(DeltaTableV2.scala:148)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$8(DeltaTableV2.scala:264)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$withAdditionalSnapshotInitializationUsageLogData$1(DeltaLog.scala:968)\n",
       "\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
       "\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
       "\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.withAdditionalSnapshotInitializationUsageLogData(DeltaLog.scala:969)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$7(DeltaTableV2.scala:263)\n",
       "\t\tat scala.Option.orElse(Option.scala:447)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$5(DeltaTableV2.scala:263)\n",
       "\t\tat scala.Option.orElse(Option.scala:447)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$2(DeltaTableV2.scala:260)\n",
       "\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:83)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$1(DeltaTableV2.scala:290)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:700)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot$lzycompute(DeltaTableV2.scala:240)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot(DeltaTableV2.scala:238)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$tableSchema$2(DeltaTableV2.scala:317)\n",
       "\t\tat scala.Option.getOrElse(Option.scala:189)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.tableSchema$lzycompute(DeltaTableV2.scala:317)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.tableSchema(DeltaTableV2.scala:315)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.schema(DeltaTableV2.scala:322)\n",
       "\t\tat org.apache.spark.sql.connector.catalog.Table.columns(Table.java:68)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.columns(DeltaTableV2.scala:83)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.ResolvedTable$.create(v2ResolutionPlans.scala:231)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$4(Analyzer.scala:1600)\n",
       "\t\tat scala.Option.map(Option.scala:230)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$3(Analyzer.scala:1592)\n",
       "\t\tat scala.Option.orElse(Option.scala:447)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableOrView(Analyzer.scala:1587)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1556)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1466)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:182)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:171)\n",
       "\t\tat scala.PartialFunction.applyOrElse(PartialFunction.scala:127)\n",
       "\t\tat scala.PartialFunction.applyOrElse$(PartialFunction.scala:126)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.applyOrElse(AnalysisHelper.scala:171)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)\n",
       "\t\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:138)\n",
       "\t\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1330)\n",
       "\t\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1329)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.DescribeRelation.mapChildren(v2Commands.scala:726)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning(AnalysisHelper.scala:186)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning$(AnalysisHelper.scala:167)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithSubqueriesAndPruning(LogicalPlan.scala:42)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.resolveOperatorsUpWithSubqueriesAndPruningByConf(Analyzer.scala:1674)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply0(Analyzer.scala:1466)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1419)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1365)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:480)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:629)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:613)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:131)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:480)\n",
       "\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:479)\n",
       "\t\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n",
       "\t\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n",
       "\t\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:475)\n",
       "\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:452)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22(RuleExecutor.scala:585)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22$adapted(RuleExecutor.scala:585)\n",
       "\t\tat scala.collection.immutable.List.foreach(List.scala:431)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:585)\n",
       "\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:349)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:498)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:491)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:397)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:491)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:416)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:341)\n",
       "\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:219)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:341)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:252)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:96)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:131)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:87)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:478)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:478)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:294)\n",
       "\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:548)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:668)\n",
       "\t\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:151)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:668)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1307)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:661)\n",
       "\t\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:658)\n",
       "\t\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:658)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:288)\n",
       "\t\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:287)\n",
       "\t\tat scala.util.Try$.apply(Try.scala:213)\n",
       "\t\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)\n",
       "\t\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n",
       "\t\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n",
       "\t\t... 24 more\n",
       "</td><td>2025-08-13T14:38:00.548404Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>table</td><td>`dr_eo_amperity`.`silver`.`unified_transactions`</td><td>clone</td><td>error</td><td>Failed to clone table `dr_eo_amperity`.`silver`.`unified_transactions`.. Error: [DELTA_CLONE_UNSUPPORTED_SOURCE] Unsupported DEEP clone source '_eo_amperity.silver.unified_transactions', whose format is deltasharing.\n",
       "The supported formats are 'delta', 'iceberg' and 'parquet'.</td><td>2025-08-13T14:37:56.910888Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>table</td><td>`dr_eo_amperity`.`silver`.`unified_scores`</td><td>clone</td><td>error</td><td>Failed to clone table `dr_eo_amperity`.`silver`.`unified_scores`.. Error: [DELTA_CLONE_UNSUPPORTED_SOURCE] Unsupported DEEP clone source '_eo_amperity.silver.unified_scores', whose format is deltasharing.\n",
       "The supported formats are 'delta', 'iceberg' and 'parquet'.</td><td>2025-08-13T14:37:56.736993Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>table/view</td><td>`accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample`.`forecast`.`us_postal_daily_metric`</td><td>clone/create</td><td>error</td><td>Failed to process table/view `accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample`.`forecast`.`us_postal_daily_metric`. Error: An error occurred while calling o427.sql.\n",
       ": com.databricks.sql.managedcatalog.acl.UnauthorizedAccessException: PERMISSION_DENIED: User does not have USE CATALOG on Catalog 'accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample'.\n",
       "\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:87)\n",
       "\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:63)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:251)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapExceptionBase(ManagedCatalogClientImpl.scala:7378)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:7364)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.getTableCredentials(ManagedCatalogClientImpl.scala:3951)\n",
       "\tat com.databricks.sql.managedcatalog.ManagedCatalogClient.getTemporaryCredentials(ManagedCatalogClient.scala:2620)\n",
       "\tat com.databricks.sql.managedcatalog.ManagedCatalogClient.getTemporaryCredentials$(ManagedCatalogClient.scala:2605)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.getTemporaryCredentials(ManagedCatalogClientImpl.scala:251)\n",
       "\tat com.databricks.unity.TempCredCache.$anonfun$getInternal$7(TemporaryCredentials.scala:448)\n",
       "\tat com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4724)\n",
       "\tat com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3522)\n",
       "\tat com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2315)\n",
       "\tat com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2278)\n",
       "\tat com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2193)\n",
       "\tat com.google.common.cache.LocalCache.get(LocalCache.java:3932)\n",
       "\tat com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4721)\n",
       "\tat com.databricks.unity.TempCredCache.liftedTree1$1(TemporaryCredentials.scala:447)\n",
       "\tat com.databricks.unity.TempCredCache.getInternal(TemporaryCredentials.scala:446)\n",
       "\tat com.databricks.unity.TempCredCache.get(TemporaryCredentials.scala:373)\n",
       "\tat com.databricks.unity.UnityCredentialManager.getTemporaryCredentials(CredentialManager.scala:488)\n",
       "\tat com.databricks.unity.CredentialManager$.getTemporaryCredentials(CredentialManager.scala:881)\n",
       "\tat com.databricks.unity.SAM.setConfFromCred$1(SAM.scala:338)\n",
       "\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$8(SAM.scala:353)\n",
       "\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$8$adapted(SAM.scala:352)\n",
       "\tat com.databricks.unity.TempCredRequest$.$anonfun$withFallbackOnOverlappingForParentPath$3(TemporaryCredentials.scala:147)\n",
       "\tat scala.util.Try$.apply(Try.scala:213)\n",
       "\tat com.databricks.unity.TempCredRequest$.withFallbackOnOverlappingForParentPath(TemporaryCredentials.scala:147)\n",
       "\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$7(SAM.scala:352)\n",
       "\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$7$adapted(SAM.scala:350)\n",
       "\tat scala.Option.map(Option.scala:230)\n",
       "\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$6(SAM.scala:350)\n",
       "\tat scala.Option.flatMap(Option.scala:271)\n",
       "\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$1(SAM.scala:349)\n",
       "\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$1$adapted(SAM.scala:336)\n",
       "\tat scala.Option.foreach(Option.scala:407)\n",
       "\tat com.databricks.unity.SAM.setFileSystemConfigs(SAM.scala:336)\n",
       "\tat com.databricks.unity.SAM.createDelegate(SAM.scala:388)\n",
       "\tat com.databricks.unity.SAM.createDelegate$(SAM.scala:372)\n",
       "\tat com.databricks.unity.TableSAM.createDelegate(SAM.scala:695)\n",
       "\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.createDelegate(CredentialScopeFileSystem.scala:74)\n",
       "\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.$anonfun$setDelegates$1(CredentialScopeFileSystem.scala:127)\n",
       "\tat com.databricks.sql.acl.fs.Lazy.apply(DelegatingFileSystem.scala:310)\n",
       "\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.readDelegateMakeQualified(CredentialScopeFileSystem.scala:282)\n",
       "\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.optimizedMakeQualifiedPath(CredentialScopeFileSystem.scala:277)\n",
       "\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.makeQualified(CredentialScopeFileSystem.scala:294)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaLog$.apply(DeltaLog.scala:1284)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaLog$.forTable(DeltaLog.scala:1087)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$5(DeltaTableV2.scala:175)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$withAdditionalSnapshotInitializationUsageLogData$1(DeltaLog.scala:968)\n",
       "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
       "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
       "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaLog$.withAdditionalSnapshotInitializationUsageLogData(DeltaLog.scala:969)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$2(DeltaTableV2.scala:169)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:700)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$1(DeltaTableV2.scala:150)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:83)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.deltaLog$lzycompute(DeltaTableV2.scala:150)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.deltaLog(DeltaTableV2.scala:148)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$8(DeltaTableV2.scala:264)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$withAdditionalSnapshotInitializationUsageLogData$1(DeltaLog.scala:968)\n",
       "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
       "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
       "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaLog$.withAdditionalSnapshotInitializationUsageLogData(DeltaLog.scala:969)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$7(DeltaTableV2.scala:263)\n",
       "\tat scala.Option.orElse(Option.scala:447)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$5(DeltaTableV2.scala:263)\n",
       "\tat scala.Option.orElse(Option.scala:447)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$2(DeltaTableV2.scala:260)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:83)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$1(DeltaTableV2.scala:290)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:700)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot$lzycompute(DeltaTableV2.scala:240)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot(DeltaTableV2.scala:238)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$tableSchema$2(DeltaTableV2.scala:317)\n",
       "\tat scala.Option.getOrElse(Option.scala:189)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.tableSchema$lzycompute(DeltaTableV2.scala:317)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.tableSchema(DeltaTableV2.scala:315)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.schema(DeltaTableV2.scala:322)\n",
       "\tat org.apache.spark.sql.connector.catalog.Table.columns(Table.java:68)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.columns(DeltaTableV2.scala:83)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.ResolvedTable$.create(v2ResolutionPlans.scala:231)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$4(Analyzer.scala:1600)\n",
       "\tat scala.Option.map(Option.scala:230)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$3(Analyzer.scala:1592)\n",
       "\tat scala.Option.orElse(Option.scala:447)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableOrView(Analyzer.scala:1587)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1556)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1466)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:182)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:171)\n",
       "\tat scala.PartialFunction.applyOrElse(PartialFunction.scala:127)\n",
       "\tat scala.PartialFunction.applyOrElse$(PartialFunction.scala:126)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.applyOrElse(AnalysisHelper.scala:171)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)\n",
       "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:138)\n",
       "\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1330)\n",
       "\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1329)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.DescribeRelation.mapChildren(v2Commands.scala:726)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning(AnalysisHelper.scala:186)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning$(AnalysisHelper.scala:167)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithSubqueriesAndPruning(LogicalPlan.scala:42)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.resolveOperatorsUpWithSubqueriesAndPruningByConf(Analyzer.scala:1674)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply0(Analyzer.scala:1466)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1419)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1365)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:480)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:629)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:613)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:131)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:480)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:479)\n",
       "\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n",
       "\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n",
       "\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:475)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:452)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22(RuleExecutor.scala:585)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22$adapted(RuleExecutor.scala:585)\n",
       "\tat scala.collection.immutable.List.foreach(List.scala:431)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:585)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:349)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:498)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:491)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:397)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:491)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:416)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:341)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:219)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:341)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:252)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:96)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:131)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:87)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:478)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:478)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:294)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:548)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:668)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:151)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:668)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1307)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:661)\n",
       "\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:658)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:658)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:288)\n",
       "\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:287)\n",
       "\tat scala.util.Try$.apply(Try.scala:213)\n",
       "\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)\n",
       "\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)\n",
       "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:321)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:267)\n",
       "\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$3(Dataset.scala:149)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n",
       "\tat org.apache.spark.sql.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1469)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1469)\n",
       "\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:141)\n",
       "\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:1048)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n",
       "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:1001)\n",
       "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:1071)\n",
       "\tat jdk.internal.reflect.GeneratedMethodAccessor593.invoke(Unknown Source)\n",
       "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
       "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
       "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n",
       "\tat py4j.Gateway.invoke(Gateway.java:306)\n",
       "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
       "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:197)\n",
       "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:117)\n",
       "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
       "\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\n",
       "\t\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:87)\n",
       "\t\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:63)\n",
       "\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:251)\n",
       "\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapExceptionBase(ManagedCatalogClientImpl.scala:7378)\n",
       "\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:7364)\n",
       "\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.getTableCredentials(ManagedCatalogClientImpl.scala:3951)\n",
       "\t\tat com.databricks.sql.managedcatalog.ManagedCatalogClient.getTemporaryCredentials(ManagedCatalogClient.scala:2620)\n",
       "\t\tat com.databricks.sql.managedcatalog.ManagedCatalogClient.getTemporaryCredentials$(ManagedCatalogClient.scala:2605)\n",
       "\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.getTemporaryCredentials(ManagedCatalogClientImpl.scala:251)\n",
       "\t\tat com.databricks.unity.TempCredCache.$anonfun$getInternal$7(TemporaryCredentials.scala:448)\n",
       "\t\tat com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4724)\n",
       "\t\tat com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3522)\n",
       "\t\tat com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2315)\n",
       "\t\tat com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2278)\n",
       "\t\tat com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2193)\n",
       "\t\tat com.google.common.cache.LocalCache.get(LocalCache.java:3932)\n",
       "\t\tat com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4721)\n",
       "\t\tat com.databricks.unity.TempCredCache.liftedTree1$1(TemporaryCredentials.scala:447)\n",
       "\t\tat com.databricks.unity.TempCredCache.getInternal(TemporaryCredentials.scala:446)\n",
       "\t\tat com.databricks.unity.TempCredCache.get(TemporaryCredentials.scala:373)\n",
       "\t\tat com.databricks.unity.UnityCredentialManager.getTemporaryCredentials(CredentialManager.scala:488)\n",
       "\t\tat com.databricks.unity.CredentialManager$.getTemporaryCredentials(CredentialManager.scala:881)\n",
       "\t\tat com.databricks.unity.SAM.setConfFromCred$1(SAM.scala:338)\n",
       "\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$8(SAM.scala:353)\n",
       "\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$8$adapted(SAM.scala:352)\n",
       "\t\tat com.databricks.unity.TempCredRequest$.$anonfun$withFallbackOnOverlappingForParentPath$3(TemporaryCredentials.scala:147)\n",
       "\t\tat scala.util.Try$.apply(Try.scala:213)\n",
       "\t\tat com.databricks.unity.TempCredRequest$.withFallbackOnOverlappingForParentPath(TemporaryCredentials.scala:147)\n",
       "\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$7(SAM.scala:352)\n",
       "\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$7$adapted(SAM.scala:350)\n",
       "\t\tat scala.Option.map(Option.scala:230)\n",
       "\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$6(SAM.scala:350)\n",
       "\t\tat scala.Option.flatMap(Option.scala:271)\n",
       "\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$1(SAM.scala:349)\n",
       "\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$1$adapted(SAM.scala:336)\n",
       "\t\tat scala.Option.foreach(Option.scala:407)\n",
       "\t\tat com.databricks.unity.SAM.setFileSystemConfigs(SAM.scala:336)\n",
       "\t\tat com.databricks.unity.SAM.createDelegate(SAM.scala:388)\n",
       "\t\tat com.databricks.unity.SAM.createDelegate$(SAM.scala:372)\n",
       "\t\tat com.databricks.unity.TableSAM.createDelegate(SAM.scala:695)\n",
       "\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.createDelegate(CredentialScopeFileSystem.scala:74)\n",
       "\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.$anonfun$setDelegates$1(CredentialScopeFileSystem.scala:127)\n",
       "\t\tat com.databricks.sql.acl.fs.Lazy.apply(DelegatingFileSystem.scala:310)\n",
       "\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.readDelegateMakeQualified(CredentialScopeFileSystem.scala:282)\n",
       "\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.optimizedMakeQualifiedPath(CredentialScopeFileSystem.scala:277)\n",
       "\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.makeQualified(CredentialScopeFileSystem.scala:294)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.apply(DeltaLog.scala:1284)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.forTable(DeltaLog.scala:1087)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$5(DeltaTableV2.scala:175)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$withAdditionalSnapshotInitializationUsageLogData$1(DeltaLog.scala:968)\n",
       "\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
       "\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
       "\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.withAdditionalSnapshotInitializationUsageLogData(DeltaLog.scala:969)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$2(DeltaTableV2.scala:169)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:700)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$1(DeltaTableV2.scala:150)\n",
       "\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:83)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.deltaLog$lzycompute(DeltaTableV2.scala:150)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.deltaLog(DeltaTableV2.scala:148)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$8(DeltaTableV2.scala:264)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$withAdditionalSnapshotInitializationUsageLogData$1(DeltaLog.scala:968)\n",
       "\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
       "\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
       "\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.withAdditionalSnapshotInitializationUsageLogData(DeltaLog.scala:969)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$7(DeltaTableV2.scala:263)\n",
       "\t\tat scala.Option.orElse(Option.scala:447)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$5(DeltaTableV2.scala:263)\n",
       "\t\tat scala.Option.orElse(Option.scala:447)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$2(DeltaTableV2.scala:260)\n",
       "\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:83)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$1(DeltaTableV2.scala:290)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:700)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot$lzycompute(DeltaTableV2.scala:240)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot(DeltaTableV2.scala:238)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$tableSchema$2(DeltaTableV2.scala:317)\n",
       "\t\tat scala.Option.getOrElse(Option.scala:189)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.tableSchema$lzycompute(DeltaTableV2.scala:317)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.tableSchema(DeltaTableV2.scala:315)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.schema(DeltaTableV2.scala:322)\n",
       "\t\tat org.apache.spark.sql.connector.catalog.Table.columns(Table.java:68)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.columns(DeltaTableV2.scala:83)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.ResolvedTable$.create(v2ResolutionPlans.scala:231)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$4(Analyzer.scala:1600)\n",
       "\t\tat scala.Option.map(Option.scala:230)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$3(Analyzer.scala:1592)\n",
       "\t\tat scala.Option.orElse(Option.scala:447)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableOrView(Analyzer.scala:1587)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1556)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1466)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:182)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:171)\n",
       "\t\tat scala.PartialFunction.applyOrElse(PartialFunction.scala:127)\n",
       "\t\tat scala.PartialFunction.applyOrElse$(PartialFunction.scala:126)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.applyOrElse(AnalysisHelper.scala:171)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)\n",
       "\t\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:138)\n",
       "\t\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1330)\n",
       "\t\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1329)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.DescribeRelation.mapChildren(v2Commands.scala:726)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning(AnalysisHelper.scala:186)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning$(AnalysisHelper.scala:167)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithSubqueriesAndPruning(LogicalPlan.scala:42)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.resolveOperatorsUpWithSubqueriesAndPruningByConf(Analyzer.scala:1674)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply0(Analyzer.scala:1466)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1419)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1365)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:480)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:629)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:613)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:131)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:480)\n",
       "\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:479)\n",
       "\t\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n",
       "\t\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n",
       "\t\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:475)\n",
       "\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:452)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22(RuleExecutor.scala:585)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22$adapted(RuleExecutor.scala:585)\n",
       "\t\tat scala.collection.immutable.List.foreach(List.scala:431)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:585)\n",
       "\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:349)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:498)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:491)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:397)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:491)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:416)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:341)\n",
       "\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:219)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:341)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:252)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:96)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:131)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:87)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:478)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:478)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:294)\n",
       "\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:548)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:668)\n",
       "\t\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:151)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:668)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1307)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:661)\n",
       "\t\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:658)\n",
       "\t\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:658)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:288)\n",
       "\t\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:287)\n",
       "\t\tat scala.util.Try$.apply(Try.scala:213)\n",
       "\t\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)\n",
       "\t\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n",
       "\t\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n",
       "\t\t... 24 more\n",
       "</td><td>2025-08-13T14:37:56.420323Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>table/view</td><td>`accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample`.`forecast`.`us_postal_daily_imperial`</td><td>clone/create</td><td>error</td><td>Failed to process table/view `accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample`.`forecast`.`us_postal_daily_imperial`. Error: An error occurred while calling o427.sql.\n",
       ": com.databricks.sql.managedcatalog.acl.UnauthorizedAccessException: PERMISSION_DENIED: User does not have USE CATALOG on Catalog 'accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample'.\n",
       "\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:87)\n",
       "\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:63)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:251)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapExceptionBase(ManagedCatalogClientImpl.scala:7378)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:7364)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.getTableCredentials(ManagedCatalogClientImpl.scala:3951)\n",
       "\tat com.databricks.sql.managedcatalog.ManagedCatalogClient.getTemporaryCredentials(ManagedCatalogClient.scala:2620)\n",
       "\tat com.databricks.sql.managedcatalog.ManagedCatalogClient.getTemporaryCredentials$(ManagedCatalogClient.scala:2605)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.getTemporaryCredentials(ManagedCatalogClientImpl.scala:251)\n",
       "\tat com.databricks.unity.TempCredCache.$anonfun$getInternal$7(TemporaryCredentials.scala:448)\n",
       "\tat com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4724)\n",
       "\tat com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3522)\n",
       "\tat com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2315)\n",
       "\tat com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2278)\n",
       "\tat com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2193)\n",
       "\tat com.google.common.cache.LocalCache.get(LocalCache.java:3932)\n",
       "\tat com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4721)\n",
       "\tat com.databricks.unity.TempCredCache.liftedTree1$1(TemporaryCredentials.scala:447)\n",
       "\tat com.databricks.unity.TempCredCache.getInternal(TemporaryCredentials.scala:446)\n",
       "\tat com.databricks.unity.TempCredCache.get(TemporaryCredentials.scala:373)\n",
       "\tat com.databricks.unity.UnityCredentialManager.getTemporaryCredentials(CredentialManager.scala:488)\n",
       "\tat com.databricks.unity.CredentialManager$.getTemporaryCredentials(CredentialManager.scala:881)\n",
       "\tat com.databricks.unity.SAM.setConfFromCred$1(SAM.scala:338)\n",
       "\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$8(SAM.scala:353)\n",
       "\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$8$adapted(SAM.scala:352)\n",
       "\tat com.databricks.unity.TempCredRequest$.$anonfun$withFallbackOnOverlappingForParentPath$3(TemporaryCredentials.scala:147)\n",
       "\tat scala.util.Try$.apply(Try.scala:213)\n",
       "\tat com.databricks.unity.TempCredRequest$.withFallbackOnOverlappingForParentPath(TemporaryCredentials.scala:147)\n",
       "\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$7(SAM.scala:352)\n",
       "\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$7$adapted(SAM.scala:350)\n",
       "\tat scala.Option.map(Option.scala:230)\n",
       "\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$6(SAM.scala:350)\n",
       "\tat scala.Option.flatMap(Option.scala:271)\n",
       "\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$1(SAM.scala:349)\n",
       "\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$1$adapted(SAM.scala:336)\n",
       "\tat scala.Option.foreach(Option.scala:407)\n",
       "\tat com.databricks.unity.SAM.setFileSystemConfigs(SAM.scala:336)\n",
       "\tat com.databricks.unity.SAM.createDelegate(SAM.scala:388)\n",
       "\tat com.databricks.unity.SAM.createDelegate$(SAM.scala:372)\n",
       "\tat com.databricks.unity.TableSAM.createDelegate(SAM.scala:695)\n",
       "\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.createDelegate(CredentialScopeFileSystem.scala:74)\n",
       "\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.$anonfun$setDelegates$1(CredentialScopeFileSystem.scala:127)\n",
       "\tat com.databricks.sql.acl.fs.Lazy.apply(DelegatingFileSystem.scala:310)\n",
       "\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.readDelegateMakeQualified(CredentialScopeFileSystem.scala:282)\n",
       "\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.optimizedMakeQualifiedPath(CredentialScopeFileSystem.scala:277)\n",
       "\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.makeQualified(CredentialScopeFileSystem.scala:294)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaLog$.apply(DeltaLog.scala:1284)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaLog$.forTable(DeltaLog.scala:1087)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$5(DeltaTableV2.scala:175)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$withAdditionalSnapshotInitializationUsageLogData$1(DeltaLog.scala:968)\n",
       "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
       "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
       "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaLog$.withAdditionalSnapshotInitializationUsageLogData(DeltaLog.scala:969)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$2(DeltaTableV2.scala:169)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:700)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$1(DeltaTableV2.scala:150)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:83)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.deltaLog$lzycompute(DeltaTableV2.scala:150)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.deltaLog(DeltaTableV2.scala:148)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$8(DeltaTableV2.scala:264)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$withAdditionalSnapshotInitializationUsageLogData$1(DeltaLog.scala:968)\n",
       "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
       "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
       "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaLog$.withAdditionalSnapshotInitializationUsageLogData(DeltaLog.scala:969)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$7(DeltaTableV2.scala:263)\n",
       "\tat scala.Option.orElse(Option.scala:447)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$5(DeltaTableV2.scala:263)\n",
       "\tat scala.Option.orElse(Option.scala:447)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$2(DeltaTableV2.scala:260)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:83)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$1(DeltaTableV2.scala:290)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:700)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot$lzycompute(DeltaTableV2.scala:240)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot(DeltaTableV2.scala:238)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$tableSchema$2(DeltaTableV2.scala:317)\n",
       "\tat scala.Option.getOrElse(Option.scala:189)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.tableSchema$lzycompute(DeltaTableV2.scala:317)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.tableSchema(DeltaTableV2.scala:315)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.schema(DeltaTableV2.scala:322)\n",
       "\tat org.apache.spark.sql.connector.catalog.Table.columns(Table.java:68)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.columns(DeltaTableV2.scala:83)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.ResolvedTable$.create(v2ResolutionPlans.scala:231)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$4(Analyzer.scala:1600)\n",
       "\tat scala.Option.map(Option.scala:230)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$3(Analyzer.scala:1592)\n",
       "\tat scala.Option.orElse(Option.scala:447)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableOrView(Analyzer.scala:1587)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1556)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1466)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:182)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:171)\n",
       "\tat scala.PartialFunction.applyOrElse(PartialFunction.scala:127)\n",
       "\tat scala.PartialFunction.applyOrElse$(PartialFunction.scala:126)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.applyOrElse(AnalysisHelper.scala:171)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)\n",
       "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:138)\n",
       "\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1330)\n",
       "\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1329)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.DescribeRelation.mapChildren(v2Commands.scala:726)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning(AnalysisHelper.scala:186)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning$(AnalysisHelper.scala:167)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithSubqueriesAndPruning(LogicalPlan.scala:42)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.resolveOperatorsUpWithSubqueriesAndPruningByConf(Analyzer.scala:1674)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply0(Analyzer.scala:1466)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1419)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1365)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:480)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:629)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:613)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:131)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:480)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:479)\n",
       "\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n",
       "\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n",
       "\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:475)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:452)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22(RuleExecutor.scala:585)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22$adapted(RuleExecutor.scala:585)\n",
       "\tat scala.collection.immutable.List.foreach(List.scala:431)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:585)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:349)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:498)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:491)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:397)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:491)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:416)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:341)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:219)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:341)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:252)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:96)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:131)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:87)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:478)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:478)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:294)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:548)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:668)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:151)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:668)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1307)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:661)\n",
       "\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:658)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:658)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:288)\n",
       "\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:287)\n",
       "\tat scala.util.Try$.apply(Try.scala:213)\n",
       "\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)\n",
       "\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)\n",
       "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:321)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:267)\n",
       "\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$3(Dataset.scala:149)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n",
       "\tat org.apache.spark.sql.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1469)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1469)\n",
       "\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:141)\n",
       "\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:1048)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n",
       "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:1001)\n",
       "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:1071)\n",
       "\tat jdk.internal.reflect.GeneratedMethodAccessor593.invoke(Unknown Source)\n",
       "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
       "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
       "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n",
       "\tat py4j.Gateway.invoke(Gateway.java:306)\n",
       "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
       "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:197)\n",
       "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:117)\n",
       "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
       "\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\n",
       "\t\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:87)\n",
       "\t\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:63)\n",
       "\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:251)\n",
       "\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapExceptionBase(ManagedCatalogClientImpl.scala:7378)\n",
       "\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:7364)\n",
       "\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.getTableCredentials(ManagedCatalogClientImpl.scala:3951)\n",
       "\t\tat com.databricks.sql.managedcatalog.ManagedCatalogClient.getTemporaryCredentials(ManagedCatalogClient.scala:2620)\n",
       "\t\tat com.databricks.sql.managedcatalog.ManagedCatalogClient.getTemporaryCredentials$(ManagedCatalogClient.scala:2605)\n",
       "\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.getTemporaryCredentials(ManagedCatalogClientImpl.scala:251)\n",
       "\t\tat com.databricks.unity.TempCredCache.$anonfun$getInternal$7(TemporaryCredentials.scala:448)\n",
       "\t\tat com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4724)\n",
       "\t\tat com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3522)\n",
       "\t\tat com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2315)\n",
       "\t\tat com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2278)\n",
       "\t\tat com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2193)\n",
       "\t\tat com.google.common.cache.LocalCache.get(LocalCache.java:3932)\n",
       "\t\tat com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4721)\n",
       "\t\tat com.databricks.unity.TempCredCache.liftedTree1$1(TemporaryCredentials.scala:447)\n",
       "\t\tat com.databricks.unity.TempCredCache.getInternal(TemporaryCredentials.scala:446)\n",
       "\t\tat com.databricks.unity.TempCredCache.get(TemporaryCredentials.scala:373)\n",
       "\t\tat com.databricks.unity.UnityCredentialManager.getTemporaryCredentials(CredentialManager.scala:488)\n",
       "\t\tat com.databricks.unity.CredentialManager$.getTemporaryCredentials(CredentialManager.scala:881)\n",
       "\t\tat com.databricks.unity.SAM.setConfFromCred$1(SAM.scala:338)\n",
       "\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$8(SAM.scala:353)\n",
       "\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$8$adapted(SAM.scala:352)\n",
       "\t\tat com.databricks.unity.TempCredRequest$.$anonfun$withFallbackOnOverlappingForParentPath$3(TemporaryCredentials.scala:147)\n",
       "\t\tat scala.util.Try$.apply(Try.scala:213)\n",
       "\t\tat com.databricks.unity.TempCredRequest$.withFallbackOnOverlappingForParentPath(TemporaryCredentials.scala:147)\n",
       "\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$7(SAM.scala:352)\n",
       "\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$7$adapted(SAM.scala:350)\n",
       "\t\tat scala.Option.map(Option.scala:230)\n",
       "\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$6(SAM.scala:350)\n",
       "\t\tat scala.Option.flatMap(Option.scala:271)\n",
       "\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$1(SAM.scala:349)\n",
       "\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$1$adapted(SAM.scala:336)\n",
       "\t\tat scala.Option.foreach(Option.scala:407)\n",
       "\t\tat com.databricks.unity.SAM.setFileSystemConfigs(SAM.scala:336)\n",
       "\t\tat com.databricks.unity.SAM.createDelegate(SAM.scala:388)\n",
       "\t\tat com.databricks.unity.SAM.createDelegate$(SAM.scala:372)\n",
       "\t\tat com.databricks.unity.TableSAM.createDelegate(SAM.scala:695)\n",
       "\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.createDelegate(CredentialScopeFileSystem.scala:74)\n",
       "\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.$anonfun$setDelegates$1(CredentialScopeFileSystem.scala:127)\n",
       "\t\tat com.databricks.sql.acl.fs.Lazy.apply(DelegatingFileSystem.scala:310)\n",
       "\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.readDelegateMakeQualified(CredentialScopeFileSystem.scala:282)\n",
       "\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.optimizedMakeQualifiedPath(CredentialScopeFileSystem.scala:277)\n",
       "\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.makeQualified(CredentialScopeFileSystem.scala:294)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.apply(DeltaLog.scala:1284)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.forTable(DeltaLog.scala:1087)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$5(DeltaTableV2.scala:175)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$withAdditionalSnapshotInitializationUsageLogData$1(DeltaLog.scala:968)\n",
       "\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
       "\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
       "\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.withAdditionalSnapshotInitializationUsageLogData(DeltaLog.scala:969)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$2(DeltaTableV2.scala:169)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:700)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$1(DeltaTableV2.scala:150)\n",
       "\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:83)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.deltaLog$lzycompute(DeltaTableV2.scala:150)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.deltaLog(DeltaTableV2.scala:148)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$8(DeltaTableV2.scala:264)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$withAdditionalSnapshotInitializationUsageLogData$1(DeltaLog.scala:968)\n",
       "\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
       "\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
       "\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.withAdditionalSnapshotInitializationUsageLogData(DeltaLog.scala:969)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$7(DeltaTableV2.scala:263)\n",
       "\t\tat scala.Option.orElse(Option.scala:447)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$5(DeltaTableV2.scala:263)\n",
       "\t\tat scala.Option.orElse(Option.scala:447)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$2(DeltaTableV2.scala:260)\n",
       "\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:83)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$1(DeltaTableV2.scala:290)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:700)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot$lzycompute(DeltaTableV2.scala:240)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot(DeltaTableV2.scala:238)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$tableSchema$2(DeltaTableV2.scala:317)\n",
       "\t\tat scala.Option.getOrElse(Option.scala:189)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.tableSchema$lzycompute(DeltaTableV2.scala:317)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.tableSchema(DeltaTableV2.scala:315)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.schema(DeltaTableV2.scala:322)\n",
       "\t\tat org.apache.spark.sql.connector.catalog.Table.columns(Table.java:68)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.columns(DeltaTableV2.scala:83)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.ResolvedTable$.create(v2ResolutionPlans.scala:231)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$4(Analyzer.scala:1600)\n",
       "\t\tat scala.Option.map(Option.scala:230)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$3(Analyzer.scala:1592)\n",
       "\t\tat scala.Option.orElse(Option.scala:447)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableOrView(Analyzer.scala:1587)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1556)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1466)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:182)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:171)\n",
       "\t\tat scala.PartialFunction.applyOrElse(PartialFunction.scala:127)\n",
       "\t\tat scala.PartialFunction.applyOrElse$(PartialFunction.scala:126)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.applyOrElse(AnalysisHelper.scala:171)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)\n",
       "\t\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:138)\n",
       "\t\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1330)\n",
       "\t\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1329)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.DescribeRelation.mapChildren(v2Commands.scala:726)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning(AnalysisHelper.scala:186)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning$(AnalysisHelper.scala:167)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithSubqueriesAndPruning(LogicalPlan.scala:42)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.resolveOperatorsUpWithSubqueriesAndPruningByConf(Analyzer.scala:1674)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply0(Analyzer.scala:1466)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1419)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1365)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:480)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:629)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:613)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:131)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:480)\n",
       "\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:479)\n",
       "\t\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n",
       "\t\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n",
       "\t\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:475)\n",
       "\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:452)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22(RuleExecutor.scala:585)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22$adapted(RuleExecutor.scala:585)\n",
       "\t\tat scala.collection.immutable.List.foreach(List.scala:431)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:585)\n",
       "\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:349)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:498)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:491)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:397)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:491)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:416)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:341)\n",
       "\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:219)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:341)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:252)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:96)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:131)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:87)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:478)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:478)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:294)\n",
       "\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:548)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:668)\n",
       "\t\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:151)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:668)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1307)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:661)\n",
       "\t\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:658)\n",
       "\t\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:658)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:288)\n",
       "\t\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:287)\n",
       "\t\tat scala.util.Try$.apply(Try.scala:213)\n",
       "\t\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)\n",
       "\t\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n",
       "\t\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n",
       "\t\t... 24 more\n",
       "</td><td>2025-08-13T14:37:56.416376Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>table</td><td>`dr_eo_amperity`.`silver`.`unified_coalesced`</td><td>clone</td><td>success</td><td>Table `dr_eo_amperity`.`silver`.`unified_coalesced` cloned successfully.</td><td>2025-08-13T14:37:56.37547Z</td><td>List(List(source_table_size, 25008603680), List(source_num_of_files, 234), List(num_of_synced_transactions, None), List(num_removed_files, 0), List(num_copied_files, 0), List(removed_files_size, 0), List(copied_files_size, 0))</td></tr><tr><td>DR_Clone_data_v2</td><td>table</td><td>`dr_eo_amperity`.`silver`.`unified_merged`</td><td>clone</td><td>error</td><td>Failed to clone table `dr_eo_amperity`.`silver`.`unified_merged`.. Error: [DELTA_CLONE_UNSUPPORTED_SOURCE] Unsupported DEEP clone source '_eo_amperity.silver.unified_merged', whose format is deltasharing.\n",
       "The supported formats are 'delta', 'iceberg' and 'parquet'.</td><td>2025-08-13T14:37:52.889861Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>table</td><td>`dr_eo_amperity`.`silver`.`unified_loyalty_events`</td><td>clone</td><td>error</td><td>Failed to clone table `dr_eo_amperity`.`silver`.`unified_loyalty_events`.. Error: [DELTA_CLONE_UNSUPPORTED_SOURCE] Unsupported DEEP clone source '_eo_amperity.silver.unified_loyalty_events', whose format is deltasharing.\n",
       "The supported formats are 'delta', 'iceberg' and 'parquet'.</td><td>2025-08-13T14:37:52.87844Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>table</td><td>`dr_eo_amperity`.`silver`.`unified_paid_media`</td><td>clone</td><td>error</td><td>Failed to clone table `dr_eo_amperity`.`silver`.`unified_paid_media`.. Error: [DELTA_CLONE_UNSUPPORTED_SOURCE] Unsupported DEEP clone source '_eo_amperity.silver.unified_paid_media', whose format is deltasharing.\n",
       "The supported formats are 'delta', 'iceberg' and 'parquet'.</td><td>2025-08-13T14:37:52.818532Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>table</td><td>`dr_eo_amperity`.`silver`.`unified_loyalty`</td><td>clone</td><td>error</td><td>Failed to clone table `dr_eo_amperity`.`silver`.`unified_loyalty`.. Error: [DELTA_CLONE_UNSUPPORTED_SOURCE] Unsupported DEEP clone source '_eo_amperity.silver.unified_loyalty', whose format is deltasharing.\n",
       "The supported formats are 'delta', 'iceberg' and 'parquet'.</td><td>2025-08-13T14:37:52.489629Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>table</td><td>`dr_eo_amperity`.`silver`.`unified_itemized_transactions`</td><td>clone</td><td>error</td><td>Failed to clone table `dr_eo_amperity`.`silver`.`unified_itemized_transactions`.. Error: [DELTA_CLONE_UNSUPPORTED_SOURCE] Unsupported DEEP clone source '_eo_amperity.silver.unified_itemized_transactions', whose format is deltasharing.\n",
       "The supported formats are 'delta', 'iceberg' and 'parquet'.</td><td>2025-08-13T14:37:33.427965Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>table</td><td>`dr_eo_amperity`.`silver`.`unified_household`</td><td>clone</td><td>error</td><td>Failed to clone table `dr_eo_amperity`.`silver`.`unified_household`.. Error: [DELTA_CLONE_UNSUPPORTED_SOURCE] Unsupported DEEP clone source '_eo_amperity.silver.unified_household', whose format is deltasharing.\n",
       "The supported formats are 'delta', 'iceberg' and 'parquet'.</td><td>2025-08-13T14:37:33.285288Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>table</td><td>`dr_eo_amperity`.`silver`.`unified_compliance`</td><td>clone</td><td>error</td><td>Failed to clone table `dr_eo_amperity`.`silver`.`unified_compliance`.. Error: [DELTA_CLONE_UNSUPPORTED_SOURCE] Unsupported DEEP clone source '_eo_amperity.silver.unified_compliance', whose format is deltasharing.\n",
       "The supported formats are 'delta', 'iceberg' and 'parquet'.</td><td>2025-08-13T14:37:33.285242Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>table</td><td>`dr_eo_amperity`.`silver`.`unified_customer`</td><td>clone</td><td>error</td><td>Failed to clone table `dr_eo_amperity`.`silver`.`unified_customer`.. Error: [DELTA_CLONE_UNSUPPORTED_SOURCE] Unsupported DEEP clone source '_eo_amperity.silver.unified_customer', whose format is deltasharing.\n",
       "The supported formats are 'delta', 'iceberg' and 'parquet'.</td><td>2025-08-13T14:37:33.204961Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>table</td><td>`dr_eo_amperity`.`silver`.`fiscal_calendar`</td><td>clone</td><td>error</td><td>Failed to clone table `dr_eo_amperity`.`silver`.`fiscal_calendar`.. Error: [DELTA_CLONE_UNSUPPORTED_SOURCE] Unsupported DEEP clone source '_eo_amperity.silver.fiscal_calendar', whose format is deltasharing.\n",
       "The supported formats are 'delta', 'iceberg' and 'parquet'.</td><td>2025-08-13T14:37:28.923444Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>table</td><td>`dr_eo_amperity`.`default`.`unified_transactions_reporting`</td><td>clone</td><td>error</td><td>Failed to clone table `dr_eo_amperity`.`default`.`unified_transactions_reporting`.. Error: [DELTA_CLONE_UNSUPPORTED_SOURCE] Unsupported DEEP clone source '_eo_amperity.default.unified_transactions_reporting', whose format is deltasharing.\n",
       "The supported formats are 'delta', 'iceberg' and 'parquet'.</td><td>2025-08-13T14:37:28.882035Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>table</td><td>`dr_eo_amperity`.`default`.`transaction_attributes_extended`</td><td>clone</td><td>error</td><td>Failed to clone table `dr_eo_amperity`.`default`.`transaction_attributes_extended`.. Error: [DELTA_CLONE_UNSUPPORTED_SOURCE] Unsupported DEEP clone source '_eo_amperity.default.transaction_attributes_extended', whose format is deltasharing.\n",
       "The supported formats are 'delta', 'iceberg' and 'parquet'.</td><td>2025-08-13T14:37:28.81271Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>table</td><td>`dr_eo_amperity`.`default`.`transaction_attributes`</td><td>clone</td><td>error</td><td>Failed to clone table `dr_eo_amperity`.`default`.`transaction_attributes`.. Error: [DELTA_CLONE_UNSUPPORTED_SOURCE] Unsupported DEEP clone source '_eo_amperity.default.transaction_attributes', whose format is deltasharing.\n",
       "The supported formats are 'delta', 'iceberg' and 'parquet'.</td><td>2025-08-13T14:37:28.741128Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>table</td><td>`dr_eo_amperity`.`default`.`predicted_clv_attributes`</td><td>clone</td><td>error</td><td>Failed to clone table `dr_eo_amperity`.`default`.`predicted_clv_attributes`.. Error: [DELTA_CLONE_UNSUPPORTED_SOURCE] Unsupported DEEP clone source '_eo_amperity.default.predicted_clv_attributes', whose format is deltasharing.\n",
       "The supported formats are 'delta', 'iceberg' and 'parquet'.</td><td>2025-08-13T14:37:28.582454Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>table</td><td>`dr_eo_amperity`.`bronze`.`wifi_connections`</td><td>clone</td><td>success</td><td>Table `dr_eo_amperity`.`bronze`.`wifi_connections` cloned successfully.</td><td>2025-08-13T14:37:25.094829Z</td><td>List(List(source_table_size, 7882742359), List(source_num_of_files, 52), List(num_of_synced_transactions, None), List(num_removed_files, 0), List(num_copied_files, 0), List(removed_files_size, 0), List(copied_files_size, 0))</td></tr><tr><td>DR_Clone_data_v2</td><td>table</td><td>`dr_eo_amperity`.`bronze`.`web_visitors`</td><td>clone</td><td>success</td><td>Table `dr_eo_amperity`.`bronze`.`web_visitors` cloned successfully.</td><td>2025-08-13T14:37:24.807545Z</td><td>List(List(source_table_size, 334349516), List(source_num_of_files, 7), List(num_of_synced_transactions, None), List(num_removed_files, 0), List(num_copied_files, 0), List(removed_files_size, 0), List(copied_files_size, 0))</td></tr><tr><td>DR_Clone_data_v2</td><td>table</td><td>`dr_eo_amperity`.`default`.`predicted_affinity`</td><td>clone</td><td>error</td><td>Failed to clone table `dr_eo_amperity`.`default`.`predicted_affinity`.. Error: [DELTA_CLONE_UNSUPPORTED_SOURCE] Unsupported DEEP clone source '_eo_amperity.default.predicted_affinity', whose format is deltasharing.\n",
       "The supported formats are 'delta', 'iceberg' and 'parquet'.</td><td>2025-08-13T14:37:24.461206Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>table</td><td>`dr_eo_amperity`.`default`.`email_engagement_summary`</td><td>clone</td><td>error</td><td>Failed to clone table `dr_eo_amperity`.`default`.`email_engagement_summary`.. Error: [DELTA_CLONE_UNSUPPORTED_SOURCE] Unsupported DEEP clone source '_eo_amperity.default.email_engagement_summary', whose format is deltasharing.\n",
       "The supported formats are 'delta', 'iceberg' and 'parquet'.</td><td>2025-08-13T14:37:24.407177Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>table</td><td>`dr_eo_amperity`.`default`.`email_engagement_attributes`</td><td>clone</td><td>error</td><td>Failed to clone table `dr_eo_amperity`.`default`.`email_engagement_attributes`.. Error: [DELTA_CLONE_UNSUPPORTED_SOURCE] Unsupported DEEP clone source '_eo_amperity.default.email_engagement_attributes', whose format is deltasharing.\n",
       "The supported formats are 'delta', 'iceberg' and 'parquet'.</td><td>2025-08-13T14:37:24.319345Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>table</td><td>`dr_eo_amperity`.`default`.`analytics_tae`</td><td>clone</td><td>error</td><td>Failed to clone table `dr_eo_amperity`.`default`.`analytics_tae`.. Error: [DELTA_CLONE_UNSUPPORTED_SOURCE] Unsupported DEEP clone source '_eo_amperity.default.analytics_tae', whose format is deltasharing.\n",
       "The supported formats are 'delta', 'iceberg' and 'parquet'.</td><td>2025-08-13T14:36:54.177016Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>table</td><td>`dr_eo_amperity`.`default`.`customer_attributes`</td><td>clone</td><td>error</td><td>Failed to clone table `dr_eo_amperity`.`default`.`customer_attributes`.. Error: [DELTA_CLONE_UNSUPPORTED_SOURCE] Unsupported DEEP clone source '_eo_amperity.default.customer_attributes', whose format is deltasharing.\n",
       "The supported formats are 'delta', 'iceberg' and 'parquet'.</td><td>2025-08-13T14:36:54.1364Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>table</td><td>`dr_eo_amperity`.`default`.`customer_360`</td><td>clone</td><td>error</td><td>Failed to clone table `dr_eo_amperity`.`default`.`customer_360`.. Error: [DELTA_CLONE_UNSUPPORTED_SOURCE] Unsupported DEEP clone source '_eo_amperity.default.customer_360', whose format is deltasharing.\n",
       "The supported formats are 'delta', 'iceberg' and 'parquet'.</td><td>2025-08-13T14:36:54.102789Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>table</td><td>`dr_eo_amperity`.`bronze`.`pos_customers`</td><td>clone</td><td>error</td><td>Failed to clone table `dr_eo_amperity`.`bronze`.`pos_customers`.. Error: [DELTA_CLONE_UNSUPPORTED_SOURCE] Unsupported DEEP clone source '_eo_amperity.bronze.pos_customers', whose format is deltasharing.\n",
       "The supported formats are 'delta', 'iceberg' and 'parquet'.</td><td>2025-08-13T14:36:50.056267Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>table</td><td>`dr_eo_amperity`.`bronze`.`e_commerce_customers`</td><td>clone</td><td>error</td><td>Failed to clone table `dr_eo_amperity`.`bronze`.`e_commerce_customers`.. Error: [DELTA_CLONE_UNSUPPORTED_SOURCE] Unsupported DEEP clone source '_eo_amperity.bronze.e_commerce_customers', whose format is deltasharing.\n",
       "The supported formats are 'delta', 'iceberg' and 'parquet'.</td><td>2025-08-13T14:36:50.053434Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>table</td><td>`dr_eo_amperity`.`bronze`.`loyalty_members`</td><td>clone</td><td>error</td><td>Failed to clone table `dr_eo_amperity`.`bronze`.`loyalty_members`.. Error: [DELTA_CLONE_UNSUPPORTED_SOURCE] Unsupported DEEP clone source '_eo_amperity.bronze.loyalty_members', whose format is deltasharing.\n",
       "The supported formats are 'delta', 'iceberg' and 'parquet'.</td><td>2025-08-13T14:36:50.030959Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>catalog_pair</td><td>satya_share -> satya_dr_catalog</td><td>process</td><td>sequential_complete</td><td>Finished sequential processing for catalog pair: satya_share -> satya_dr_catalog</td><td>2025-08-13T14:36:47.566078Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>schema</td><td>`satya_dr_catalog`.`satyendranath_sure`</td><td>change_ownership</td><td>skipped</td><td>No ownership mapping found for this schema. Skipping ownership change.</td><td>2025-08-13T14:36:46.094778Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>schema</td><td>`satya_dr_catalog`.`satyendranath_sure`</td><td>create</td><td>success</td><td>Schema `satya_dr_catalog`.`satyendranath_sure` created or already exists.</td><td>2025-08-13T14:36:44.714022Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>schema</td><td>`satya_share`.`information_schema`</td><td>skip</td><td>success</td><td>Skipping `information_schema`.</td><td>2025-08-13T14:36:43.224012Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>catalog</td><td>`satya_dr_catalog`</td><td>create</td><td>success</td><td>Catalog `satya_dr_catalog` created or already exists.</td><td>2025-08-13T14:36:40.594553Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>catalog_pair</td><td>satya_share -> satya_dr_catalog</td><td>process</td><td>start</td><td>Starting processing for catalog pair: satya_share -> satya_dr_catalog</td><td>2025-08-13T14:36:38.839076Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>catalog</td><td>`test_not`</td><td>get_schemas</td><td>error</td><td>Failed to get schemas from source catalog `test_not`. Error: [NO_SUCH_CATALOG_EXCEPTION] Catalog 'test_not' was not found. Please verify the catalog name and then retry the query or command again. SQLSTATE: 42704</td><td>2025-08-13T14:36:37.307457Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>catalog</td><td>`dr_test_not`</td><td>create</td><td>success</td><td>Catalog `dr_test_not` created or already exists.</td><td>2025-08-13T14:36:35.677033Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>catalog_pair</td><td>test_not -> dr_test_not</td><td>process</td><td>start</td><td>Starting processing for catalog pair: test_not -> dr_test_not</td><td>2025-08-13T14:36:33.857224Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>catalog_pair</td><td>accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample -> dr_accuweather</td><td>process</td><td>sequential_complete</td><td>Finished sequential processing for catalog pair: accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample -> dr_accuweather</td><td>2025-08-13T14:36:32.457098Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>schema</td><td>`accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample`.`information_schema`</td><td>skip</td><td>success</td><td>Skipping `information_schema`.</td><td>2025-08-13T14:36:31.116006Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>schema</td><td>`dr_accuweather`.`forecast`</td><td>change_ownership</td><td>skipped</td><td>No ownership mapping found for this schema. Skipping ownership change.</td><td>2025-08-13T14:36:29.498239Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>schema</td><td>`dr_accuweather`.`forecast`</td><td>create</td><td>success</td><td>Schema `dr_accuweather`.`forecast` created or already exists.</td><td>2025-08-13T14:36:28.109957Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>catalog</td><td>`dr_accuweather`</td><td>create</td><td>success</td><td>Catalog `dr_accuweather` created or already exists.</td><td>2025-08-13T14:36:25.530754Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>catalog_pair</td><td>accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample -> dr_accuweather</td><td>process</td><td>start</td><td>Starting processing for catalog pair: accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample -> dr_accuweather</td><td>2025-08-13T14:36:23.955123Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>catalog_pair</td><td>_eo_amperity -> dr_eo_amperity</td><td>process</td><td>sequential_complete</td><td>Finished sequential processing for catalog pair: _eo_amperity -> dr_eo_amperity</td><td>2025-08-13T14:36:21.862531Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>schema</td><td>`dr_eo_amperity`.`silver`</td><td>change_ownership</td><td>success</td><td>Ownership of schema `dr_eo_amperity`.`silver` changed to `grp_test_clt_usa`.</td><td>2025-08-13T14:36:20.152484Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>schema</td><td>`dr_eo_amperity`.`silver`</td><td>create</td><td>success</td><td>Schema `dr_eo_amperity`.`silver` created or already exists.</td><td>2025-08-13T14:36:18.376482Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>schema</td><td>`_eo_amperity`.`information_schema`</td><td>skip</td><td>success</td><td>Skipping `information_schema`.</td><td>2025-08-13T14:36:16.607961Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>schema</td><td>`dr_eo_amperity`.`default`</td><td>change_ownership</td><td>success</td><td>Ownership of schema `dr_eo_amperity`.`default` changed to `grp_test_clt_usa`.</td><td>2025-08-13T14:36:11.496898Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>schema</td><td>`dr_eo_amperity`.`default`</td><td>create</td><td>success</td><td>Schema `dr_eo_amperity`.`default` created or already exists.</td><td>2025-08-13T14:36:09.844828Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>schema</td><td>`dr_eo_amperity`.`bronze`</td><td>change_ownership</td><td>success</td><td>Ownership of schema `dr_eo_amperity`.`bronze` changed to `grp_test_clt_usa`.</td><td>2025-08-13T14:36:08.030969Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>schema</td><td>`dr_eo_amperity`.`bronze`</td><td>create</td><td>success</td><td>Schema `dr_eo_amperity`.`bronze` created or already exists.</td><td>2025-08-13T14:36:06.289496Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>catalog</td><td>`dr_eo_amperity`</td><td>create</td><td>success</td><td>Catalog `dr_eo_amperity` created or already exists.</td><td>2025-08-13T14:36:03.906827Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>catalog_pair</td><td>_eo_amperity -> dr_eo_amperity</td><td>process</td><td>start</td><td>Starting processing for catalog pair: _eo_amperity -> dr_eo_amperity</td><td>2025-08-13T14:36:00.443416Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>table</td><td>`satya_dr_catalog`.`satyendranath_sure`.`satya_external_table`</td><td>clone</td><td>success</td><td>Table `satya_dr_catalog`.`satyendranath_sure`.`satya_external_table` cloned successfully.</td><td>2025-08-13T14:34:18.160459Z</td><td>List(List(source_table_size, 826), List(source_num_of_files, 1), List(num_of_synced_transactions, None), List(num_removed_files, 0), List(num_copied_files, 1), List(removed_files_size, 0), List(copied_files_size, 826))</td></tr><tr><td>DR_Clone_data_v2</td><td>table</td><td>`satya_dr_catalog`.`satyendranath_sure`.`satya_managed_table`</td><td>clone</td><td>success</td><td>Table `satya_dr_catalog`.`satyendranath_sure`.`satya_managed_table` cloned successfully.</td><td>2025-08-13T14:34:16.161858Z</td><td>List(List(source_table_size, 827), List(source_num_of_files, 1), List(num_of_synced_transactions, None), List(num_removed_files, 0), List(num_copied_files, 1), List(removed_files_size, 0), List(copied_files_size, 827))</td></tr><tr><td>DR_Clone_data_v2</td><td>table/view</td><td>`accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample`.`forecast`.`us_postal_hourly_metric`</td><td>clone/create</td><td>error</td><td>Failed to process table/view `accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample`.`forecast`.`us_postal_hourly_metric`. Error: An error occurred while calling o427.sql.\n",
       ": com.databricks.sql.managedcatalog.acl.UnauthorizedAccessException: PERMISSION_DENIED: User does not have USE CATALOG on Catalog 'accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample'.\n",
       "\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:87)\n",
       "\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:63)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:251)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapExceptionBase(ManagedCatalogClientImpl.scala:7378)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:7364)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.getTableCredentials(ManagedCatalogClientImpl.scala:3951)\n",
       "\tat com.databricks.sql.managedcatalog.ManagedCatalogClient.getTemporaryCredentials(ManagedCatalogClient.scala:2620)\n",
       "\tat com.databricks.sql.managedcatalog.ManagedCatalogClient.getTemporaryCredentials$(ManagedCatalogClient.scala:2605)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.getTemporaryCredentials(ManagedCatalogClientImpl.scala:251)\n",
       "\tat com.databricks.unity.TempCredCache.$anonfun$getInternal$7(TemporaryCredentials.scala:448)\n",
       "\tat com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4724)\n",
       "\tat com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3522)\n",
       "\tat com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2315)\n",
       "\tat com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2278)\n",
       "\tat com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2193)\n",
       "\tat com.google.common.cache.LocalCache.get(LocalCache.java:3932)\n",
       "\tat com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4721)\n",
       "\tat com.databricks.unity.TempCredCache.liftedTree1$1(TemporaryCredentials.scala:447)\n",
       "\tat com.databricks.unity.TempCredCache.getInternal(TemporaryCredentials.scala:446)\n",
       "\tat com.databricks.unity.TempCredCache.get(TemporaryCredentials.scala:373)\n",
       "\tat com.databricks.unity.UnityCredentialManager.getTemporaryCredentials(CredentialManager.scala:488)\n",
       "\tat com.databricks.unity.CredentialManager$.getTemporaryCredentials(CredentialManager.scala:881)\n",
       "\tat com.databricks.unity.SAM.setConfFromCred$1(SAM.scala:338)\n",
       "\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$8(SAM.scala:353)\n",
       "\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$8$adapted(SAM.scala:352)\n",
       "\tat com.databricks.unity.TempCredRequest$.$anonfun$withFallbackOnOverlappingForParentPath$3(TemporaryCredentials.scala:147)\n",
       "\tat scala.util.Try$.apply(Try.scala:213)\n",
       "\tat com.databricks.unity.TempCredRequest$.withFallbackOnOverlappingForParentPath(TemporaryCredentials.scala:147)\n",
       "\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$7(SAM.scala:352)\n",
       "\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$7$adapted(SAM.scala:350)\n",
       "\tat scala.Option.map(Option.scala:230)\n",
       "\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$6(SAM.scala:350)\n",
       "\tat scala.Option.flatMap(Option.scala:271)\n",
       "\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$1(SAM.scala:349)\n",
       "\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$1$adapted(SAM.scala:336)\n",
       "\tat scala.Option.foreach(Option.scala:407)\n",
       "\tat com.databricks.unity.SAM.setFileSystemConfigs(SAM.scala:336)\n",
       "\tat com.databricks.unity.SAM.createDelegate(SAM.scala:388)\n",
       "\tat com.databricks.unity.SAM.createDelegate$(SAM.scala:372)\n",
       "\tat com.databricks.unity.TableSAM.createDelegate(SAM.scala:695)\n",
       "\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.createDelegate(CredentialScopeFileSystem.scala:74)\n",
       "\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.$anonfun$setDelegates$1(CredentialScopeFileSystem.scala:127)\n",
       "\tat com.databricks.sql.acl.fs.Lazy.apply(DelegatingFileSystem.scala:310)\n",
       "\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.readDelegateMakeQualified(CredentialScopeFileSystem.scala:282)\n",
       "\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.optimizedMakeQualifiedPath(CredentialScopeFileSystem.scala:277)\n",
       "\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.makeQualified(CredentialScopeFileSystem.scala:294)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaLog$.apply(DeltaLog.scala:1284)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaLog$.forTable(DeltaLog.scala:1087)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$5(DeltaTableV2.scala:175)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$withAdditionalSnapshotInitializationUsageLogData$1(DeltaLog.scala:968)\n",
       "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
       "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
       "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaLog$.withAdditionalSnapshotInitializationUsageLogData(DeltaLog.scala:969)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$2(DeltaTableV2.scala:169)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:700)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$1(DeltaTableV2.scala:150)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:83)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.deltaLog$lzycompute(DeltaTableV2.scala:150)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.deltaLog(DeltaTableV2.scala:148)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$8(DeltaTableV2.scala:264)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$withAdditionalSnapshotInitializationUsageLogData$1(DeltaLog.scala:968)\n",
       "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
       "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
       "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaLog$.withAdditionalSnapshotInitializationUsageLogData(DeltaLog.scala:969)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$7(DeltaTableV2.scala:263)\n",
       "\tat scala.Option.orElse(Option.scala:447)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$5(DeltaTableV2.scala:263)\n",
       "\tat scala.Option.orElse(Option.scala:447)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$2(DeltaTableV2.scala:260)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:83)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$1(DeltaTableV2.scala:290)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:700)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot$lzycompute(DeltaTableV2.scala:240)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot(DeltaTableV2.scala:238)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$tableSchema$2(DeltaTableV2.scala:317)\n",
       "\tat scala.Option.getOrElse(Option.scala:189)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.tableSchema$lzycompute(DeltaTableV2.scala:317)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.tableSchema(DeltaTableV2.scala:315)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.schema(DeltaTableV2.scala:322)\n",
       "\tat org.apache.spark.sql.connector.catalog.Table.columns(Table.java:68)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.columns(DeltaTableV2.scala:83)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.ResolvedTable$.create(v2ResolutionPlans.scala:231)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$4(Analyzer.scala:1600)\n",
       "\tat scala.Option.map(Option.scala:230)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$3(Analyzer.scala:1592)\n",
       "\tat scala.Option.orElse(Option.scala:447)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableOrView(Analyzer.scala:1587)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1556)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1466)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:182)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:171)\n",
       "\tat scala.PartialFunction.applyOrElse(PartialFunction.scala:127)\n",
       "\tat scala.PartialFunction.applyOrElse$(PartialFunction.scala:126)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.applyOrElse(AnalysisHelper.scala:171)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)\n",
       "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:138)\n",
       "\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1330)\n",
       "\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1329)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.DescribeRelation.mapChildren(v2Commands.scala:726)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning(AnalysisHelper.scala:186)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning$(AnalysisHelper.scala:167)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithSubqueriesAndPruning(LogicalPlan.scala:42)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.resolveOperatorsUpWithSubqueriesAndPruningByConf(Analyzer.scala:1674)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply0(Analyzer.scala:1466)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1419)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1365)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:480)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:629)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:613)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:131)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:480)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:479)\n",
       "\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n",
       "\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n",
       "\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:475)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:452)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22(RuleExecutor.scala:585)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22$adapted(RuleExecutor.scala:585)\n",
       "\tat scala.collection.immutable.List.foreach(List.scala:431)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:585)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:349)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:498)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:491)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:397)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:491)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:416)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:341)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:219)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:341)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:252)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:96)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:131)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:87)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:478)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:478)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:294)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:548)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:668)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:151)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:668)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1307)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:661)\n",
       "\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:658)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:658)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:288)\n",
       "\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:287)\n",
       "\tat scala.util.Try$.apply(Try.scala:213)\n",
       "\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)\n",
       "\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)\n",
       "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:321)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:267)\n",
       "\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$3(Dataset.scala:149)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n",
       "\tat org.apache.spark.sql.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1469)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1469)\n",
       "\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:141)\n",
       "\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:1048)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n",
       "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:1001)\n",
       "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:1071)\n",
       "\tat jdk.internal.reflect.GeneratedMethodAccessor593.invoke(Unknown Source)\n",
       "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
       "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
       "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n",
       "\tat py4j.Gateway.invoke(Gateway.java:306)\n",
       "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
       "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:197)\n",
       "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:117)\n",
       "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
       "\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\n",
       "\t\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:87)\n",
       "\t\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:63)\n",
       "\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:251)\n",
       "\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapExceptionBase(ManagedCatalogClientImpl.scala:7378)\n",
       "\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:7364)\n",
       "\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.getTableCredentials(ManagedCatalogClientImpl.scala:3951)\n",
       "\t\tat com.databricks.sql.managedcatalog.ManagedCatalogClient.getTemporaryCredentials(ManagedCatalogClient.scala:2620)\n",
       "\t\tat com.databricks.sql.managedcatalog.ManagedCatalogClient.getTemporaryCredentials$(ManagedCatalogClient.scala:2605)\n",
       "\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.getTemporaryCredentials(ManagedCatalogClientImpl.scala:251)\n",
       "\t\tat com.databricks.unity.TempCredCache.$anonfun$getInternal$7(TemporaryCredentials.scala:448)\n",
       "\t\tat com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4724)\n",
       "\t\tat com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3522)\n",
       "\t\tat com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2315)\n",
       "\t\tat com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2278)\n",
       "\t\tat com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2193)\n",
       "\t\tat com.google.common.cache.LocalCache.get(LocalCache.java:3932)\n",
       "\t\tat com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4721)\n",
       "\t\tat com.databricks.unity.TempCredCache.liftedTree1$1(TemporaryCredentials.scala:447)\n",
       "\t\tat com.databricks.unity.TempCredCache.getInternal(TemporaryCredentials.scala:446)\n",
       "\t\tat com.databricks.unity.TempCredCache.get(TemporaryCredentials.scala:373)\n",
       "\t\tat com.databricks.unity.UnityCredentialManager.getTemporaryCredentials(CredentialManager.scala:488)\n",
       "\t\tat com.databricks.unity.CredentialManager$.getTemporaryCredentials(CredentialManager.scala:881)\n",
       "\t\tat com.databricks.unity.SAM.setConfFromCred$1(SAM.scala:338)\n",
       "\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$8(SAM.scala:353)\n",
       "\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$8$adapted(SAM.scala:352)\n",
       "\t\tat com.databricks.unity.TempCredRequest$.$anonfun$withFallbackOnOverlappingForParentPath$3(TemporaryCredentials.scala:147)\n",
       "\t\tat scala.util.Try$.apply(Try.scala:213)\n",
       "\t\tat com.databricks.unity.TempCredRequest$.withFallbackOnOverlappingForParentPath(TemporaryCredentials.scala:147)\n",
       "\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$7(SAM.scala:352)\n",
       "\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$7$adapted(SAM.scala:350)\n",
       "\t\tat scala.Option.map(Option.scala:230)\n",
       "\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$6(SAM.scala:350)\n",
       "\t\tat scala.Option.flatMap(Option.scala:271)\n",
       "\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$1(SAM.scala:349)\n",
       "\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$1$adapted(SAM.scala:336)\n",
       "\t\tat scala.Option.foreach(Option.scala:407)\n",
       "\t\tat com.databricks.unity.SAM.setFileSystemConfigs(SAM.scala:336)\n",
       "\t\tat com.databricks.unity.SAM.createDelegate(SAM.scala:388)\n",
       "\t\tat com.databricks.unity.SAM.createDelegate$(SAM.scala:372)\n",
       "\t\tat com.databricks.unity.TableSAM.createDelegate(SAM.scala:695)\n",
       "\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.createDelegate(CredentialScopeFileSystem.scala:74)\n",
       "\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.$anonfun$setDelegates$1(CredentialScopeFileSystem.scala:127)\n",
       "\t\tat com.databricks.sql.acl.fs.Lazy.apply(DelegatingFileSystem.scala:310)\n",
       "\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.readDelegateMakeQualified(CredentialScopeFileSystem.scala:282)\n",
       "\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.optimizedMakeQualifiedPath(CredentialScopeFileSystem.scala:277)\n",
       "\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.makeQualified(CredentialScopeFileSystem.scala:294)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.apply(DeltaLog.scala:1284)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.forTable(DeltaLog.scala:1087)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$5(DeltaTableV2.scala:175)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$withAdditionalSnapshotInitializationUsageLogData$1(DeltaLog.scala:968)\n",
       "\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
       "\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
       "\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.withAdditionalSnapshotInitializationUsageLogData(DeltaLog.scala:969)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$2(DeltaTableV2.scala:169)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:700)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$1(DeltaTableV2.scala:150)\n",
       "\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:83)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.deltaLog$lzycompute(DeltaTableV2.scala:150)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.deltaLog(DeltaTableV2.scala:148)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$8(DeltaTableV2.scala:264)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$withAdditionalSnapshotInitializationUsageLogData$1(DeltaLog.scala:968)\n",
       "\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
       "\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
       "\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.withAdditionalSnapshotInitializationUsageLogData(DeltaLog.scala:969)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$7(DeltaTableV2.scala:263)\n",
       "\t\tat scala.Option.orElse(Option.scala:447)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$5(DeltaTableV2.scala:263)\n",
       "\t\tat scala.Option.orElse(Option.scala:447)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$2(DeltaTableV2.scala:260)\n",
       "\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:83)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$1(DeltaTableV2.scala:290)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:700)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot$lzycompute(DeltaTableV2.scala:240)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot(DeltaTableV2.scala:238)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$tableSchema$2(DeltaTableV2.scala:317)\n",
       "\t\tat scala.Option.getOrElse(Option.scala:189)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.tableSchema$lzycompute(DeltaTableV2.scala:317)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.tableSchema(DeltaTableV2.scala:315)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.schema(DeltaTableV2.scala:322)\n",
       "\t\tat org.apache.spark.sql.connector.catalog.Table.columns(Table.java:68)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.columns(DeltaTableV2.scala:83)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.ResolvedTable$.create(v2ResolutionPlans.scala:231)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$4(Analyzer.scala:1600)\n",
       "\t\tat scala.Option.map(Option.scala:230)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$3(Analyzer.scala:1592)\n",
       "\t\tat scala.Option.orElse(Option.scala:447)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableOrView(Analyzer.scala:1587)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1556)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1466)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:182)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:171)\n",
       "\t\tat scala.PartialFunction.applyOrElse(PartialFunction.scala:127)\n",
       "\t\tat scala.PartialFunction.applyOrElse$(PartialFunction.scala:126)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.applyOrElse(AnalysisHelper.scala:171)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)\n",
       "\t\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:138)\n",
       "\t\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1330)\n",
       "\t\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1329)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.DescribeRelation.mapChildren(v2Commands.scala:726)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning(AnalysisHelper.scala:186)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning$(AnalysisHelper.scala:167)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithSubqueriesAndPruning(LogicalPlan.scala:42)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.resolveOperatorsUpWithSubqueriesAndPruningByConf(Analyzer.scala:1674)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply0(Analyzer.scala:1466)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1419)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1365)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:480)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:629)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:613)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:131)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:480)\n",
       "\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:479)\n",
       "\t\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n",
       "\t\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n",
       "\t\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:475)\n",
       "\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:452)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22(RuleExecutor.scala:585)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22$adapted(RuleExecutor.scala:585)\n",
       "\t\tat scala.collection.immutable.List.foreach(List.scala:431)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:585)\n",
       "\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:349)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:498)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:491)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:397)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:491)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:416)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:341)\n",
       "\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:219)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:341)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:252)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:96)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:131)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:87)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:478)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:478)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:294)\n",
       "\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:548)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:668)\n",
       "\t\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:151)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:668)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1307)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:661)\n",
       "\t\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:658)\n",
       "\t\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:658)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:288)\n",
       "\t\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:287)\n",
       "\t\tat scala.util.Try$.apply(Try.scala:213)\n",
       "\t\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)\n",
       "\t\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n",
       "\t\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n",
       "\t\t... 24 more\n",
       "</td><td>2025-08-13T14:34:05.612553Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>table/view</td><td>`accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample`.`forecast`.`us_postal_hourly_imperial`</td><td>clone/create</td><td>error</td><td>Failed to process table/view `accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample`.`forecast`.`us_postal_hourly_imperial`. Error: An error occurred while calling o427.sql.\n",
       ": com.databricks.sql.managedcatalog.acl.UnauthorizedAccessException: PERMISSION_DENIED: User does not have USE CATALOG on Catalog 'accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample'.\n",
       "\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:87)\n",
       "\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:63)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:251)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapExceptionBase(ManagedCatalogClientImpl.scala:7378)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:7364)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.getTableCredentials(ManagedCatalogClientImpl.scala:3951)\n",
       "\tat com.databricks.sql.managedcatalog.ManagedCatalogClient.getTemporaryCredentials(ManagedCatalogClient.scala:2620)\n",
       "\tat com.databricks.sql.managedcatalog.ManagedCatalogClient.getTemporaryCredentials$(ManagedCatalogClient.scala:2605)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.getTemporaryCredentials(ManagedCatalogClientImpl.scala:251)\n",
       "\tat com.databricks.unity.TempCredCache.$anonfun$getInternal$7(TemporaryCredentials.scala:448)\n",
       "\tat com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4724)\n",
       "\tat com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3522)\n",
       "\tat com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2315)\n",
       "\tat com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2278)\n",
       "\tat com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2193)\n",
       "\tat com.google.common.cache.LocalCache.get(LocalCache.java:3932)\n",
       "\tat com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4721)\n",
       "\tat com.databricks.unity.TempCredCache.liftedTree1$1(TemporaryCredentials.scala:447)\n",
       "\tat com.databricks.unity.TempCredCache.getInternal(TemporaryCredentials.scala:446)\n",
       "\tat com.databricks.unity.TempCredCache.get(TemporaryCredentials.scala:373)\n",
       "\tat com.databricks.unity.UnityCredentialManager.getTemporaryCredentials(CredentialManager.scala:488)\n",
       "\tat com.databricks.unity.CredentialManager$.getTemporaryCredentials(CredentialManager.scala:881)\n",
       "\tat com.databricks.unity.SAM.setConfFromCred$1(SAM.scala:338)\n",
       "\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$8(SAM.scala:353)\n",
       "\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$8$adapted(SAM.scala:352)\n",
       "\tat com.databricks.unity.TempCredRequest$.$anonfun$withFallbackOnOverlappingForParentPath$3(TemporaryCredentials.scala:147)\n",
       "\tat scala.util.Try$.apply(Try.scala:213)\n",
       "\tat com.databricks.unity.TempCredRequest$.withFallbackOnOverlappingForParentPath(TemporaryCredentials.scala:147)\n",
       "\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$7(SAM.scala:352)\n",
       "\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$7$adapted(SAM.scala:350)\n",
       "\tat scala.Option.map(Option.scala:230)\n",
       "\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$6(SAM.scala:350)\n",
       "\tat scala.Option.flatMap(Option.scala:271)\n",
       "\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$1(SAM.scala:349)\n",
       "\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$1$adapted(SAM.scala:336)\n",
       "\tat scala.Option.foreach(Option.scala:407)\n",
       "\tat com.databricks.unity.SAM.setFileSystemConfigs(SAM.scala:336)\n",
       "\tat com.databricks.unity.SAM.createDelegate(SAM.scala:388)\n",
       "\tat com.databricks.unity.SAM.createDelegate$(SAM.scala:372)\n",
       "\tat com.databricks.unity.TableSAM.createDelegate(SAM.scala:695)\n",
       "\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.createDelegate(CredentialScopeFileSystem.scala:74)\n",
       "\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.$anonfun$setDelegates$1(CredentialScopeFileSystem.scala:127)\n",
       "\tat com.databricks.sql.acl.fs.Lazy.apply(DelegatingFileSystem.scala:310)\n",
       "\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.readDelegateMakeQualified(CredentialScopeFileSystem.scala:282)\n",
       "\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.optimizedMakeQualifiedPath(CredentialScopeFileSystem.scala:277)\n",
       "\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.makeQualified(CredentialScopeFileSystem.scala:294)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaLog$.apply(DeltaLog.scala:1284)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaLog$.forTable(DeltaLog.scala:1087)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$5(DeltaTableV2.scala:175)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$withAdditionalSnapshotInitializationUsageLogData$1(DeltaLog.scala:968)\n",
       "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
       "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
       "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaLog$.withAdditionalSnapshotInitializationUsageLogData(DeltaLog.scala:969)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$2(DeltaTableV2.scala:169)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:700)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$1(DeltaTableV2.scala:150)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:83)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.deltaLog$lzycompute(DeltaTableV2.scala:150)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.deltaLog(DeltaTableV2.scala:148)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$8(DeltaTableV2.scala:264)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$withAdditionalSnapshotInitializationUsageLogData$1(DeltaLog.scala:968)\n",
       "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
       "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
       "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaLog$.withAdditionalSnapshotInitializationUsageLogData(DeltaLog.scala:969)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$7(DeltaTableV2.scala:263)\n",
       "\tat scala.Option.orElse(Option.scala:447)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$5(DeltaTableV2.scala:263)\n",
       "\tat scala.Option.orElse(Option.scala:447)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$2(DeltaTableV2.scala:260)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:83)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$1(DeltaTableV2.scala:290)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:700)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot$lzycompute(DeltaTableV2.scala:240)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot(DeltaTableV2.scala:238)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$tableSchema$2(DeltaTableV2.scala:317)\n",
       "\tat scala.Option.getOrElse(Option.scala:189)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.tableSchema$lzycompute(DeltaTableV2.scala:317)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.tableSchema(DeltaTableV2.scala:315)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.schema(DeltaTableV2.scala:322)\n",
       "\tat org.apache.spark.sql.connector.catalog.Table.columns(Table.java:68)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.columns(DeltaTableV2.scala:83)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.ResolvedTable$.create(v2ResolutionPlans.scala:231)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$4(Analyzer.scala:1600)\n",
       "\tat scala.Option.map(Option.scala:230)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$3(Analyzer.scala:1592)\n",
       "\tat scala.Option.orElse(Option.scala:447)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableOrView(Analyzer.scala:1587)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1556)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1466)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:182)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:171)\n",
       "\tat scala.PartialFunction.applyOrElse(PartialFunction.scala:127)\n",
       "\tat scala.PartialFunction.applyOrElse$(PartialFunction.scala:126)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.applyOrElse(AnalysisHelper.scala:171)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)\n",
       "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:138)\n",
       "\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1330)\n",
       "\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1329)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.DescribeRelation.mapChildren(v2Commands.scala:726)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning(AnalysisHelper.scala:186)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning$(AnalysisHelper.scala:167)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithSubqueriesAndPruning(LogicalPlan.scala:42)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.resolveOperatorsUpWithSubqueriesAndPruningByConf(Analyzer.scala:1674)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply0(Analyzer.scala:1466)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1419)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1365)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:480)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:629)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:613)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:131)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:480)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:479)\n",
       "\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n",
       "\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n",
       "\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:475)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:452)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22(RuleExecutor.scala:585)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22$adapted(RuleExecutor.scala:585)\n",
       "\tat scala.collection.immutable.List.foreach(List.scala:431)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:585)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:349)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:498)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:491)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:397)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:491)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:416)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:341)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:219)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:341)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:252)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:96)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:131)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:87)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:478)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:478)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:294)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:548)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:668)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:151)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:668)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1307)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:661)\n",
       "\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:658)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:658)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:288)\n",
       "\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:287)\n",
       "\tat scala.util.Try$.apply(Try.scala:213)\n",
       "\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)\n",
       "\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)\n",
       "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:321)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:267)\n",
       "\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$3(Dataset.scala:149)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n",
       "\tat org.apache.spark.sql.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1469)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1469)\n",
       "\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:141)\n",
       "\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:1048)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n",
       "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:1001)\n",
       "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:1071)\n",
       "\tat jdk.internal.reflect.GeneratedMethodAccessor593.invoke(Unknown Source)\n",
       "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
       "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
       "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n",
       "\tat py4j.Gateway.invoke(Gateway.java:306)\n",
       "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
       "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:197)\n",
       "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:117)\n",
       "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
       "\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\n",
       "\t\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:87)\n",
       "\t\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:63)\n",
       "\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:251)\n",
       "\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapExceptionBase(ManagedCatalogClientImpl.scala:7378)\n",
       "\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:7364)\n",
       "\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.getTableCredentials(ManagedCatalogClientImpl.scala:3951)\n",
       "\t\tat com.databricks.sql.managedcatalog.ManagedCatalogClient.getTemporaryCredentials(ManagedCatalogClient.scala:2620)\n",
       "\t\tat com.databricks.sql.managedcatalog.ManagedCatalogClient.getTemporaryCredentials$(ManagedCatalogClient.scala:2605)\n",
       "\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.getTemporaryCredentials(ManagedCatalogClientImpl.scala:251)\n",
       "\t\tat com.databricks.unity.TempCredCache.$anonfun$getInternal$7(TemporaryCredentials.scala:448)\n",
       "\t\tat com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4724)\n",
       "\t\tat com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3522)\n",
       "\t\tat com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2315)\n",
       "\t\tat com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2278)\n",
       "\t\tat com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2193)\n",
       "\t\tat com.google.common.cache.LocalCache.get(LocalCache.java:3932)\n",
       "\t\tat com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4721)\n",
       "\t\tat com.databricks.unity.TempCredCache.liftedTree1$1(TemporaryCredentials.scala:447)\n",
       "\t\tat com.databricks.unity.TempCredCache.getInternal(TemporaryCredentials.scala:446)\n",
       "\t\tat com.databricks.unity.TempCredCache.get(TemporaryCredentials.scala:373)\n",
       "\t\tat com.databricks.unity.UnityCredentialManager.getTemporaryCredentials(CredentialManager.scala:488)\n",
       "\t\tat com.databricks.unity.CredentialManager$.getTemporaryCredentials(CredentialManager.scala:881)\n",
       "\t\tat com.databricks.unity.SAM.setConfFromCred$1(SAM.scala:338)\n",
       "\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$8(SAM.scala:353)\n",
       "\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$8$adapted(SAM.scala:352)\n",
       "\t\tat com.databricks.unity.TempCredRequest$.$anonfun$withFallbackOnOverlappingForParentPath$3(TemporaryCredentials.scala:147)\n",
       "\t\tat scala.util.Try$.apply(Try.scala:213)\n",
       "\t\tat com.databricks.unity.TempCredRequest$.withFallbackOnOverlappingForParentPath(TemporaryCredentials.scala:147)\n",
       "\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$7(SAM.scala:352)\n",
       "\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$7$adapted(SAM.scala:350)\n",
       "\t\tat scala.Option.map(Option.scala:230)\n",
       "\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$6(SAM.scala:350)\n",
       "\t\tat scala.Option.flatMap(Option.scala:271)\n",
       "\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$1(SAM.scala:349)\n",
       "\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$1$adapted(SAM.scala:336)\n",
       "\t\tat scala.Option.foreach(Option.scala:407)\n",
       "\t\tat com.databricks.unity.SAM.setFileSystemConfigs(SAM.scala:336)\n",
       "\t\tat com.databricks.unity.SAM.createDelegate(SAM.scala:388)\n",
       "\t\tat com.databricks.unity.SAM.createDelegate$(SAM.scala:372)\n",
       "\t\tat com.databricks.unity.TableSAM.createDelegate(SAM.scala:695)\n",
       "\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.createDelegate(CredentialScopeFileSystem.scala:74)\n",
       "\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.$anonfun$setDelegates$1(CredentialScopeFileSystem.scala:127)\n",
       "\t\tat com.databricks.sql.acl.fs.Lazy.apply(DelegatingFileSystem.scala:310)\n",
       "\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.readDelegateMakeQualified(CredentialScopeFileSystem.scala:282)\n",
       "\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.optimizedMakeQualifiedPath(CredentialScopeFileSystem.scala:277)\n",
       "\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.makeQualified(CredentialScopeFileSystem.scala:294)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.apply(DeltaLog.scala:1284)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.forTable(DeltaLog.scala:1087)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$5(DeltaTableV2.scala:175)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$withAdditionalSnapshotInitializationUsageLogData$1(DeltaLog.scala:968)\n",
       "\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
       "\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
       "\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.withAdditionalSnapshotInitializationUsageLogData(DeltaLog.scala:969)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$2(DeltaTableV2.scala:169)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:700)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$1(DeltaTableV2.scala:150)\n",
       "\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:83)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.deltaLog$lzycompute(DeltaTableV2.scala:150)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.deltaLog(DeltaTableV2.scala:148)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$8(DeltaTableV2.scala:264)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$withAdditionalSnapshotInitializationUsageLogData$1(DeltaLog.scala:968)\n",
       "\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
       "\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
       "\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.withAdditionalSnapshotInitializationUsageLogData(DeltaLog.scala:969)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$7(DeltaTableV2.scala:263)\n",
       "\t\tat scala.Option.orElse(Option.scala:447)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$5(DeltaTableV2.scala:263)\n",
       "\t\tat scala.Option.orElse(Option.scala:447)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$2(DeltaTableV2.scala:260)\n",
       "\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:83)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$1(DeltaTableV2.scala:290)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:700)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot$lzycompute(DeltaTableV2.scala:240)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot(DeltaTableV2.scala:238)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$tableSchema$2(DeltaTableV2.scala:317)\n",
       "\t\tat scala.Option.getOrElse(Option.scala:189)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.tableSchema$lzycompute(DeltaTableV2.scala:317)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.tableSchema(DeltaTableV2.scala:315)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.schema(DeltaTableV2.scala:322)\n",
       "\t\tat org.apache.spark.sql.connector.catalog.Table.columns(Table.java:68)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.columns(DeltaTableV2.scala:83)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.ResolvedTable$.create(v2ResolutionPlans.scala:231)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$4(Analyzer.scala:1600)\n",
       "\t\tat scala.Option.map(Option.scala:230)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$3(Analyzer.scala:1592)\n",
       "\t\tat scala.Option.orElse(Option.scala:447)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableOrView(Analyzer.scala:1587)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1556)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1466)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:182)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:171)\n",
       "\t\tat scala.PartialFunction.applyOrElse(PartialFunction.scala:127)\n",
       "\t\tat scala.PartialFunction.applyOrElse$(PartialFunction.scala:126)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.applyOrElse(AnalysisHelper.scala:171)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)\n",
       "\t\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:138)\n",
       "\t\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1330)\n",
       "\t\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1329)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.DescribeRelation.mapChildren(v2Commands.scala:726)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning(AnalysisHelper.scala:186)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning$(AnalysisHelper.scala:167)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithSubqueriesAndPruning(LogicalPlan.scala:42)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.resolveOperatorsUpWithSubqueriesAndPruningByConf(Analyzer.scala:1674)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply0(Analyzer.scala:1466)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1419)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1365)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:480)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:629)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:613)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:131)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:480)\n",
       "\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:479)\n",
       "\t\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n",
       "\t\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n",
       "\t\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:475)\n",
       "\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:452)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22(RuleExecutor.scala:585)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22$adapted(RuleExecutor.scala:585)\n",
       "\t\tat scala.collection.immutable.List.foreach(List.scala:431)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:585)\n",
       "\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:349)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:498)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:491)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:397)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:491)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:416)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:341)\n",
       "\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:219)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:341)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:252)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:96)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:131)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:87)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:478)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:478)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:294)\n",
       "\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:548)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:668)\n",
       "\t\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:151)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:668)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1307)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:661)\n",
       "\t\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:658)\n",
       "\t\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:658)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:288)\n",
       "\t\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:287)\n",
       "\t\tat scala.util.Try$.apply(Try.scala:213)\n",
       "\t\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)\n",
       "\t\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n",
       "\t\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n",
       "\t\t... 24 more\n",
       "</td><td>2025-08-13T14:34:05.336454Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>table/view</td><td>`accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample`.`forecast`.`us_postal_daynight_metric`</td><td>clone/create</td><td>error</td><td>Failed to process table/view `accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample`.`forecast`.`us_postal_daynight_metric`. Error: An error occurred while calling o427.sql.\n",
       ": com.databricks.sql.managedcatalog.acl.UnauthorizedAccessException: PERMISSION_DENIED: User does not have USE CATALOG on Catalog 'accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample'.\n",
       "\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:87)\n",
       "\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:63)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:251)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapExceptionBase(ManagedCatalogClientImpl.scala:7378)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:7364)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.getTableCredentials(ManagedCatalogClientImpl.scala:3951)\n",
       "\tat com.databricks.sql.managedcatalog.ManagedCatalogClient.getTemporaryCredentials(ManagedCatalogClient.scala:2620)\n",
       "\tat com.databricks.sql.managedcatalog.ManagedCatalogClient.getTemporaryCredentials$(ManagedCatalogClient.scala:2605)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.getTemporaryCredentials(ManagedCatalogClientImpl.scala:251)\n",
       "\tat com.databricks.unity.TempCredCache.$anonfun$getInternal$7(TemporaryCredentials.scala:448)\n",
       "\tat com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4724)\n",
       "\tat com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3522)\n",
       "\tat com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2315)\n",
       "\tat com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2278)\n",
       "\tat com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2193)\n",
       "\tat com.google.common.cache.LocalCache.get(LocalCache.java:3932)\n",
       "\tat com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4721)\n",
       "\tat com.databricks.unity.TempCredCache.liftedTree1$1(TemporaryCredentials.scala:447)\n",
       "\tat com.databricks.unity.TempCredCache.getInternal(TemporaryCredentials.scala:446)\n",
       "\tat com.databricks.unity.TempCredCache.get(TemporaryCredentials.scala:373)\n",
       "\tat com.databricks.unity.UnityCredentialManager.getTemporaryCredentials(CredentialManager.scala:488)\n",
       "\tat com.databricks.unity.CredentialManager$.getTemporaryCredentials(CredentialManager.scala:881)\n",
       "\tat com.databricks.unity.SAM.setConfFromCred$1(SAM.scala:338)\n",
       "\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$8(SAM.scala:353)\n",
       "\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$8$adapted(SAM.scala:352)\n",
       "\tat com.databricks.unity.TempCredRequest$.$anonfun$withFallbackOnOverlappingForParentPath$3(TemporaryCredentials.scala:147)\n",
       "\tat scala.util.Try$.apply(Try.scala:213)\n",
       "\tat com.databricks.unity.TempCredRequest$.withFallbackOnOverlappingForParentPath(TemporaryCredentials.scala:147)\n",
       "\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$7(SAM.scala:352)\n",
       "\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$7$adapted(SAM.scala:350)\n",
       "\tat scala.Option.map(Option.scala:230)\n",
       "\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$6(SAM.scala:350)\n",
       "\tat scala.Option.flatMap(Option.scala:271)\n",
       "\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$1(SAM.scala:349)\n",
       "\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$1$adapted(SAM.scala:336)\n",
       "\tat scala.Option.foreach(Option.scala:407)\n",
       "\tat com.databricks.unity.SAM.setFileSystemConfigs(SAM.scala:336)\n",
       "\tat com.databricks.unity.SAM.createDelegate(SAM.scala:388)\n",
       "\tat com.databricks.unity.SAM.createDelegate$(SAM.scala:372)\n",
       "\tat com.databricks.unity.TableSAM.createDelegate(SAM.scala:695)\n",
       "\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.createDelegate(CredentialScopeFileSystem.scala:74)\n",
       "\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.$anonfun$setDelegates$1(CredentialScopeFileSystem.scala:127)\n",
       "\tat com.databricks.sql.acl.fs.Lazy.apply(DelegatingFileSystem.scala:310)\n",
       "\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.readDelegateMakeQualified(CredentialScopeFileSystem.scala:282)\n",
       "\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.optimizedMakeQualifiedPath(CredentialScopeFileSystem.scala:277)\n",
       "\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.makeQualified(CredentialScopeFileSystem.scala:294)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaLog$.apply(DeltaLog.scala:1284)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaLog$.forTable(DeltaLog.scala:1087)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$5(DeltaTableV2.scala:175)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$withAdditionalSnapshotInitializationUsageLogData$1(DeltaLog.scala:968)\n",
       "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
       "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
       "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaLog$.withAdditionalSnapshotInitializationUsageLogData(DeltaLog.scala:969)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$2(DeltaTableV2.scala:169)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:700)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$1(DeltaTableV2.scala:150)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:83)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.deltaLog$lzycompute(DeltaTableV2.scala:150)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.deltaLog(DeltaTableV2.scala:148)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$8(DeltaTableV2.scala:264)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$withAdditionalSnapshotInitializationUsageLogData$1(DeltaLog.scala:968)\n",
       "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
       "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
       "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaLog$.withAdditionalSnapshotInitializationUsageLogData(DeltaLog.scala:969)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$7(DeltaTableV2.scala:263)\n",
       "\tat scala.Option.orElse(Option.scala:447)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$5(DeltaTableV2.scala:263)\n",
       "\tat scala.Option.orElse(Option.scala:447)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$2(DeltaTableV2.scala:260)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:83)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$1(DeltaTableV2.scala:290)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:700)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot$lzycompute(DeltaTableV2.scala:240)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot(DeltaTableV2.scala:238)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$tableSchema$2(DeltaTableV2.scala:317)\n",
       "\tat scala.Option.getOrElse(Option.scala:189)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.tableSchema$lzycompute(DeltaTableV2.scala:317)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.tableSchema(DeltaTableV2.scala:315)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.schema(DeltaTableV2.scala:322)\n",
       "\tat org.apache.spark.sql.connector.catalog.Table.columns(Table.java:68)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.columns(DeltaTableV2.scala:83)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.ResolvedTable$.create(v2ResolutionPlans.scala:231)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$4(Analyzer.scala:1600)\n",
       "\tat scala.Option.map(Option.scala:230)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$3(Analyzer.scala:1592)\n",
       "\tat scala.Option.orElse(Option.scala:447)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableOrView(Analyzer.scala:1587)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1556)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1466)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:182)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:171)\n",
       "\tat scala.PartialFunction.applyOrElse(PartialFunction.scala:127)\n",
       "\tat scala.PartialFunction.applyOrElse$(PartialFunction.scala:126)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.applyOrElse(AnalysisHelper.scala:171)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)\n",
       "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:138)\n",
       "\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1330)\n",
       "\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1329)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.DescribeRelation.mapChildren(v2Commands.scala:726)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning(AnalysisHelper.scala:186)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning$(AnalysisHelper.scala:167)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithSubqueriesAndPruning(LogicalPlan.scala:42)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.resolveOperatorsUpWithSubqueriesAndPruningByConf(Analyzer.scala:1674)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply0(Analyzer.scala:1466)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1419)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1365)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:480)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:629)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:613)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:131)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:480)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:479)\n",
       "\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n",
       "\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n",
       "\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:475)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:452)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22(RuleExecutor.scala:585)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22$adapted(RuleExecutor.scala:585)\n",
       "\tat scala.collection.immutable.List.foreach(List.scala:431)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:585)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:349)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:498)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:491)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:397)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:491)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:416)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:341)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:219)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:341)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:252)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:96)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:131)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:87)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:478)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:478)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:294)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:548)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:668)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:151)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:668)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1307)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:661)\n",
       "\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:658)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:658)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:288)\n",
       "\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:287)\n",
       "\tat scala.util.Try$.apply(Try.scala:213)\n",
       "\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)\n",
       "\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)\n",
       "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:321)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:267)\n",
       "\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$3(Dataset.scala:149)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n",
       "\tat org.apache.spark.sql.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1469)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1469)\n",
       "\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:141)\n",
       "\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:1048)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n",
       "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:1001)\n",
       "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:1071)\n",
       "\tat jdk.internal.reflect.GeneratedMethodAccessor593.invoke(Unknown Source)\n",
       "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
       "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
       "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n",
       "\tat py4j.Gateway.invoke(Gateway.java:306)\n",
       "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
       "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:197)\n",
       "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:117)\n",
       "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
       "\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\n",
       "\t\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:87)\n",
       "\t\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:63)\n",
       "\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:251)\n",
       "\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapExceptionBase(ManagedCatalogClientImpl.scala:7378)\n",
       "\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:7364)\n",
       "\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.getTableCredentials(ManagedCatalogClientImpl.scala:3951)\n",
       "\t\tat com.databricks.sql.managedcatalog.ManagedCatalogClient.getTemporaryCredentials(ManagedCatalogClient.scala:2620)\n",
       "\t\tat com.databricks.sql.managedcatalog.ManagedCatalogClient.getTemporaryCredentials$(ManagedCatalogClient.scala:2605)\n",
       "\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.getTemporaryCredentials(ManagedCatalogClientImpl.scala:251)\n",
       "\t\tat com.databricks.unity.TempCredCache.$anonfun$getInternal$7(TemporaryCredentials.scala:448)\n",
       "\t\tat com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4724)\n",
       "\t\tat com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3522)\n",
       "\t\tat com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2315)\n",
       "\t\tat com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2278)\n",
       "\t\tat com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2193)\n",
       "\t\tat com.google.common.cache.LocalCache.get(LocalCache.java:3932)\n",
       "\t\tat com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4721)\n",
       "\t\tat com.databricks.unity.TempCredCache.liftedTree1$1(TemporaryCredentials.scala:447)\n",
       "\t\tat com.databricks.unity.TempCredCache.getInternal(TemporaryCredentials.scala:446)\n",
       "\t\tat com.databricks.unity.TempCredCache.get(TemporaryCredentials.scala:373)\n",
       "\t\tat com.databricks.unity.UnityCredentialManager.getTemporaryCredentials(CredentialManager.scala:488)\n",
       "\t\tat com.databricks.unity.CredentialManager$.getTemporaryCredentials(CredentialManager.scala:881)\n",
       "\t\tat com.databricks.unity.SAM.setConfFromCred$1(SAM.scala:338)\n",
       "\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$8(SAM.scala:353)\n",
       "\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$8$adapted(SAM.scala:352)\n",
       "\t\tat com.databricks.unity.TempCredRequest$.$anonfun$withFallbackOnOverlappingForParentPath$3(TemporaryCredentials.scala:147)\n",
       "\t\tat scala.util.Try$.apply(Try.scala:213)\n",
       "\t\tat com.databricks.unity.TempCredRequest$.withFallbackOnOverlappingForParentPath(TemporaryCredentials.scala:147)\n",
       "\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$7(SAM.scala:352)\n",
       "\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$7$adapted(SAM.scala:350)\n",
       "\t\tat scala.Option.map(Option.scala:230)\n",
       "\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$6(SAM.scala:350)\n",
       "\t\tat scala.Option.flatMap(Option.scala:271)\n",
       "\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$1(SAM.scala:349)\n",
       "\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$1$adapted(SAM.scala:336)\n",
       "\t\tat scala.Option.foreach(Option.scala:407)\n",
       "\t\tat com.databricks.unity.SAM.setFileSystemConfigs(SAM.scala:336)\n",
       "\t\tat com.databricks.unity.SAM.createDelegate(SAM.scala:388)\n",
       "\t\tat com.databricks.unity.SAM.createDelegate$(SAM.scala:372)\n",
       "\t\tat com.databricks.unity.TableSAM.createDelegate(SAM.scala:695)\n",
       "\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.createDelegate(CredentialScopeFileSystem.scala:74)\n",
       "\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.$anonfun$setDelegates$1(CredentialScopeFileSystem.scala:127)\n",
       "\t\tat com.databricks.sql.acl.fs.Lazy.apply(DelegatingFileSystem.scala:310)\n",
       "\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.readDelegateMakeQualified(CredentialScopeFileSystem.scala:282)\n",
       "\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.optimizedMakeQualifiedPath(CredentialScopeFileSystem.scala:277)\n",
       "\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.makeQualified(CredentialScopeFileSystem.scala:294)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.apply(DeltaLog.scala:1284)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.forTable(DeltaLog.scala:1087)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$5(DeltaTableV2.scala:175)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$withAdditionalSnapshotInitializationUsageLogData$1(DeltaLog.scala:968)\n",
       "\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
       "\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
       "\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.withAdditionalSnapshotInitializationUsageLogData(DeltaLog.scala:969)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$2(DeltaTableV2.scala:169)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:700)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$1(DeltaTableV2.scala:150)\n",
       "\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:83)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.deltaLog$lzycompute(DeltaTableV2.scala:150)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.deltaLog(DeltaTableV2.scala:148)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$8(DeltaTableV2.scala:264)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$withAdditionalSnapshotInitializationUsageLogData$1(DeltaLog.scala:968)\n",
       "\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
       "\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
       "\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.withAdditionalSnapshotInitializationUsageLogData(DeltaLog.scala:969)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$7(DeltaTableV2.scala:263)\n",
       "\t\tat scala.Option.orElse(Option.scala:447)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$5(DeltaTableV2.scala:263)\n",
       "\t\tat scala.Option.orElse(Option.scala:447)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$2(DeltaTableV2.scala:260)\n",
       "\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:83)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$1(DeltaTableV2.scala:290)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:700)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot$lzycompute(DeltaTableV2.scala:240)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot(DeltaTableV2.scala:238)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$tableSchema$2(DeltaTableV2.scala:317)\n",
       "\t\tat scala.Option.getOrElse(Option.scala:189)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.tableSchema$lzycompute(DeltaTableV2.scala:317)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.tableSchema(DeltaTableV2.scala:315)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.schema(DeltaTableV2.scala:322)\n",
       "\t\tat org.apache.spark.sql.connector.catalog.Table.columns(Table.java:68)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.columns(DeltaTableV2.scala:83)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.ResolvedTable$.create(v2ResolutionPlans.scala:231)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$4(Analyzer.scala:1600)\n",
       "\t\tat scala.Option.map(Option.scala:230)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$3(Analyzer.scala:1592)\n",
       "\t\tat scala.Option.orElse(Option.scala:447)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableOrView(Analyzer.scala:1587)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1556)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1466)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:182)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:171)\n",
       "\t\tat scala.PartialFunction.applyOrElse(PartialFunction.scala:127)\n",
       "\t\tat scala.PartialFunction.applyOrElse$(PartialFunction.scala:126)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.applyOrElse(AnalysisHelper.scala:171)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)\n",
       "\t\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:138)\n",
       "\t\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1330)\n",
       "\t\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1329)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.DescribeRelation.mapChildren(v2Commands.scala:726)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning(AnalysisHelper.scala:186)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning$(AnalysisHelper.scala:167)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithSubqueriesAndPruning(LogicalPlan.scala:42)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.resolveOperatorsUpWithSubqueriesAndPruningByConf(Analyzer.scala:1674)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply0(Analyzer.scala:1466)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1419)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1365)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:480)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:629)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:613)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:131)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:480)\n",
       "\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:479)\n",
       "\t\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n",
       "\t\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n",
       "\t\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:475)\n",
       "\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:452)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22(RuleExecutor.scala:585)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22$adapted(RuleExecutor.scala:585)\n",
       "\t\tat scala.collection.immutable.List.foreach(List.scala:431)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:585)\n",
       "\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:349)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:498)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:491)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:397)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:491)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:416)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:341)\n",
       "\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:219)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:341)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:252)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:96)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:131)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:87)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:478)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:478)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:294)\n",
       "\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:548)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:668)\n",
       "\t\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:151)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:668)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1307)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:661)\n",
       "\t\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:658)\n",
       "\t\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:658)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:288)\n",
       "\t\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:287)\n",
       "\t\tat scala.util.Try$.apply(Try.scala:213)\n",
       "\t\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)\n",
       "\t\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n",
       "\t\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n",
       "\t\t... 24 more\n",
       "</td><td>2025-08-13T14:34:02.00451Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>table</td><td>`dr_eo_amperity`.`silver`.`unified_transactions`</td><td>clone</td><td>error</td><td>Failed to clone table `dr_eo_amperity`.`silver`.`unified_transactions`.. Error: [DELTA_CLONE_UNSUPPORTED_SOURCE] Unsupported DEEP clone source '_eo_amperity.silver.unified_transactions', whose format is deltasharing.\n",
       "The supported formats are 'delta', 'iceberg' and 'parquet'.</td><td>2025-08-13T14:34:00.880227Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>table/view</td><td>`accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample`.`forecast`.`us_postal_daynight_imperial`</td><td>clone/create</td><td>error</td><td>Failed to process table/view `accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample`.`forecast`.`us_postal_daynight_imperial`. Error: An error occurred while calling o427.sql.\n",
       ": com.databricks.sql.managedcatalog.acl.UnauthorizedAccessException: PERMISSION_DENIED: User does not have USE CATALOG on Catalog 'accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample'.\n",
       "\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:87)\n",
       "\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:63)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:251)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapExceptionBase(ManagedCatalogClientImpl.scala:7378)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:7364)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.getTableCredentials(ManagedCatalogClientImpl.scala:3951)\n",
       "\tat com.databricks.sql.managedcatalog.ManagedCatalogClient.getTemporaryCredentials(ManagedCatalogClient.scala:2620)\n",
       "\tat com.databricks.sql.managedcatalog.ManagedCatalogClient.getTemporaryCredentials$(ManagedCatalogClient.scala:2605)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.getTemporaryCredentials(ManagedCatalogClientImpl.scala:251)\n",
       "\tat com.databricks.unity.TempCredCache.$anonfun$getInternal$7(TemporaryCredentials.scala:448)\n",
       "\tat com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4724)\n",
       "\tat com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3522)\n",
       "\tat com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2315)\n",
       "\tat com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2278)\n",
       "\tat com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2193)\n",
       "\tat com.google.common.cache.LocalCache.get(LocalCache.java:3932)\n",
       "\tat com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4721)\n",
       "\tat com.databricks.unity.TempCredCache.liftedTree1$1(TemporaryCredentials.scala:447)\n",
       "\tat com.databricks.unity.TempCredCache.getInternal(TemporaryCredentials.scala:446)\n",
       "\tat com.databricks.unity.TempCredCache.get(TemporaryCredentials.scala:373)\n",
       "\tat com.databricks.unity.UnityCredentialManager.getTemporaryCredentials(CredentialManager.scala:488)\n",
       "\tat com.databricks.unity.CredentialManager$.getTemporaryCredentials(CredentialManager.scala:881)\n",
       "\tat com.databricks.unity.SAM.setConfFromCred$1(SAM.scala:338)\n",
       "\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$8(SAM.scala:353)\n",
       "\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$8$adapted(SAM.scala:352)\n",
       "\tat com.databricks.unity.TempCredRequest$.$anonfun$withFallbackOnOverlappingForParentPath$3(TemporaryCredentials.scala:147)\n",
       "\tat scala.util.Try$.apply(Try.scala:213)\n",
       "\tat com.databricks.unity.TempCredRequest$.withFallbackOnOverlappingForParentPath(TemporaryCredentials.scala:147)\n",
       "\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$7(SAM.scala:352)\n",
       "\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$7$adapted(SAM.scala:350)\n",
       "\tat scala.Option.map(Option.scala:230)\n",
       "\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$6(SAM.scala:350)\n",
       "\tat scala.Option.flatMap(Option.scala:271)\n",
       "\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$1(SAM.scala:349)\n",
       "\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$1$adapted(SAM.scala:336)\n",
       "\tat scala.Option.foreach(Option.scala:407)\n",
       "\tat com.databricks.unity.SAM.setFileSystemConfigs(SAM.scala:336)\n",
       "\tat com.databricks.unity.SAM.createDelegate(SAM.scala:388)\n",
       "\tat com.databricks.unity.SAM.createDelegate$(SAM.scala:372)\n",
       "\tat com.databricks.unity.TableSAM.createDelegate(SAM.scala:695)\n",
       "\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.createDelegate(CredentialScopeFileSystem.scala:74)\n",
       "\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.$anonfun$setDelegates$1(CredentialScopeFileSystem.scala:127)\n",
       "\tat com.databricks.sql.acl.fs.Lazy.apply(DelegatingFileSystem.scala:310)\n",
       "\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.readDelegateMakeQualified(CredentialScopeFileSystem.scala:282)\n",
       "\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.optimizedMakeQualifiedPath(CredentialScopeFileSystem.scala:277)\n",
       "\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.makeQualified(CredentialScopeFileSystem.scala:294)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaLog$.apply(DeltaLog.scala:1284)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaLog$.forTable(DeltaLog.scala:1087)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$5(DeltaTableV2.scala:175)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$withAdditionalSnapshotInitializationUsageLogData$1(DeltaLog.scala:968)\n",
       "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
       "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
       "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaLog$.withAdditionalSnapshotInitializationUsageLogData(DeltaLog.scala:969)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$2(DeltaTableV2.scala:169)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:700)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$1(DeltaTableV2.scala:150)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:83)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.deltaLog$lzycompute(DeltaTableV2.scala:150)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.deltaLog(DeltaTableV2.scala:148)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$8(DeltaTableV2.scala:264)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$withAdditionalSnapshotInitializationUsageLogData$1(DeltaLog.scala:968)\n",
       "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
       "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
       "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaLog$.withAdditionalSnapshotInitializationUsageLogData(DeltaLog.scala:969)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$7(DeltaTableV2.scala:263)\n",
       "\tat scala.Option.orElse(Option.scala:447)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$5(DeltaTableV2.scala:263)\n",
       "\tat scala.Option.orElse(Option.scala:447)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$2(DeltaTableV2.scala:260)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:83)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$1(DeltaTableV2.scala:290)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:700)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot$lzycompute(DeltaTableV2.scala:240)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot(DeltaTableV2.scala:238)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$tableSchema$2(DeltaTableV2.scala:317)\n",
       "\tat scala.Option.getOrElse(Option.scala:189)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.tableSchema$lzycompute(DeltaTableV2.scala:317)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.tableSchema(DeltaTableV2.scala:315)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.schema(DeltaTableV2.scala:322)\n",
       "\tat org.apache.spark.sql.connector.catalog.Table.columns(Table.java:68)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.columns(DeltaTableV2.scala:83)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.ResolvedTable$.create(v2ResolutionPlans.scala:231)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$4(Analyzer.scala:1600)\n",
       "\tat scala.Option.map(Option.scala:230)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$3(Analyzer.scala:1592)\n",
       "\tat scala.Option.orElse(Option.scala:447)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableOrView(Analyzer.scala:1587)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1556)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1466)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:182)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:171)\n",
       "\tat scala.PartialFunction.applyOrElse(PartialFunction.scala:127)\n",
       "\tat scala.PartialFunction.applyOrElse$(PartialFunction.scala:126)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.applyOrElse(AnalysisHelper.scala:171)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)\n",
       "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:138)\n",
       "\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1330)\n",
       "\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1329)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.DescribeRelation.mapChildren(v2Commands.scala:726)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning(AnalysisHelper.scala:186)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning$(AnalysisHelper.scala:167)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithSubqueriesAndPruning(LogicalPlan.scala:42)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.resolveOperatorsUpWithSubqueriesAndPruningByConf(Analyzer.scala:1674)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply0(Analyzer.scala:1466)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1419)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1365)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:480)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:629)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:613)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:131)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:480)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:479)\n",
       "\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n",
       "\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n",
       "\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:475)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:452)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22(RuleExecutor.scala:585)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22$adapted(RuleExecutor.scala:585)\n",
       "\tat scala.collection.immutable.List.foreach(List.scala:431)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:585)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:349)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:498)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:491)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:397)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:491)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:416)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:341)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:219)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:341)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:252)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:96)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:131)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:87)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:478)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:478)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:294)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:548)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:668)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:151)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:668)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1307)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:661)\n",
       "\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:658)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:658)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:288)\n",
       "\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:287)\n",
       "\tat scala.util.Try$.apply(Try.scala:213)\n",
       "\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)\n",
       "\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)\n",
       "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:321)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:267)\n",
       "\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$3(Dataset.scala:149)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n",
       "\tat org.apache.spark.sql.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1469)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1469)\n",
       "\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:141)\n",
       "\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:1048)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n",
       "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:1001)\n",
       "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:1071)\n",
       "\tat jdk.internal.reflect.GeneratedMethodAccessor593.invoke(Unknown Source)\n",
       "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
       "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
       "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n",
       "\tat py4j.Gateway.invoke(Gateway.java:306)\n",
       "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
       "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:197)\n",
       "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:117)\n",
       "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
       "\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\n",
       "\t\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:87)\n",
       "\t\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:63)\n",
       "\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:251)\n",
       "\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapExceptionBase(ManagedCatalogClientImpl.scala:7378)\n",
       "\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:7364)\n",
       "\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.getTableCredentials(ManagedCatalogClientImpl.scala:3951)\n",
       "\t\tat com.databricks.sql.managedcatalog.ManagedCatalogClient.getTemporaryCredentials(ManagedCatalogClient.scala:2620)\n",
       "\t\tat com.databricks.sql.managedcatalog.ManagedCatalogClient.getTemporaryCredentials$(ManagedCatalogClient.scala:2605)\n",
       "\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.getTemporaryCredentials(ManagedCatalogClientImpl.scala:251)\n",
       "\t\tat com.databricks.unity.TempCredCache.$anonfun$getInternal$7(TemporaryCredentials.scala:448)\n",
       "\t\tat com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4724)\n",
       "\t\tat com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3522)\n",
       "\t\tat com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2315)\n",
       "\t\tat com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2278)\n",
       "\t\tat com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2193)\n",
       "\t\tat com.google.common.cache.LocalCache.get(LocalCache.java:3932)\n",
       "\t\tat com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4721)\n",
       "\t\tat com.databricks.unity.TempCredCache.liftedTree1$1(TemporaryCredentials.scala:447)\n",
       "\t\tat com.databricks.unity.TempCredCache.getInternal(TemporaryCredentials.scala:446)\n",
       "\t\tat com.databricks.unity.TempCredCache.get(TemporaryCredentials.scala:373)\n",
       "\t\tat com.databricks.unity.UnityCredentialManager.getTemporaryCredentials(CredentialManager.scala:488)\n",
       "\t\tat com.databricks.unity.CredentialManager$.getTemporaryCredentials(CredentialManager.scala:881)\n",
       "\t\tat com.databricks.unity.SAM.setConfFromCred$1(SAM.scala:338)\n",
       "\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$8(SAM.scala:353)\n",
       "\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$8$adapted(SAM.scala:352)\n",
       "\t\tat com.databricks.unity.TempCredRequest$.$anonfun$withFallbackOnOverlappingForParentPath$3(TemporaryCredentials.scala:147)\n",
       "\t\tat scala.util.Try$.apply(Try.scala:213)\n",
       "\t\tat com.databricks.unity.TempCredRequest$.withFallbackOnOverlappingForParentPath(TemporaryCredentials.scala:147)\n",
       "\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$7(SAM.scala:352)\n",
       "\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$7$adapted(SAM.scala:350)\n",
       "\t\tat scala.Option.map(Option.scala:230)\n",
       "\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$6(SAM.scala:350)\n",
       "\t\tat scala.Option.flatMap(Option.scala:271)\n",
       "\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$1(SAM.scala:349)\n",
       "\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$1$adapted(SAM.scala:336)\n",
       "\t\tat scala.Option.foreach(Option.scala:407)\n",
       "\t\tat com.databricks.unity.SAM.setFileSystemConfigs(SAM.scala:336)\n",
       "\t\tat com.databricks.unity.SAM.createDelegate(SAM.scala:388)\n",
       "\t\tat com.databricks.unity.SAM.createDelegate$(SAM.scala:372)\n",
       "\t\tat com.databricks.unity.TableSAM.createDelegate(SAM.scala:695)\n",
       "\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.createDelegate(CredentialScopeFileSystem.scala:74)\n",
       "\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.$anonfun$setDelegates$1(CredentialScopeFileSystem.scala:127)\n",
       "\t\tat com.databricks.sql.acl.fs.Lazy.apply(DelegatingFileSystem.scala:310)\n",
       "\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.readDelegateMakeQualified(CredentialScopeFileSystem.scala:282)\n",
       "\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.optimizedMakeQualifiedPath(CredentialScopeFileSystem.scala:277)\n",
       "\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.makeQualified(CredentialScopeFileSystem.scala:294)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.apply(DeltaLog.scala:1284)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.forTable(DeltaLog.scala:1087)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$5(DeltaTableV2.scala:175)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$withAdditionalSnapshotInitializationUsageLogData$1(DeltaLog.scala:968)\n",
       "\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
       "\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
       "\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.withAdditionalSnapshotInitializationUsageLogData(DeltaLog.scala:969)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$2(DeltaTableV2.scala:169)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:700)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$1(DeltaTableV2.scala:150)\n",
       "\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:83)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.deltaLog$lzycompute(DeltaTableV2.scala:150)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.deltaLog(DeltaTableV2.scala:148)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$8(DeltaTableV2.scala:264)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$withAdditionalSnapshotInitializationUsageLogData$1(DeltaLog.scala:968)\n",
       "\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
       "\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
       "\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.withAdditionalSnapshotInitializationUsageLogData(DeltaLog.scala:969)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$7(DeltaTableV2.scala:263)\n",
       "\t\tat scala.Option.orElse(Option.scala:447)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$5(DeltaTableV2.scala:263)\n",
       "\t\tat scala.Option.orElse(Option.scala:447)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$2(DeltaTableV2.scala:260)\n",
       "\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:83)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$1(DeltaTableV2.scala:290)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:700)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot$lzycompute(DeltaTableV2.scala:240)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot(DeltaTableV2.scala:238)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$tableSchema$2(DeltaTableV2.scala:317)\n",
       "\t\tat scala.Option.getOrElse(Option.scala:189)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.tableSchema$lzycompute(DeltaTableV2.scala:317)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.tableSchema(DeltaTableV2.scala:315)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.schema(DeltaTableV2.scala:322)\n",
       "\t\tat org.apache.spark.sql.connector.catalog.Table.columns(Table.java:68)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.columns(DeltaTableV2.scala:83)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.ResolvedTable$.create(v2ResolutionPlans.scala:231)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$4(Analyzer.scala:1600)\n",
       "\t\tat scala.Option.map(Option.scala:230)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$3(Analyzer.scala:1592)\n",
       "\t\tat scala.Option.orElse(Option.scala:447)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableOrView(Analyzer.scala:1587)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1556)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1466)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:182)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:171)\n",
       "\t\tat scala.PartialFunction.applyOrElse(PartialFunction.scala:127)\n",
       "\t\tat scala.PartialFunction.applyOrElse$(PartialFunction.scala:126)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.applyOrElse(AnalysisHelper.scala:171)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)\n",
       "\t\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:138)\n",
       "\t\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1330)\n",
       "\t\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1329)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.DescribeRelation.mapChildren(v2Commands.scala:726)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning(AnalysisHelper.scala:186)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning$(AnalysisHelper.scala:167)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithSubqueriesAndPruning(LogicalPlan.scala:42)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.resolveOperatorsUpWithSubqueriesAndPruningByConf(Analyzer.scala:1674)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply0(Analyzer.scala:1466)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1419)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1365)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:480)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:629)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:613)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:131)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:480)\n",
       "\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:479)\n",
       "\t\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n",
       "\t\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n",
       "\t\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:475)\n",
       "\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:452)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22(RuleExecutor.scala:585)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22$adapted(RuleExecutor.scala:585)\n",
       "\t\tat scala.collection.immutable.List.foreach(List.scala:431)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:585)\n",
       "\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:349)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:498)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:491)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:397)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:491)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:416)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:341)\n",
       "\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:219)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:341)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:252)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:96)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:131)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:87)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:478)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:478)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:294)\n",
       "\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:548)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:668)\n",
       "\t\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:151)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:668)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1307)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:661)\n",
       "\t\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:658)\n",
       "\t\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:658)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:288)\n",
       "\t\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:287)\n",
       "\t\tat scala.util.Try$.apply(Try.scala:213)\n",
       "\t\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)\n",
       "\t\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n",
       "\t\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n",
       "\t\t... 24 more\n",
       "</td><td>2025-08-13T14:34:00.796513Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>table/view</td><td>`accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample`.`forecast`.`us_postal_daily_metric`</td><td>clone/create</td><td>error</td><td>Failed to process table/view `accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample`.`forecast`.`us_postal_daily_metric`. Error: An error occurred while calling o427.sql.\n",
       ": com.databricks.sql.managedcatalog.acl.UnauthorizedAccessException: PERMISSION_DENIED: User does not have USE CATALOG on Catalog 'accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample'.\n",
       "\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:87)\n",
       "\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:63)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:251)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapExceptionBase(ManagedCatalogClientImpl.scala:7378)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:7364)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.getTableCredentials(ManagedCatalogClientImpl.scala:3951)\n",
       "\tat com.databricks.sql.managedcatalog.ManagedCatalogClient.getTemporaryCredentials(ManagedCatalogClient.scala:2620)\n",
       "\tat com.databricks.sql.managedcatalog.ManagedCatalogClient.getTemporaryCredentials$(ManagedCatalogClient.scala:2605)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.getTemporaryCredentials(ManagedCatalogClientImpl.scala:251)\n",
       "\tat com.databricks.unity.TempCredCache.$anonfun$getInternal$7(TemporaryCredentials.scala:448)\n",
       "\tat com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4724)\n",
       "\tat com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3522)\n",
       "\tat com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2315)\n",
       "\tat com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2278)\n",
       "\tat com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2193)\n",
       "\tat com.google.common.cache.LocalCache.get(LocalCache.java:3932)\n",
       "\tat com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4721)\n",
       "\tat com.databricks.unity.TempCredCache.liftedTree1$1(TemporaryCredentials.scala:447)\n",
       "\tat com.databricks.unity.TempCredCache.getInternal(TemporaryCredentials.scala:446)\n",
       "\tat com.databricks.unity.TempCredCache.get(TemporaryCredentials.scala:373)\n",
       "\tat com.databricks.unity.UnityCredentialManager.getTemporaryCredentials(CredentialManager.scala:488)\n",
       "\tat com.databricks.unity.CredentialManager$.getTemporaryCredentials(CredentialManager.scala:881)\n",
       "\tat com.databricks.unity.SAM.setConfFromCred$1(SAM.scala:338)\n",
       "\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$8(SAM.scala:353)\n",
       "\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$8$adapted(SAM.scala:352)\n",
       "\tat com.databricks.unity.TempCredRequest$.$anonfun$withFallbackOnOverlappingForParentPath$3(TemporaryCredentials.scala:147)\n",
       "\tat scala.util.Try$.apply(Try.scala:213)\n",
       "\tat com.databricks.unity.TempCredRequest$.withFallbackOnOverlappingForParentPath(TemporaryCredentials.scala:147)\n",
       "\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$7(SAM.scala:352)\n",
       "\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$7$adapted(SAM.scala:350)\n",
       "\tat scala.Option.map(Option.scala:230)\n",
       "\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$6(SAM.scala:350)\n",
       "\tat scala.Option.flatMap(Option.scala:271)\n",
       "\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$1(SAM.scala:349)\n",
       "\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$1$adapted(SAM.scala:336)\n",
       "\tat scala.Option.foreach(Option.scala:407)\n",
       "\tat com.databricks.unity.SAM.setFileSystemConfigs(SAM.scala:336)\n",
       "\tat com.databricks.unity.SAM.createDelegate(SAM.scala:388)\n",
       "\tat com.databricks.unity.SAM.createDelegate$(SAM.scala:372)\n",
       "\tat com.databricks.unity.TableSAM.createDelegate(SAM.scala:695)\n",
       "\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.createDelegate(CredentialScopeFileSystem.scala:74)\n",
       "\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.$anonfun$setDelegates$1(CredentialScopeFileSystem.scala:127)\n",
       "\tat com.databricks.sql.acl.fs.Lazy.apply(DelegatingFileSystem.scala:310)\n",
       "\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.readDelegateMakeQualified(CredentialScopeFileSystem.scala:282)\n",
       "\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.optimizedMakeQualifiedPath(CredentialScopeFileSystem.scala:277)\n",
       "\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.makeQualified(CredentialScopeFileSystem.scala:294)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaLog$.apply(DeltaLog.scala:1284)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaLog$.forTable(DeltaLog.scala:1087)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$5(DeltaTableV2.scala:175)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$withAdditionalSnapshotInitializationUsageLogData$1(DeltaLog.scala:968)\n",
       "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
       "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
       "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaLog$.withAdditionalSnapshotInitializationUsageLogData(DeltaLog.scala:969)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$2(DeltaTableV2.scala:169)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:700)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$1(DeltaTableV2.scala:150)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:83)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.deltaLog$lzycompute(DeltaTableV2.scala:150)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.deltaLog(DeltaTableV2.scala:148)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$8(DeltaTableV2.scala:264)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$withAdditionalSnapshotInitializationUsageLogData$1(DeltaLog.scala:968)\n",
       "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
       "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
       "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaLog$.withAdditionalSnapshotInitializationUsageLogData(DeltaLog.scala:969)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$7(DeltaTableV2.scala:263)\n",
       "\tat scala.Option.orElse(Option.scala:447)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$5(DeltaTableV2.scala:263)\n",
       "\tat scala.Option.orElse(Option.scala:447)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$2(DeltaTableV2.scala:260)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:83)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$1(DeltaTableV2.scala:290)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:700)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot$lzycompute(DeltaTableV2.scala:240)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot(DeltaTableV2.scala:238)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$tableSchema$2(DeltaTableV2.scala:317)\n",
       "\tat scala.Option.getOrElse(Option.scala:189)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.tableSchema$lzycompute(DeltaTableV2.scala:317)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.tableSchema(DeltaTableV2.scala:315)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.schema(DeltaTableV2.scala:322)\n",
       "\tat org.apache.spark.sql.connector.catalog.Table.columns(Table.java:68)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.columns(DeltaTableV2.scala:83)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.ResolvedTable$.create(v2ResolutionPlans.scala:231)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$4(Analyzer.scala:1600)\n",
       "\tat scala.Option.map(Option.scala:230)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$3(Analyzer.scala:1592)\n",
       "\tat scala.Option.orElse(Option.scala:447)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableOrView(Analyzer.scala:1587)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1556)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1466)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:182)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:171)\n",
       "\tat scala.PartialFunction.applyOrElse(PartialFunction.scala:127)\n",
       "\tat scala.PartialFunction.applyOrElse$(PartialFunction.scala:126)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.applyOrElse(AnalysisHelper.scala:171)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)\n",
       "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:138)\n",
       "\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1330)\n",
       "\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1329)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.DescribeRelation.mapChildren(v2Commands.scala:726)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning(AnalysisHelper.scala:186)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning$(AnalysisHelper.scala:167)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithSubqueriesAndPruning(LogicalPlan.scala:42)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.resolveOperatorsUpWithSubqueriesAndPruningByConf(Analyzer.scala:1674)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply0(Analyzer.scala:1466)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1419)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1365)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:480)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:629)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:613)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:131)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:480)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:479)\n",
       "\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n",
       "\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n",
       "\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:475)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:452)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22(RuleExecutor.scala:585)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22$adapted(RuleExecutor.scala:585)\n",
       "\tat scala.collection.immutable.List.foreach(List.scala:431)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:585)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:349)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:498)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:491)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:397)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:491)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:416)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:341)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:219)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:341)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:252)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:96)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:131)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:87)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:478)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:478)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:294)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:548)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:668)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:151)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:668)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1307)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:661)\n",
       "\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:658)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:658)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:288)\n",
       "\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:287)\n",
       "\tat scala.util.Try$.apply(Try.scala:213)\n",
       "\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)\n",
       "\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)\n",
       "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:321)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:267)\n",
       "\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$3(Dataset.scala:149)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n",
       "\tat org.apache.spark.sql.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1469)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1469)\n",
       "\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:141)\n",
       "\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:1048)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n",
       "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:1001)\n",
       "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:1071)\n",
       "\tat jdk.internal.reflect.GeneratedMethodAccessor593.invoke(Unknown Source)\n",
       "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
       "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
       "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n",
       "\tat py4j.Gateway.invoke(Gateway.java:306)\n",
       "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
       "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:197)\n",
       "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:117)\n",
       "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
       "\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\n",
       "\t\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:87)\n",
       "\t\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:63)\n",
       "\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:251)\n",
       "\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapExceptionBase(ManagedCatalogClientImpl.scala:7378)\n",
       "\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:7364)\n",
       "\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.getTableCredentials(ManagedCatalogClientImpl.scala:3951)\n",
       "\t\tat com.databricks.sql.managedcatalog.ManagedCatalogClient.getTemporaryCredentials(ManagedCatalogClient.scala:2620)\n",
       "\t\tat com.databricks.sql.managedcatalog.ManagedCatalogClient.getTemporaryCredentials$(ManagedCatalogClient.scala:2605)\n",
       "\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.getTemporaryCredentials(ManagedCatalogClientImpl.scala:251)\n",
       "\t\tat com.databricks.unity.TempCredCache.$anonfun$getInternal$7(TemporaryCredentials.scala:448)\n",
       "\t\tat com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4724)\n",
       "\t\tat com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3522)\n",
       "\t\tat com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2315)\n",
       "\t\tat com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2278)\n",
       "\t\tat com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2193)\n",
       "\t\tat com.google.common.cache.LocalCache.get(LocalCache.java:3932)\n",
       "\t\tat com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4721)\n",
       "\t\tat com.databricks.unity.TempCredCache.liftedTree1$1(TemporaryCredentials.scala:447)\n",
       "\t\tat com.databricks.unity.TempCredCache.getInternal(TemporaryCredentials.scala:446)\n",
       "\t\tat com.databricks.unity.TempCredCache.get(TemporaryCredentials.scala:373)\n",
       "\t\tat com.databricks.unity.UnityCredentialManager.getTemporaryCredentials(CredentialManager.scala:488)\n",
       "\t\tat com.databricks.unity.CredentialManager$.getTemporaryCredentials(CredentialManager.scala:881)\n",
       "\t\tat com.databricks.unity.SAM.setConfFromCred$1(SAM.scala:338)\n",
       "\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$8(SAM.scala:353)\n",
       "\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$8$adapted(SAM.scala:352)\n",
       "\t\tat com.databricks.unity.TempCredRequest$.$anonfun$withFallbackOnOverlappingForParentPath$3(TemporaryCredentials.scala:147)\n",
       "\t\tat scala.util.Try$.apply(Try.scala:213)\n",
       "\t\tat com.databricks.unity.TempCredRequest$.withFallbackOnOverlappingForParentPath(TemporaryCredentials.scala:147)\n",
       "\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$7(SAM.scala:352)\n",
       "\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$7$adapted(SAM.scala:350)\n",
       "\t\tat scala.Option.map(Option.scala:230)\n",
       "\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$6(SAM.scala:350)\n",
       "\t\tat scala.Option.flatMap(Option.scala:271)\n",
       "\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$1(SAM.scala:349)\n",
       "\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$1$adapted(SAM.scala:336)\n",
       "\t\tat scala.Option.foreach(Option.scala:407)\n",
       "\t\tat com.databricks.unity.SAM.setFileSystemConfigs(SAM.scala:336)\n",
       "\t\tat com.databricks.unity.SAM.createDelegate(SAM.scala:388)\n",
       "\t\tat com.databricks.unity.SAM.createDelegate$(SAM.scala:372)\n",
       "\t\tat com.databricks.unity.TableSAM.createDelegate(SAM.scala:695)\n",
       "\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.createDelegate(CredentialScopeFileSystem.scala:74)\n",
       "\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.$anonfun$setDelegates$1(CredentialScopeFileSystem.scala:127)\n",
       "\t\tat com.databricks.sql.acl.fs.Lazy.apply(DelegatingFileSystem.scala:310)\n",
       "\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.readDelegateMakeQualified(CredentialScopeFileSystem.scala:282)\n",
       "\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.optimizedMakeQualifiedPath(CredentialScopeFileSystem.scala:277)\n",
       "\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.makeQualified(CredentialScopeFileSystem.scala:294)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.apply(DeltaLog.scala:1284)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.forTable(DeltaLog.scala:1087)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$5(DeltaTableV2.scala:175)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$withAdditionalSnapshotInitializationUsageLogData$1(DeltaLog.scala:968)\n",
       "\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
       "\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
       "\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.withAdditionalSnapshotInitializationUsageLogData(DeltaLog.scala:969)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$2(DeltaTableV2.scala:169)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:700)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$1(DeltaTableV2.scala:150)\n",
       "\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:83)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.deltaLog$lzycompute(DeltaTableV2.scala:150)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.deltaLog(DeltaTableV2.scala:148)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$8(DeltaTableV2.scala:264)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$withAdditionalSnapshotInitializationUsageLogData$1(DeltaLog.scala:968)\n",
       "\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
       "\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
       "\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.withAdditionalSnapshotInitializationUsageLogData(DeltaLog.scala:969)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$7(DeltaTableV2.scala:263)\n",
       "\t\tat scala.Option.orElse(Option.scala:447)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$5(DeltaTableV2.scala:263)\n",
       "\t\tat scala.Option.orElse(Option.scala:447)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$2(DeltaTableV2.scala:260)\n",
       "\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:83)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$1(DeltaTableV2.scala:290)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:700)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot$lzycompute(DeltaTableV2.scala:240)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot(DeltaTableV2.scala:238)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$tableSchema$2(DeltaTableV2.scala:317)\n",
       "\t\tat scala.Option.getOrElse(Option.scala:189)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.tableSchema$lzycompute(DeltaTableV2.scala:317)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.tableSchema(DeltaTableV2.scala:315)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.schema(DeltaTableV2.scala:322)\n",
       "\t\tat org.apache.spark.sql.connector.catalog.Table.columns(Table.java:68)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.columns(DeltaTableV2.scala:83)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.ResolvedTable$.create(v2ResolutionPlans.scala:231)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$4(Analyzer.scala:1600)\n",
       "\t\tat scala.Option.map(Option.scala:230)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$3(Analyzer.scala:1592)\n",
       "\t\tat scala.Option.orElse(Option.scala:447)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableOrView(Analyzer.scala:1587)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1556)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1466)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:182)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:171)\n",
       "\t\tat scala.PartialFunction.applyOrElse(PartialFunction.scala:127)\n",
       "\t\tat scala.PartialFunction.applyOrElse$(PartialFunction.scala:126)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.applyOrElse(AnalysisHelper.scala:171)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)\n",
       "\t\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:138)\n",
       "\t\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1330)\n",
       "\t\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1329)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.DescribeRelation.mapChildren(v2Commands.scala:726)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning(AnalysisHelper.scala:186)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning$(AnalysisHelper.scala:167)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithSubqueriesAndPruning(LogicalPlan.scala:42)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.resolveOperatorsUpWithSubqueriesAndPruningByConf(Analyzer.scala:1674)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply0(Analyzer.scala:1466)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1419)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1365)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:480)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:629)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:613)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:131)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:480)\n",
       "\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:479)\n",
       "\t\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n",
       "\t\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n",
       "\t\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:475)\n",
       "\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:452)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22(RuleExecutor.scala:585)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22$adapted(RuleExecutor.scala:585)\n",
       "\t\tat scala.collection.immutable.List.foreach(List.scala:431)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:585)\n",
       "\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:349)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:498)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:491)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:397)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:491)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:416)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:341)\n",
       "\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:219)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:341)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:252)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:96)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:131)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:87)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:478)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:478)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:294)\n",
       "\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:548)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:668)\n",
       "\t\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:151)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:668)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1307)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:661)\n",
       "\t\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:658)\n",
       "\t\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:658)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:288)\n",
       "\t\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:287)\n",
       "\t\tat scala.util.Try$.apply(Try.scala:213)\n",
       "\t\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)\n",
       "\t\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n",
       "\t\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n",
       "\t\t... 24 more\n",
       "</td><td>2025-08-13T14:34:00.416501Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>table/view</td><td>`accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample`.`forecast`.`us_postal_daily_imperial`</td><td>clone/create</td><td>error</td><td>Failed to process table/view `accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample`.`forecast`.`us_postal_daily_imperial`. Error: An error occurred while calling o427.sql.\n",
       ": com.databricks.sql.managedcatalog.acl.UnauthorizedAccessException: PERMISSION_DENIED: User does not have USE CATALOG on Catalog 'accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample'.\n",
       "\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:87)\n",
       "\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:63)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:251)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapExceptionBase(ManagedCatalogClientImpl.scala:7378)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:7364)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.getTableCredentials(ManagedCatalogClientImpl.scala:3951)\n",
       "\tat com.databricks.sql.managedcatalog.ManagedCatalogClient.getTemporaryCredentials(ManagedCatalogClient.scala:2620)\n",
       "\tat com.databricks.sql.managedcatalog.ManagedCatalogClient.getTemporaryCredentials$(ManagedCatalogClient.scala:2605)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.getTemporaryCredentials(ManagedCatalogClientImpl.scala:251)\n",
       "\tat com.databricks.unity.TempCredCache.$anonfun$getInternal$7(TemporaryCredentials.scala:448)\n",
       "\tat com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4724)\n",
       "\tat com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3522)\n",
       "\tat com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2315)\n",
       "\tat com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2278)\n",
       "\tat com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2193)\n",
       "\tat com.google.common.cache.LocalCache.get(LocalCache.java:3932)\n",
       "\tat com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4721)\n",
       "\tat com.databricks.unity.TempCredCache.liftedTree1$1(TemporaryCredentials.scala:447)\n",
       "\tat com.databricks.unity.TempCredCache.getInternal(TemporaryCredentials.scala:446)\n",
       "\tat com.databricks.unity.TempCredCache.get(TemporaryCredentials.scala:373)\n",
       "\tat com.databricks.unity.UnityCredentialManager.getTemporaryCredentials(CredentialManager.scala:488)\n",
       "\tat com.databricks.unity.CredentialManager$.getTemporaryCredentials(CredentialManager.scala:881)\n",
       "\tat com.databricks.unity.SAM.setConfFromCred$1(SAM.scala:338)\n",
       "\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$8(SAM.scala:353)\n",
       "\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$8$adapted(SAM.scala:352)\n",
       "\tat com.databricks.unity.TempCredRequest$.$anonfun$withFallbackOnOverlappingForParentPath$3(TemporaryCredentials.scala:147)\n",
       "\tat scala.util.Try$.apply(Try.scala:213)\n",
       "\tat com.databricks.unity.TempCredRequest$.withFallbackOnOverlappingForParentPath(TemporaryCredentials.scala:147)\n",
       "\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$7(SAM.scala:352)\n",
       "\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$7$adapted(SAM.scala:350)\n",
       "\tat scala.Option.map(Option.scala:230)\n",
       "\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$6(SAM.scala:350)\n",
       "\tat scala.Option.flatMap(Option.scala:271)\n",
       "\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$1(SAM.scala:349)\n",
       "\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$1$adapted(SAM.scala:336)\n",
       "\tat scala.Option.foreach(Option.scala:407)\n",
       "\tat com.databricks.unity.SAM.setFileSystemConfigs(SAM.scala:336)\n",
       "\tat com.databricks.unity.SAM.createDelegate(SAM.scala:388)\n",
       "\tat com.databricks.unity.SAM.createDelegate$(SAM.scala:372)\n",
       "\tat com.databricks.unity.TableSAM.createDelegate(SAM.scala:695)\n",
       "\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.createDelegate(CredentialScopeFileSystem.scala:74)\n",
       "\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.$anonfun$setDelegates$1(CredentialScopeFileSystem.scala:127)\n",
       "\tat com.databricks.sql.acl.fs.Lazy.apply(DelegatingFileSystem.scala:310)\n",
       "\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.readDelegateMakeQualified(CredentialScopeFileSystem.scala:282)\n",
       "\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.optimizedMakeQualifiedPath(CredentialScopeFileSystem.scala:277)\n",
       "\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.makeQualified(CredentialScopeFileSystem.scala:294)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaLog$.apply(DeltaLog.scala:1284)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaLog$.forTable(DeltaLog.scala:1087)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$5(DeltaTableV2.scala:175)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$withAdditionalSnapshotInitializationUsageLogData$1(DeltaLog.scala:968)\n",
       "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
       "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
       "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaLog$.withAdditionalSnapshotInitializationUsageLogData(DeltaLog.scala:969)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$2(DeltaTableV2.scala:169)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:700)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$1(DeltaTableV2.scala:150)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:83)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.deltaLog$lzycompute(DeltaTableV2.scala:150)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.deltaLog(DeltaTableV2.scala:148)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$8(DeltaTableV2.scala:264)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$withAdditionalSnapshotInitializationUsageLogData$1(DeltaLog.scala:968)\n",
       "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
       "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
       "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaLog$.withAdditionalSnapshotInitializationUsageLogData(DeltaLog.scala:969)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$7(DeltaTableV2.scala:263)\n",
       "\tat scala.Option.orElse(Option.scala:447)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$5(DeltaTableV2.scala:263)\n",
       "\tat scala.Option.orElse(Option.scala:447)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$2(DeltaTableV2.scala:260)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:83)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$1(DeltaTableV2.scala:290)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:700)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot$lzycompute(DeltaTableV2.scala:240)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot(DeltaTableV2.scala:238)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$tableSchema$2(DeltaTableV2.scala:317)\n",
       "\tat scala.Option.getOrElse(Option.scala:189)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.tableSchema$lzycompute(DeltaTableV2.scala:317)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.tableSchema(DeltaTableV2.scala:315)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.schema(DeltaTableV2.scala:322)\n",
       "\tat org.apache.spark.sql.connector.catalog.Table.columns(Table.java:68)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.columns(DeltaTableV2.scala:83)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.ResolvedTable$.create(v2ResolutionPlans.scala:231)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$4(Analyzer.scala:1600)\n",
       "\tat scala.Option.map(Option.scala:230)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$3(Analyzer.scala:1592)\n",
       "\tat scala.Option.orElse(Option.scala:447)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableOrView(Analyzer.scala:1587)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1556)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1466)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:182)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:171)\n",
       "\tat scala.PartialFunction.applyOrElse(PartialFunction.scala:127)\n",
       "\tat scala.PartialFunction.applyOrElse$(PartialFunction.scala:126)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.applyOrElse(AnalysisHelper.scala:171)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)\n",
       "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:138)\n",
       "\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1330)\n",
       "\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1329)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.DescribeRelation.mapChildren(v2Commands.scala:726)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning(AnalysisHelper.scala:186)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning$(AnalysisHelper.scala:167)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithSubqueriesAndPruning(LogicalPlan.scala:42)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.resolveOperatorsUpWithSubqueriesAndPruningByConf(Analyzer.scala:1674)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply0(Analyzer.scala:1466)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1419)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1365)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:480)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:629)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:613)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:131)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:480)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:479)\n",
       "\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n",
       "\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n",
       "\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:475)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:452)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22(RuleExecutor.scala:585)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22$adapted(RuleExecutor.scala:585)\n",
       "\tat scala.collection.immutable.List.foreach(List.scala:431)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:585)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:349)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:498)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:491)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:397)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:491)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:416)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:341)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:219)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:341)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:252)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:96)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:131)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:87)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:478)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:478)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:294)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:548)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:668)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:151)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:668)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1307)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:661)\n",
       "\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:658)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:658)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:288)\n",
       "\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:287)\n",
       "\tat scala.util.Try$.apply(Try.scala:213)\n",
       "\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)\n",
       "\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)\n",
       "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:321)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:267)\n",
       "\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$3(Dataset.scala:149)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n",
       "\tat org.apache.spark.sql.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1469)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1469)\n",
       "\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:141)\n",
       "\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:1048)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n",
       "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:1001)\n",
       "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:1071)\n",
       "\tat jdk.internal.reflect.GeneratedMethodAccessor593.invoke(Unknown Source)\n",
       "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
       "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
       "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n",
       "\tat py4j.Gateway.invoke(Gateway.java:306)\n",
       "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
       "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:197)\n",
       "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:117)\n",
       "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
       "\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\n",
       "\t\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:87)\n",
       "\t\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:63)\n",
       "\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:251)\n",
       "\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapExceptionBase(ManagedCatalogClientImpl.scala:7378)\n",
       "\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:7364)\n",
       "\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.getTableCredentials(ManagedCatalogClientImpl.scala:3951)\n",
       "\t\tat com.databricks.sql.managedcatalog.ManagedCatalogClient.getTemporaryCredentials(ManagedCatalogClient.scala:2620)\n",
       "\t\tat com.databricks.sql.managedcatalog.ManagedCatalogClient.getTemporaryCredentials$(ManagedCatalogClient.scala:2605)\n",
       "\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.getTemporaryCredentials(ManagedCatalogClientImpl.scala:251)\n",
       "\t\tat com.databricks.unity.TempCredCache.$anonfun$getInternal$7(TemporaryCredentials.scala:448)\n",
       "\t\tat com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4724)\n",
       "\t\tat com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3522)\n",
       "\t\tat com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2315)\n",
       "\t\tat com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2278)\n",
       "\t\tat com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2193)\n",
       "\t\tat com.google.common.cache.LocalCache.get(LocalCache.java:3932)\n",
       "\t\tat com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4721)\n",
       "\t\tat com.databricks.unity.TempCredCache.liftedTree1$1(TemporaryCredentials.scala:447)\n",
       "\t\tat com.databricks.unity.TempCredCache.getInternal(TemporaryCredentials.scala:446)\n",
       "\t\tat com.databricks.unity.TempCredCache.get(TemporaryCredentials.scala:373)\n",
       "\t\tat com.databricks.unity.UnityCredentialManager.getTemporaryCredentials(CredentialManager.scala:488)\n",
       "\t\tat com.databricks.unity.CredentialManager$.getTemporaryCredentials(CredentialManager.scala:881)\n",
       "\t\tat com.databricks.unity.SAM.setConfFromCred$1(SAM.scala:338)\n",
       "\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$8(SAM.scala:353)\n",
       "\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$8$adapted(SAM.scala:352)\n",
       "\t\tat com.databricks.unity.TempCredRequest$.$anonfun$withFallbackOnOverlappingForParentPath$3(TemporaryCredentials.scala:147)\n",
       "\t\tat scala.util.Try$.apply(Try.scala:213)\n",
       "\t\tat com.databricks.unity.TempCredRequest$.withFallbackOnOverlappingForParentPath(TemporaryCredentials.scala:147)\n",
       "\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$7(SAM.scala:352)\n",
       "\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$7$adapted(SAM.scala:350)\n",
       "\t\tat scala.Option.map(Option.scala:230)\n",
       "\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$6(SAM.scala:350)\n",
       "\t\tat scala.Option.flatMap(Option.scala:271)\n",
       "\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$1(SAM.scala:349)\n",
       "\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$1$adapted(SAM.scala:336)\n",
       "\t\tat scala.Option.foreach(Option.scala:407)\n",
       "\t\tat com.databricks.unity.SAM.setFileSystemConfigs(SAM.scala:336)\n",
       "\t\tat com.databricks.unity.SAM.createDelegate(SAM.scala:388)\n",
       "\t\tat com.databricks.unity.SAM.createDelegate$(SAM.scala:372)\n",
       "\t\tat com.databricks.unity.TableSAM.createDelegate(SAM.scala:695)\n",
       "\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.createDelegate(CredentialScopeFileSystem.scala:74)\n",
       "\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.$anonfun$setDelegates$1(CredentialScopeFileSystem.scala:127)\n",
       "\t\tat com.databricks.sql.acl.fs.Lazy.apply(DelegatingFileSystem.scala:310)\n",
       "\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.readDelegateMakeQualified(CredentialScopeFileSystem.scala:282)\n",
       "\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.optimizedMakeQualifiedPath(CredentialScopeFileSystem.scala:277)\n",
       "\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.makeQualified(CredentialScopeFileSystem.scala:294)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.apply(DeltaLog.scala:1284)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.forTable(DeltaLog.scala:1087)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$5(DeltaTableV2.scala:175)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$withAdditionalSnapshotInitializationUsageLogData$1(DeltaLog.scala:968)\n",
       "\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
       "\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
       "\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.withAdditionalSnapshotInitializationUsageLogData(DeltaLog.scala:969)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$2(DeltaTableV2.scala:169)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:700)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$1(DeltaTableV2.scala:150)\n",
       "\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:83)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.deltaLog$lzycompute(DeltaTableV2.scala:150)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.deltaLog(DeltaTableV2.scala:148)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$8(DeltaTableV2.scala:264)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$withAdditionalSnapshotInitializationUsageLogData$1(DeltaLog.scala:968)\n",
       "\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
       "\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
       "\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.withAdditionalSnapshotInitializationUsageLogData(DeltaLog.scala:969)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$7(DeltaTableV2.scala:263)\n",
       "\t\tat scala.Option.orElse(Option.scala:447)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$5(DeltaTableV2.scala:263)\n",
       "\t\tat scala.Option.orElse(Option.scala:447)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$2(DeltaTableV2.scala:260)\n",
       "\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:83)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$1(DeltaTableV2.scala:290)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:700)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot$lzycompute(DeltaTableV2.scala:240)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot(DeltaTableV2.scala:238)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$tableSchema$2(DeltaTableV2.scala:317)\n",
       "\t\tat scala.Option.getOrElse(Option.scala:189)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.tableSchema$lzycompute(DeltaTableV2.scala:317)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.tableSchema(DeltaTableV2.scala:315)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.schema(DeltaTableV2.scala:322)\n",
       "\t\tat org.apache.spark.sql.connector.catalog.Table.columns(Table.java:68)\n",
       "\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.columns(DeltaTableV2.scala:83)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.ResolvedTable$.create(v2ResolutionPlans.scala:231)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$4(Analyzer.scala:1600)\n",
       "\t\tat scala.Option.map(Option.scala:230)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$3(Analyzer.scala:1592)\n",
       "\t\tat scala.Option.orElse(Option.scala:447)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableOrView(Analyzer.scala:1587)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1556)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1466)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:182)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:171)\n",
       "\t\tat scala.PartialFunction.applyOrElse(PartialFunction.scala:127)\n",
       "\t\tat scala.PartialFunction.applyOrElse$(PartialFunction.scala:126)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.applyOrElse(AnalysisHelper.scala:171)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)\n",
       "\t\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:138)\n",
       "\t\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1330)\n",
       "\t\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1329)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.DescribeRelation.mapChildren(v2Commands.scala:726)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning(AnalysisHelper.scala:186)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning$(AnalysisHelper.scala:167)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithSubqueriesAndPruning(LogicalPlan.scala:42)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.resolveOperatorsUpWithSubqueriesAndPruningByConf(Analyzer.scala:1674)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply0(Analyzer.scala:1466)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1419)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1365)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:480)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:629)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:613)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:131)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:480)\n",
       "\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:479)\n",
       "\t\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n",
       "\t\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n",
       "\t\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:475)\n",
       "\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:452)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22(RuleExecutor.scala:585)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22$adapted(RuleExecutor.scala:585)\n",
       "\t\tat scala.collection.immutable.List.foreach(List.scala:431)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:585)\n",
       "\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:349)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:498)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:491)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:397)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:491)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:416)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:341)\n",
       "\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:219)\n",
       "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:341)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:252)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:96)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:131)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:87)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:478)\n",
       "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n",
       "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:478)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:294)\n",
       "\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:548)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:668)\n",
       "\t\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:151)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:668)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1307)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:661)\n",
       "\t\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:658)\n",
       "\t\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:658)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:288)\n",
       "\t\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)\n",
       "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:287)\n",
       "\t\tat scala.util.Try$.apply(Try.scala:213)\n",
       "\t\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)\n",
       "\t\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n",
       "\t\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n",
       "\t\t... 24 more\n",
       "</td><td>2025-08-13T14:34:00.30046Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>table</td><td>`dr_eo_amperity`.`silver`.`unified_paid_media`</td><td>clone</td><td>error</td><td>Failed to clone table `dr_eo_amperity`.`silver`.`unified_paid_media`.. Error: [DELTA_CLONE_UNSUPPORTED_SOURCE] Unsupported DEEP clone source '_eo_amperity.silver.unified_paid_media', whose format is deltasharing.\n",
       "The supported formats are 'delta', 'iceberg' and 'parquet'.</td><td>2025-08-13T14:33:56.226377Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>table</td><td>`dr_eo_amperity`.`silver`.`unified_scores`</td><td>clone</td><td>error</td><td>Failed to clone table `dr_eo_amperity`.`silver`.`unified_scores`.. Error: [DELTA_CLONE_UNSUPPORTED_SOURCE] Unsupported DEEP clone source '_eo_amperity.silver.unified_scores', whose format is deltasharing.\n",
       "The supported formats are 'delta', 'iceberg' and 'parquet'.</td><td>2025-08-13T14:33:56.137999Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>table</td><td>`dr_eo_amperity`.`silver`.`unified_merged`</td><td>clone</td><td>error</td><td>Failed to clone table `dr_eo_amperity`.`silver`.`unified_merged`.. Error: [DELTA_CLONE_UNSUPPORTED_SOURCE] Unsupported DEEP clone source '_eo_amperity.silver.unified_merged', whose format is deltasharing.\n",
       "The supported formats are 'delta', 'iceberg' and 'parquet'.</td><td>2025-08-13T14:33:55.785192Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>table</td><td>`dr_eo_amperity`.`silver`.`unified_loyalty_events`</td><td>clone</td><td>error</td><td>Failed to clone table `dr_eo_amperity`.`silver`.`unified_loyalty_events`.. Error: [DELTA_CLONE_UNSUPPORTED_SOURCE] Unsupported DEEP clone source '_eo_amperity.silver.unified_loyalty_events', whose format is deltasharing.\n",
       "The supported formats are 'delta', 'iceberg' and 'parquet'.</td><td>2025-08-13T14:33:51.634003Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>table</td><td>`dr_eo_amperity`.`silver`.`unified_loyalty`</td><td>clone</td><td>error</td><td>Failed to clone table `dr_eo_amperity`.`silver`.`unified_loyalty`.. Error: [DELTA_CLONE_UNSUPPORTED_SOURCE] Unsupported DEEP clone source '_eo_amperity.silver.unified_loyalty', whose format is deltasharing.\n",
       "The supported formats are 'delta', 'iceberg' and 'parquet'.</td><td>2025-08-13T14:33:51.346011Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>table</td><td>`dr_eo_amperity`.`silver`.`unified_coalesced`</td><td>clone</td><td>success</td><td>Table `dr_eo_amperity`.`silver`.`unified_coalesced` cloned successfully.</td><td>2025-08-13T14:33:51.172862Z</td><td>List(List(source_table_size, 25008603680), List(source_num_of_files, 234), List(num_of_synced_transactions, None), List(num_removed_files, 0), List(num_copied_files, 234), List(removed_files_size, 0), List(copied_files_size, 25008603680))</td></tr><tr><td>DR_Clone_data_v2</td><td>table</td><td>`dr_eo_amperity`.`silver`.`unified_itemized_transactions`</td><td>clone</td><td>error</td><td>Failed to clone table `dr_eo_amperity`.`silver`.`unified_itemized_transactions`.. Error: [DELTA_CLONE_UNSUPPORTED_SOURCE] Unsupported DEEP clone source '_eo_amperity.silver.unified_itemized_transactions', whose format is deltasharing.\n",
       "The supported formats are 'delta', 'iceberg' and 'parquet'.</td><td>2025-08-13T14:28:34.785531Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>table</td><td>`dr_eo_amperity`.`silver`.`unified_customer`</td><td>clone</td><td>error</td><td>Failed to clone table `dr_eo_amperity`.`silver`.`unified_customer`.. Error: [DELTA_CLONE_UNSUPPORTED_SOURCE] Unsupported DEEP clone source '_eo_amperity.silver.unified_customer', whose format is deltasharing.\n",
       "The supported formats are 'delta', 'iceberg' and 'parquet'.</td><td>2025-08-13T14:28:34.54264Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>table</td><td>`dr_eo_amperity`.`silver`.`unified_household`</td><td>clone</td><td>error</td><td>Failed to clone table `dr_eo_amperity`.`silver`.`unified_household`.. Error: [DELTA_CLONE_UNSUPPORTED_SOURCE] Unsupported DEEP clone source '_eo_amperity.silver.unified_household', whose format is deltasharing.\n",
       "The supported formats are 'delta', 'iceberg' and 'parquet'.</td><td>2025-08-13T14:28:34.530074Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>table</td><td>`dr_eo_amperity`.`silver`.`unified_compliance`</td><td>clone</td><td>error</td><td>Failed to clone table `dr_eo_amperity`.`silver`.`unified_compliance`.. Error: [DELTA_CLONE_UNSUPPORTED_SOURCE] Unsupported DEEP clone source '_eo_amperity.silver.unified_compliance', whose format is deltasharing.\n",
       "The supported formats are 'delta', 'iceberg' and 'parquet'.</td><td>2025-08-13T14:28:34.33462Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>table</td><td>`dr_eo_amperity`.`silver`.`fiscal_calendar`</td><td>clone</td><td>error</td><td>Failed to clone table `dr_eo_amperity`.`silver`.`fiscal_calendar`.. Error: [DELTA_CLONE_UNSUPPORTED_SOURCE] Unsupported DEEP clone source '_eo_amperity.silver.fiscal_calendar', whose format is deltasharing.\n",
       "The supported formats are 'delta', 'iceberg' and 'parquet'.</td><td>2025-08-13T14:28:29.570012Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>table</td><td>`dr_eo_amperity`.`default`.`unified_transactions_reporting`</td><td>clone</td><td>error</td><td>Failed to clone table `dr_eo_amperity`.`default`.`unified_transactions_reporting`.. Error: [DELTA_CLONE_UNSUPPORTED_SOURCE] Unsupported DEEP clone source '_eo_amperity.default.unified_transactions_reporting', whose format is deltasharing.\n",
       "The supported formats are 'delta', 'iceberg' and 'parquet'.</td><td>2025-08-13T14:28:29.515228Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>table</td><td>`dr_eo_amperity`.`default`.`transaction_attributes_extended`</td><td>clone</td><td>error</td><td>Failed to clone table `dr_eo_amperity`.`default`.`transaction_attributes_extended`.. Error: [DELTA_CLONE_UNSUPPORTED_SOURCE] Unsupported DEEP clone source '_eo_amperity.default.transaction_attributes_extended', whose format is deltasharing.\n",
       "The supported formats are 'delta', 'iceberg' and 'parquet'.</td><td>2025-08-13T14:28:29.444302Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>table</td><td>`dr_eo_amperity`.`default`.`transaction_attributes`</td><td>clone</td><td>error</td><td>Failed to clone table `dr_eo_amperity`.`default`.`transaction_attributes`.. Error: [DELTA_CLONE_UNSUPPORTED_SOURCE] Unsupported DEEP clone source '_eo_amperity.default.transaction_attributes', whose format is deltasharing.\n",
       "The supported formats are 'delta', 'iceberg' and 'parquet'.</td><td>2025-08-13T14:28:29.150441Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>table</td><td>`dr_eo_amperity`.`default`.`predicted_clv_attributes`</td><td>clone</td><td>error</td><td>Failed to clone table `dr_eo_amperity`.`default`.`predicted_clv_attributes`.. Error: [DELTA_CLONE_UNSUPPORTED_SOURCE] Unsupported DEEP clone source '_eo_amperity.default.predicted_clv_attributes', whose format is deltasharing.\n",
       "The supported formats are 'delta', 'iceberg' and 'parquet'.</td><td>2025-08-13T14:28:29.007067Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>table</td><td>`dr_eo_amperity`.`bronze`.`web_visitors`</td><td>clone</td><td>success</td><td>Table `dr_eo_amperity`.`bronze`.`web_visitors` cloned successfully.</td><td>2025-08-13T14:28:24.794847Z</td><td>List(List(source_table_size, 334349516), List(source_num_of_files, 7), List(num_of_synced_transactions, None), List(num_removed_files, 0), List(num_copied_files, 7), List(removed_files_size, 0), List(copied_files_size, 334349516))</td></tr><tr><td>DR_Clone_data_v2</td><td>table</td><td>`dr_eo_amperity`.`bronze`.`wifi_connections`</td><td>clone</td><td>success</td><td>Table `dr_eo_amperity`.`bronze`.`wifi_connections` cloned successfully.</td><td>2025-08-13T14:28:24.790242Z</td><td>List(List(source_table_size, 7882742359), List(source_num_of_files, 52), List(num_of_synced_transactions, None), List(num_removed_files, 0), List(num_copied_files, 52), List(removed_files_size, 0), List(copied_files_size, 7882742359))</td></tr><tr><td>DR_Clone_data_v2</td><td>table</td><td>`dr_eo_amperity`.`default`.`predicted_affinity`</td><td>clone</td><td>error</td><td>Failed to clone table `dr_eo_amperity`.`default`.`predicted_affinity`.. Error: [DELTA_CLONE_UNSUPPORTED_SOURCE] Unsupported DEEP clone source '_eo_amperity.default.predicted_affinity', whose format is deltasharing.\n",
       "The supported formats are 'delta', 'iceberg' and 'parquet'.</td><td>2025-08-13T14:28:23.84342Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>table</td><td>`dr_eo_amperity`.`default`.`email_engagement_summary`</td><td>clone</td><td>error</td><td>Failed to clone table `dr_eo_amperity`.`default`.`email_engagement_summary`.. Error: [DELTA_CLONE_UNSUPPORTED_SOURCE] Unsupported DEEP clone source '_eo_amperity.default.email_engagement_summary', whose format is deltasharing.\n",
       "The supported formats are 'delta', 'iceberg' and 'parquet'.</td><td>2025-08-13T14:28:23.765253Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>table</td><td>`dr_eo_amperity`.`default`.`email_engagement_attributes`</td><td>clone</td><td>error</td><td>Failed to clone table `dr_eo_amperity`.`default`.`email_engagement_attributes`.. Error: [DELTA_CLONE_UNSUPPORTED_SOURCE] Unsupported DEEP clone source '_eo_amperity.default.email_engagement_attributes', whose format is deltasharing.\n",
       "The supported formats are 'delta', 'iceberg' and 'parquet'.</td><td>2025-08-13T14:28:23.595463Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>table</td><td>`dr_eo_amperity`.`default`.`customer_attributes`</td><td>clone</td><td>error</td><td>Failed to clone table `dr_eo_amperity`.`default`.`customer_attributes`.. Error: [DELTA_CLONE_UNSUPPORTED_SOURCE] Unsupported DEEP clone source '_eo_amperity.default.customer_attributes', whose format is deltasharing.\n",
       "The supported formats are 'delta', 'iceberg' and 'parquet'.</td><td>2025-08-13T14:26:14.538529Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>table</td><td>`dr_eo_amperity`.`default`.`customer_360`</td><td>clone</td><td>error</td><td>Failed to clone table `dr_eo_amperity`.`default`.`customer_360`.. Error: [DELTA_CLONE_UNSUPPORTED_SOURCE] Unsupported DEEP clone source '_eo_amperity.default.customer_360', whose format is deltasharing.\n",
       "The supported formats are 'delta', 'iceberg' and 'parquet'.</td><td>2025-08-13T14:26:14.526599Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>table</td><td>`dr_eo_amperity`.`default`.`analytics_tae`</td><td>clone</td><td>error</td><td>Failed to clone table `dr_eo_amperity`.`default`.`analytics_tae`.. Error: [DELTA_CLONE_UNSUPPORTED_SOURCE] Unsupported DEEP clone source '_eo_amperity.default.analytics_tae', whose format is deltasharing.\n",
       "The supported formats are 'delta', 'iceberg' and 'parquet'.</td><td>2025-08-13T14:26:14.104877Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>table</td><td>`dr_eo_amperity`.`bronze`.`e_commerce_customers`</td><td>clone</td><td>error</td><td>Failed to clone table `dr_eo_amperity`.`bronze`.`e_commerce_customers`.. Error: [DELTA_CLONE_UNSUPPORTED_SOURCE] Unsupported DEEP clone source '_eo_amperity.bronze.e_commerce_customers', whose format is deltasharing.\n",
       "The supported formats are 'delta', 'iceberg' and 'parquet'.</td><td>2025-08-13T14:25:56.750087Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>table</td><td>`dr_eo_amperity`.`bronze`.`pos_customers`</td><td>clone</td><td>error</td><td>Failed to clone table `dr_eo_amperity`.`bronze`.`pos_customers`.. Error: [DELTA_CLONE_UNSUPPORTED_SOURCE] Unsupported DEEP clone source '_eo_amperity.bronze.pos_customers', whose format is deltasharing.\n",
       "The supported formats are 'delta', 'iceberg' and 'parquet'.</td><td>2025-08-13T14:25:56.641197Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>table</td><td>`dr_eo_amperity`.`bronze`.`loyalty_members`</td><td>clone</td><td>error</td><td>Failed to clone table `dr_eo_amperity`.`bronze`.`loyalty_members`.. Error: [DELTA_CLONE_UNSUPPORTED_SOURCE] Unsupported DEEP clone source '_eo_amperity.bronze.loyalty_members', whose format is deltasharing.\n",
       "The supported formats are 'delta', 'iceberg' and 'parquet'.</td><td>2025-08-13T14:25:56.641138Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>catalog_pair</td><td>satya_share -> satya_dr_catalog</td><td>process</td><td>sequential_complete</td><td>Finished sequential processing for catalog pair: satya_share -> satya_dr_catalog</td><td>2025-08-13T14:25:53.439758Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>schema</td><td>`satya_dr_catalog`.`satyendranath_sure`</td><td>change_ownership</td><td>skipped</td><td>No ownership mapping found for this schema. Skipping ownership change.</td><td>2025-08-13T14:25:51.58873Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>schema</td><td>`satya_dr_catalog`.`satyendranath_sure`</td><td>create</td><td>success</td><td>Schema `satya_dr_catalog`.`satyendranath_sure` created or already exists.</td><td>2025-08-13T14:25:50.008498Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>schema</td><td>`satya_share`.`information_schema`</td><td>skip</td><td>success</td><td>Skipping `information_schema`.</td><td>2025-08-13T14:25:48.101563Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>catalog</td><td>`satya_dr_catalog`</td><td>create</td><td>success</td><td>Catalog `satya_dr_catalog` created or already exists.</td><td>2025-08-13T14:25:45.08812Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>catalog_pair</td><td>satya_share -> satya_dr_catalog</td><td>process</td><td>start</td><td>Starting processing for catalog pair: satya_share -> satya_dr_catalog</td><td>2025-08-13T14:25:42.396262Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>catalog</td><td>`test_not`</td><td>get_schemas</td><td>error</td><td>Failed to get schemas from source catalog `test_not`. Error: [NO_SUCH_CATALOG_EXCEPTION] Catalog 'test_not' was not found. Please verify the catalog name and then retry the query or command again. SQLSTATE: 42704</td><td>2025-08-13T14:25:40.674286Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>catalog</td><td>`dr_test_not`</td><td>create</td><td>success</td><td>Catalog `dr_test_not` created or already exists.</td><td>2025-08-13T14:25:38.514428Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>catalog_pair</td><td>test_not -> dr_test_not</td><td>process</td><td>start</td><td>Starting processing for catalog pair: test_not -> dr_test_not</td><td>2025-08-13T14:25:35.74414Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>catalog_pair</td><td>accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample -> dr_accuweather</td><td>process</td><td>sequential_complete</td><td>Finished sequential processing for catalog pair: accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample -> dr_accuweather</td><td>2025-08-13T14:25:33.833988Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>schema</td><td>`accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample`.`information_schema`</td><td>skip</td><td>success</td><td>Skipping `information_schema`.</td><td>2025-08-13T14:25:32.000314Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>schema</td><td>`dr_accuweather`.`forecast`</td><td>change_ownership</td><td>skipped</td><td>No ownership mapping found for this schema. Skipping ownership change.</td><td>2025-08-13T14:25:29.949426Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>schema</td><td>`dr_accuweather`.`forecast`</td><td>create</td><td>success</td><td>Schema `dr_accuweather`.`forecast` created or already exists.</td><td>2025-08-13T14:25:28.070324Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>catalog</td><td>`dr_accuweather`</td><td>create</td><td>success</td><td>Catalog `dr_accuweather` created or already exists.</td><td>2025-08-13T14:25:24.39967Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>catalog_pair</td><td>accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample -> dr_accuweather</td><td>process</td><td>start</td><td>Starting processing for catalog pair: accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample -> dr_accuweather</td><td>2025-08-13T14:25:21.774662Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>catalog_pair</td><td>_eo_amperity -> dr_eo_amperity</td><td>process</td><td>sequential_complete</td><td>Finished sequential processing for catalog pair: _eo_amperity -> dr_eo_amperity</td><td>2025-08-13T14:25:19.87602Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>schema</td><td>`dr_eo_amperity`.`silver`</td><td>change_ownership</td><td>success</td><td>Ownership of schema `dr_eo_amperity`.`silver` changed to `grp_test_clt_usa`.</td><td>2025-08-13T14:25:17.099907Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>schema</td><td>`dr_eo_amperity`.`silver`</td><td>create</td><td>success</td><td>Schema `dr_eo_amperity`.`silver` created or already exists.</td><td>2025-08-13T14:25:14.533455Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>schema</td><td>`_eo_amperity`.`information_schema`</td><td>skip</td><td>success</td><td>Skipping `information_schema`.</td><td>2025-08-13T14:25:11.844947Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>schema</td><td>`dr_eo_amperity`.`default`</td><td>change_ownership</td><td>success</td><td>Ownership of schema `dr_eo_amperity`.`default` changed to `grp_test_clt_usa`.</td><td>2025-08-13T14:25:09.542687Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>schema</td><td>`dr_eo_amperity`.`default`</td><td>create</td><td>success</td><td>Schema `dr_eo_amperity`.`default` created or already exists.</td><td>2025-08-13T14:25:07.045578Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>schema</td><td>`dr_eo_amperity`.`bronze`</td><td>change_ownership</td><td>success</td><td>Ownership of schema `dr_eo_amperity`.`bronze` changed to `grp_test_clt_usa`.</td><td>2025-08-13T14:25:04.35033Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>schema</td><td>`dr_eo_amperity`.`bronze`</td><td>create</td><td>success</td><td>Schema `dr_eo_amperity`.`bronze` created or already exists.</td><td>2025-08-13T14:25:01.683511Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>catalog</td><td>`dr_eo_amperity`</td><td>create</td><td>success</td><td>Catalog `dr_eo_amperity` created or already exists.</td><td>2025-08-13T14:24:58.068969Z</td><td>List()</td></tr><tr><td>DR_Clone_data_v2</td><td>catalog_pair</td><td>_eo_amperity -> dr_eo_amperity</td><td>process</td><td>start</td><td>Starting processing for catalog pair: _eo_amperity -> dr_eo_amperity</td><td>2025-08-13T14:24:47.473232Z</td><td>List()</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "DR_Volume_data_temp",
         "info",
         "N/A",
         "end_sync",
         "success",
         "Databricks Volume DR Sync completed.",
         "2025-09-18T21:20:03.37281Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/tiny.json",
         "skip",
         "success",
         "File already exists with same or newer modification date. Skipping.",
         "2025-09-18T21:20:02.469058Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch.txt",
         "skip",
         "success",
         "File already exists with same or newer modification date. Skipping.",
         "2025-09-18T21:20:01.555607Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-06-11_10-46.json",
         "skip",
         "success",
         "File already exists with same or newer modification date. Skipping.",
         "2025-09-18T21:20:00.692051Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-06-10_11-26.json",
         "skip",
         "success",
         "File already exists with same or newer modification date. Skipping.",
         "2025-09-18T21:19:59.799818Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-06-08_18-02.json",
         "skip",
         "success",
         "File already exists with same or newer modification date. Skipping.",
         "2025-09-18T21:19:58.983252Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-06-08_15-57.json",
         "skip",
         "success",
         "File already exists with same or newer modification date. Skipping.",
         "2025-09-18T21:19:58.034685Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-06-05_13-39.json",
         "skip",
         "success",
         "File already exists with same or newer modification date. Skipping.",
         "2025-09-18T21:19:57.36587Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-06-04_18-14.json",
         "skip",
         "success",
         "File already exists with same or newer modification date. Skipping.",
         "2025-09-18T21:19:56.391117Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-06-04_17-57.json",
         "skip",
         "success",
         "File already exists with same or newer modification date. Skipping.",
         "2025-09-18T21:19:55.536806Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-06-04_09-48.json",
         "skip",
         "success",
         "File already exists with same or newer modification date. Skipping.",
         "2025-09-18T21:19:53.629036Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-06-04_07-39.json",
         "skip",
         "success",
         "File already exists with same or newer modification date. Skipping.",
         "2025-09-18T21:19:52.762565Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-27_11-32.json",
         "skip",
         "success",
         "File already exists with same or newer modification date. Skipping.",
         "2025-09-18T21:19:51.936918Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-27_11-31.json",
         "skip",
         "success",
         "File already exists with same or newer modification date. Skipping.",
         "2025-09-18T21:19:51.125157Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-27_09-44.json",
         "skip",
         "success",
         "File already exists with same or newer modification date. Skipping.",
         "2025-09-18T21:19:50.288529Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-27_09-43.json",
         "skip",
         "success",
         "File already exists with same or newer modification date. Skipping.",
         "2025-09-18T21:19:49.467932Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-22_14-06.json",
         "skip",
         "success",
         "File already exists with same or newer modification date. Skipping.",
         "2025-09-18T21:19:48.651115Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-22_07-41.json",
         "skip",
         "success",
         "File already exists with same or newer modification date. Skipping.",
         "2025-09-18T21:19:47.820868Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-22_06-42.json",
         "skip",
         "success",
         "File already exists with same or newer modification date. Skipping.",
         "2025-09-18T21:19:47.025697Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-20_10-25.json",
         "skip",
         "success",
         "File already exists with same or newer modification date. Skipping.",
         "2025-09-18T21:19:46.20171Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-20_09-27.json",
         "skip",
         "success",
         "File already exists with same or newer modification date. Skipping.",
         "2025-09-18T21:19:45.397738Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-19_15-16.json",
         "skip",
         "success",
         "File already exists with same or newer modification date. Skipping.",
         "2025-09-18T21:19:44.572887Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-15_15-16.json",
         "skip",
         "success",
         "File already exists with same or newer modification date. Skipping.",
         "2025-09-18T21:19:43.747801Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-15_14-55.json",
         "skip",
         "success",
         "File already exists with same or newer modification date. Skipping.",
         "2025-09-18T21:19:42.925986Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-15_14-54.json",
         "skip",
         "success",
         "File already exists with same or newer modification date. Skipping.",
         "2025-09-18T21:19:42.135871Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-15_14-31.json",
         "skip",
         "success",
         "File already exists with same or newer modification date. Skipping.",
         "2025-09-18T21:19:41.323378Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-15_14-22.json",
         "skip",
         "success",
         "File already exists with same or newer modification date. Skipping.",
         "2025-09-18T21:19:40.502436Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-15_14-21.json",
         "skip",
         "success",
         "File already exists with same or newer modification date. Skipping.",
         "2025-09-18T21:19:39.698118Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-15_13-55.json",
         "skip",
         "success",
         "File already exists with same or newer modification date. Skipping.",
         "2025-09-18T21:19:38.894359Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-15_13-38.json",
         "skip",
         "success",
         "File already exists with same or newer modification date. Skipping.",
         "2025-09-18T21:19:38.102521Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-15_13-30.json",
         "skip",
         "success",
         "File already exists with same or newer modification date. Skipping.",
         "2025-09-18T21:19:37.279743Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-15_12-54.json",
         "skip",
         "success",
         "File already exists with same or newer modification date. Skipping.",
         "2025-09-18T21:19:36.473005Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-15_12-10.json",
         "skip",
         "success",
         "File already exists with same or newer modification date. Skipping.",
         "2025-09-18T21:19:35.665367Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-15_11-55.json",
         "skip",
         "success",
         "File already exists with same or newer modification date. Skipping.",
         "2025-09-18T21:19:34.844343Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-15_11-38.json",
         "skip",
         "success",
         "File already exists with same or newer modification date. Skipping.",
         "2025-09-18T21:19:34.058382Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-15_11-33.json",
         "skip",
         "success",
         "File already exists with same or newer modification date. Skipping.",
         "2025-09-18T21:19:33.253783Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-15_11-31.json",
         "skip",
         "success",
         "File already exists with same or newer modification date. Skipping.",
         "2025-09-18T21:19:32.475309Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-15_10-17.json",
         "skip",
         "success",
         "File already exists with same or newer modification date. Skipping.",
         "2025-09-18T21:19:31.655833Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-14_16-23.json",
         "skip",
         "success",
         "File already exists with same or newer modification date. Skipping.",
         "2025-09-18T21:19:30.731213Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-14_15-44.json",
         "skip",
         "success",
         "File already exists with same or newer modification date. Skipping.",
         "2025-09-18T21:19:30.081065Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-14_15-39.json",
         "skip",
         "success",
         "File already exists with same or newer modification date. Skipping.",
         "2025-09-18T21:19:29.266718Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-14_15-34.json",
         "skip",
         "success",
         "File already exists with same or newer modification date. Skipping.",
         "2025-09-18T21:19:28.515953Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-14_15-32.json",
         "skip",
         "success",
         "File already exists with same or newer modification date. Skipping.",
         "2025-09-18T21:19:27.792318Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-14_14-55.json",
         "skip",
         "success",
         "File already exists with same or newer modification date. Skipping.",
         "2025-09-18T21:19:26.98471Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-14_14-45.json",
         "skip",
         "success",
         "File already exists with same or newer modification date. Skipping.",
         "2025-09-18T21:19:26.144862Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-14_13-47.json",
         "skip",
         "success",
         "File already exists with same or newer modification date. Skipping.",
         "2025-09-18T21:19:25.356949Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-29_12-32.json",
         "skip",
         "success",
         "File already exists with same or newer modification date. Skipping.",
         "2025-09-18T21:19:24.116548Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_21-09.json",
         "skip",
         "success",
         "File already exists with same or newer modification date. Skipping.",
         "2025-09-18T21:19:23.21363Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_20-52.json",
         "skip",
         "success",
         "File already exists with same or newer modification date. Skipping.",
         "2025-09-18T21:19:22.457299Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_20-48.json",
         "skip",
         "success",
         "File already exists with same or newer modification date. Skipping.",
         "2025-09-18T21:19:21.630858Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_20-44.json",
         "skip",
         "success",
         "File already exists with same or newer modification date. Skipping.",
         "2025-09-18T21:19:20.817316Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_20-38.json",
         "skip",
         "success",
         "File already exists with same or newer modification date. Skipping.",
         "2025-09-18T21:19:20.002616Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_20-26.json",
         "skip",
         "success",
         "File already exists with same or newer modification date. Skipping.",
         "2025-09-18T21:19:19.196496Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_20-18.json",
         "skip",
         "success",
         "File already exists with same or newer modification date. Skipping.",
         "2025-09-18T21:19:18.389669Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_19-59.json",
         "skip",
         "success",
         "File already exists with same or newer modification date. Skipping.",
         "2025-09-18T21:19:17.584697Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_19-56.json",
         "skip",
         "success",
         "File already exists with same or newer modification date. Skipping.",
         "2025-09-18T21:19:16.784985Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_19-37.json",
         "skip",
         "success",
         "File already exists with same or newer modification date. Skipping.",
         "2025-09-18T21:19:15.956947Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_19-31.json",
         "skip",
         "success",
         "File already exists with same or newer modification date. Skipping.",
         "2025-09-18T21:19:15.097549Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_18-42.json",
         "skip",
         "success",
         "File already exists with same or newer modification date. Skipping.",
         "2025-09-18T21:19:14.298044Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_18-41.json",
         "skip",
         "success",
         "File already exists with same or newer modification date. Skipping.",
         "2025-09-18T21:19:13.493073Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_18-40.json",
         "skip",
         "success",
         "File already exists with same or newer modification date. Skipping.",
         "2025-09-18T21:19:12.688412Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_18-33.json",
         "skip",
         "success",
         "File already exists with same or newer modification date. Skipping.",
         "2025-09-18T21:19:11.856812Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_18-30.json",
         "skip",
         "success",
         "File already exists with same or newer modification date. Skipping.",
         "2025-09-18T21:19:11.023448Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_18-26.json",
         "skip",
         "success",
         "File already exists with same or newer modification date. Skipping.",
         "2025-09-18T21:19:10.219391Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_18-15.json",
         "skip",
         "success",
         "File already exists with same or newer modification date. Skipping.",
         "2025-09-18T21:19:09.422372Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_18-14.json",
         "skip",
         "success",
         "File already exists with same or newer modification date. Skipping.",
         "2025-09-18T21:19:08.646502Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_18-11.json",
         "skip",
         "success",
         "File already exists with same or newer modification date. Skipping.",
         "2025-09-18T21:19:07.890478Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_18-10.json",
         "skip",
         "success",
         "File already exists with same or newer modification date. Skipping.",
         "2025-09-18T21:19:07.006284Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_18-09.json",
         "skip",
         "success",
         "File already exists with same or newer modification date. Skipping.",
         "2025-09-18T21:19:06.226916Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_18-08.json",
         "skip",
         "success",
         "File already exists with same or newer modification date. Skipping.",
         "2025-09-18T21:19:05.481427Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_18-03.json",
         "skip",
         "success",
         "File already exists with same or newer modification date. Skipping.",
         "2025-09-18T21:19:03.852356Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_18-02.json",
         "skip",
         "success",
         "File already exists with same or newer modification date. Skipping.",
         "2025-09-18T21:19:03.11445Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_17-58.json",
         "skip",
         "success",
         "File already exists with same or newer modification date. Skipping.",
         "2025-09-18T21:19:02.137156Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_17-55.json",
         "skip",
         "success",
         "File already exists with same or newer modification date. Skipping.",
         "2025-09-18T21:19:01.326754Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_17-53.json",
         "skip",
         "success",
         "File already exists with same or newer modification date. Skipping.",
         "2025-09-18T21:19:00.574946Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_17-31.json",
         "skip",
         "success",
         "File already exists with same or newer modification date. Skipping.",
         "2025-09-18T21:18:59.803776Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_16-23.json",
         "skip",
         "success",
         "File already exists with same or newer modification date. Skipping.",
         "2025-09-18T21:18:59.025693Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_16-22.json",
         "skip",
         "success",
         "File already exists with same or newer modification date. Skipping.",
         "2025-09-18T21:18:58.256405Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_16-21.json",
         "skip",
         "success",
         "File already exists with same or newer modification date. Skipping.",
         "2025-09-18T21:18:57.516385Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_15-54.json",
         "skip",
         "success",
         "File already exists with same or newer modification date. Skipping.",
         "2025-09-18T21:18:56.733128Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_15-53.json",
         "skip",
         "success",
         "File already exists with same or newer modification date. Skipping.",
         "2025-09-18T21:18:55.837284Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_15-48.json",
         "skip",
         "success",
         "File already exists with same or newer modification date. Skipping.",
         "2025-09-18T21:18:55.046105Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_15-47.json",
         "skip",
         "success",
         "File already exists with same or newer modification date. Skipping.",
         "2025-09-18T21:18:54.233327Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_15-46.json",
         "skip",
         "success",
         "File already exists with same or newer modification date. Skipping.",
         "2025-09-18T21:18:53.411614Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_15-42.json",
         "skip",
         "success",
         "File already exists with same or newer modification date. Skipping.",
         "2025-09-18T21:18:52.653976Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_15-29.json",
         "skip",
         "success",
         "File already exists with same or newer modification date. Skipping.",
         "2025-09-18T21:18:51.914796Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_15-23.json",
         "skip",
         "success",
         "File already exists with same or newer modification date. Skipping.",
         "2025-09-18T21:18:51.037709Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_12-40.json",
         "skip",
         "success",
         "File already exists with same or newer modification date. Skipping.",
         "2025-09-18T21:18:50.241422Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_12-01.json",
         "skip",
         "success",
         "File already exists with same or newer modification date. Skipping.",
         "2025-09-18T21:18:49.453236Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_10-57.json",
         "skip",
         "success",
         "File already exists with same or newer modification date. Skipping.",
         "2025-09-18T21:18:48.64584Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-25_17-38.json",
         "skip",
         "success",
         "File already exists with same or newer modification date. Skipping.",
         "2025-09-18T21:18:47.852961Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-25_17-34.json",
         "skip",
         "success",
         "File already exists with same or newer modification date. Skipping.",
         "2025-09-18T21:18:47.045576Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-25_17-20.json",
         "skip",
         "success",
         "File already exists with same or newer modification date. Skipping.",
         "2025-09-18T21:18:46.244033Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-25_13-29.json",
         "skip",
         "success",
         "File already exists with same or newer modification date. Skipping.",
         "2025-09-18T21:18:45.452131Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-25_12-04.json",
         "skip",
         "success",
         "File already exists with same or newer modification date. Skipping.",
         "2025-09-18T21:18:44.649239Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-24_16-31.json",
         "skip",
         "success",
         "File already exists with same or newer modification date. Skipping.",
         "2025-09-18T21:18:43.039297Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-24_16-28.json",
         "skip",
         "success",
         "File already exists with same or newer modification date. Skipping.",
         "2025-09-18T21:18:39.644097Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "directory",
         "dbfs:/Volumes/satya_share/satyendranath_sure/satya_volume/test/test1/",
         "list",
         "success",
         "Found directory to traverse.",
         "2025-09-18T21:18:39.247705Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-24_16-16.json",
         "skip",
         "success",
         "File already exists with same or newer modification date. Skipping.",
         "2025-09-18T21:18:38.184657Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/satya_share/satyendranath_sure/satya_volume/test/file_str11.0.parquet",
         "copy",
         "success",
         "File copied successfully.",
         "2025-09-18T21:18:37.953779Z",
         [
          [
           "result",
           "True"
          ],
          [
           "modificationTime",
           "1758221596000"
          ],
          [
           "fileSize",
           "6302661"
          ],
          [
           "source_path",
           "dbfs:/Volumes/satya_share/satyendranath_sure/satya_volume/test/file_str11.0.parquet"
          ],
          [
           "target_path",
           "/Volumes/satya_dr_catalog/satyendranath_sure/satya_volume/test/"
          ]
         ]
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-24_15-46.json",
         "skip",
         "success",
         "File already exists with same or newer modification date. Skipping.",
         "2025-09-18T21:18:37.525047Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-24_15-42.json",
         "skip",
         "success",
         "File already exists with same or newer modification date. Skipping.",
         "2025-09-18T21:18:36.660668Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-24_15-28.txt",
         "skip",
         "success",
         "File already exists with same or newer modification date. Skipping.",
         "2025-09-18T21:18:35.601026Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "directory",
         "dbfs:/Volumes/satya_share/satyendranath_sure/satya_volume/test/",
         "list",
         "success",
         "Found directory to traverse.",
         "2025-09-18T21:18:34.69322Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-24_15-23.txt",
         "skip",
         "success",
         "File already exists with same or newer modification date. Skipping.",
         "2025-09-18T21:18:34.125829Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/satya_share/satyendranath_sure/satya_volume/file_str13.0.parquet",
         "skip",
         "success",
         "File already exists with same or newer modification date. Skipping.",
         "2025-09-18T21:18:33.793662Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-24_15-14.txt",
         "skip",
         "success",
         "File already exists with same or newer modification date. Skipping.",
         "2025-09-18T21:18:33.082249Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/satya_share/satyendranath_sure/satya_volume/file_str11.0.parquet",
         "skip",
         "success",
         "File already exists with same or newer modification date. Skipping.",
         "2025-09-18T21:18:32.704189Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/satya_share/satyendranath_sure/satya_external_volume/file_str14.0.parquet",
         "skip",
         "success",
         "File already exists with same or newer modification date. Skipping.",
         "2025-09-18T21:18:32.36678Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-24_15-12.json",
         "skip",
         "success",
         "File already exists with same or newer modification date. Skipping.",
         "2025-09-18T21:18:32.01537Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/satya_share/satyendranath_sure/satya_volume/Group_access_revoke.csv",
         "skip",
         "success",
         "File already exists with same or newer modification date. Skipping.",
         "2025-09-18T21:18:31.136748Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/joe-stitch-2025-05-29_10-51.json",
         "skip",
         "success",
         "File already exists with same or newer modification date. Skipping.",
         "2025-09-18T21:18:30.722164Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/satya_share/satyendranath_sure/satya_external_volume/file_str13.0.parquet",
         "skip",
         "success",
         "File already exists with same or newer modification date. Skipping.",
         "2025-09-18T21:18:30.418506Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/file.txt",
         "skip",
         "success",
         "File already exists with same or newer modification date. Skipping.",
         "2025-09-18T21:18:29.445365Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/satya_share/satyendranath_sure/satya_volume/DR FY26 SOP.pdf",
         "skip",
         "success",
         "File already exists with same or newer modification date. Skipping.",
         "2025-09-18T21:18:28.989427Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/satya_share/satyendranath_sure/satya_external_volume/file_str11.0.parquet",
         "skip",
         "success",
         "File already exists with same or newer modification date. Skipping.",
         "2025-09-18T21:18:28.959274Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/democracy-manifest.json",
         "skip",
         "success",
         "File already exists with same or newer modification date. Skipping.",
         "2025-09-18T21:18:28.553817Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/cluster_init.sh",
         "skip",
         "success",
         "File already exists with same or newer modification date. Skipping.",
         "2025-09-18T21:18:26.893449Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/cluster_init-2025-06-11_10-46.sh",
         "skip",
         "success",
         "File already exists with same or newer modification date. Skipping.",
         "2025-09-18T21:18:25.569206Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "volume",
         "satya_dr_catalog.satyendranath_sure.satya_external_volume",
         "create",
         "success",
         "Volume created or already exists.",
         "2025-09-18T21:18:25.499743Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "volume",
         "satya_dr_catalog.satyendranath_sure.satya_volume",
         "create",
         "success",
         "Volume created or already exists.",
         "2025-09-18T21:18:25.492391Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/cluster_init-2025-06-10_11-26.sh",
         "skip",
         "success",
         "File already exists with same or newer modification date. Skipping.",
         "2025-09-18T21:18:24.682934Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "schema",
         "satya_dr_catalog.satyendranath_sure",
         "create",
         "success",
         "Schema `satyendranath_sure` in catalog `satya_dr_catalog` created or already exists.",
         "2025-09-18T21:18:23.674571Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/cluster_init-2025-06-08_18-02.sh",
         "skip",
         "success",
         "File already exists with same or newer modification date. Skipping.",
         "2025-09-18T21:18:22.990131Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "schema",
         "satya_share.information_schema",
         "skip",
         "success",
         "Skipping 'information_schema'.",
         "2025-09-18T21:18:22.597177Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/cluster_init-2025-06-08_15-57.sh",
         "skip",
         "success",
         "File already exists with same or newer modification date. Skipping.",
         "2025-09-18T21:18:22.172986Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/cluster_init-2025-06-05_13-39.sh",
         "skip",
         "success",
         "File already exists with same or newer modification date. Skipping.",
         "2025-09-18T21:18:21.263503Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "catalog",
         "satya_dr_catalog",
         "create",
         "success",
         "Catalog `satya_dr_catalog` created or already exists.",
         "2025-09-18T21:18:21.058016Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/cluster_init-2025-06-04_18-14.sh",
         "skip",
         "success",
         "File already exists with same or newer modification date. Skipping.",
         "2025-09-18T21:18:20.102671Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "catalog",
         "test_not",
         "process",
         "error",
         "An error occurred while processing catalog pair. Error: [NO_SUCH_CATALOG_EXCEPTION] Catalog 'test_not' was not found. Please verify the catalog name and then retry the query or command again. SQLSTATE: 42704\n\nJVM stacktrace:\norg.apache.spark.sql.catalyst.analysis.NoSuchCatalogException\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$listSchemasProto$1(ManagedCatalogClientImpl.scala:1249)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$recordAndWrapExceptionBase$2(ManagedCatalogClientImpl.scala:7501)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:113)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:159)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:104)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$recordAndWrapExceptionBase$1(ManagedCatalogClientImpl.scala:7500)\n\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:74)\n\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:66)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:268)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapExceptionBase(ManagedCatalogClientImpl.scala:7481)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:7467)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.listSchemasProto(ManagedCatalogClientImpl.scala:1215)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.listSchemas(ManagedCatalogClientImpl.scala:1201)\n\tat com.databricks.sql.managedcatalog.ManagedCatalogCommon.listSchemas(ManagedCatalogCommon.scala:824)\n\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.$anonfun$listSchemas$1(ProfiledManagedCatalog.scala:173)\n\tat org.apache.spark.sql.catalyst.MetricKeyUtils$.measure(MetricKey.scala:1836)\n\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.$anonfun$profile$1(ProfiledManagedCatalog.scala:64)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:113)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:159)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:104)\n\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.profile(ProfiledManagedCatalog.scala:63)\n\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.listSchemas(ProfiledManagedCatalog.scala:173)\n\tat com.databricks.sql.managedcatalog.ManagedCatalogSessionCatalog.listDatabasesWithCatalog(ManagedCatalogSessionCatalog.scala:928)\n\tat com.databricks.sql.managedcatalog.UnityCatalogV2Proxy.listNamespaces(UnityCatalogV2Proxy.scala:141)\n\tat org.apache.spark.sql.execution.command.ShowNamespacesCommand.run(ShowNamespacesCommand.scala:44)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$2(commands.scala:87)\n\tat org.apache.spark.sql.execution.SparkPlan.runCommandInAetherOrSpark(SparkPlan.scala:195)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$1(commands.scala:87)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:113)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:159)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:104)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:84)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:83)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:97)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$5(QueryExecution.scala:507)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$4(QueryExecution.scala:507)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:253)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$3(QueryExecution.scala:506)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$13(SQLExecution.scala:536)\n\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$12(SQLExecution.scala:458)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:834)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$11(SQLExecution.scala:386)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:121)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:115)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:120)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$10(SQLExecution.scala:386)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:863)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:385)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:238)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:787)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:502)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1449)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:498)\n\tat org.apache.spark.sql.execution.QueryExecution.withMVTagsIfNecessary(QueryExecution.scala:418)\n\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:496)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:578)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:570)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:529)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:529)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:361)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:357)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:505)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$8(QueryExecution.scala:570)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:570)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:374)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1686)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1747)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:75)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:379)\n\tat org.apache.spark.sql.classic.Dataset.<init>(Dataset.scala:432)\n\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$3(Dataset.scala:155)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.classic.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1071)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:113)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:159)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:104)\n\tat org.apache.spark.sql.classic.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1071)\n\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:146)\n\tat org.apache.spark.sql.classic.SparkSession.$anonfun$sql$4(SparkSession.scala:850)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.classic.SparkSession.sql(SparkSession.scala:813)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.executeSQL(SparkConnectPlanner.scala:3500)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleSqlCommand(SparkConnectPlanner.scala:3330)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3207)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:385)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:282)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:238)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:466)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:466)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:121)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:115)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:120)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:465)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:238)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$execute$1(ExecuteThreadRunner.scala:141)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries(UtilizationMetrics.scala:43)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries$(UtilizationMetrics.scala:40)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.recordActiveQueries(ExecuteThreadRunner.scala:53)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:139)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:586)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n\tat scala.util.Using$.resource(Using.scala:296)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:586)",
         "2025-09-18T21:18:19.607918Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/cluster_init-2025-06-04_17-57.sh",
         "skip",
         "success",
         "File already exists with same or newer modification date. Skipping.",
         "2025-09-18T21:18:18.485502Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "catalog",
         "dr_test_not",
         "create",
         "success",
         "Catalog `dr_test_not` created or already exists.",
         "2025-09-18T21:18:17.924352Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/cluster_init-2025-06-04_09-48.sh",
         "skip",
         "success",
         "File already exists with same or newer modification date. Skipping.",
         "2025-09-18T21:18:16.759192Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "schema",
         "accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample.information_schema",
         "skip",
         "success",
         "Skipping 'information_schema'.",
         "2025-09-18T21:18:16.021792Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/cluster_init-2025-06-04_07-39.sh",
         "skip",
         "success",
         "File already exists with same or newer modification date. Skipping.",
         "2025-09-18T21:18:15.028374Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/cluster_init-2025-06-02_20-37.sh",
         "skip",
         "success",
         "File already exists with same or newer modification date. Skipping.",
         "2025-09-18T21:18:14.077744Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "schema",
         "dr_accuweather.forecast",
         "create",
         "success",
         "Schema `forecast` in catalog `dr_accuweather` created or already exists.",
         "2025-09-18T21:18:13.49173Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/chinese-meal.json",
         "skip",
         "success",
         "File already exists with same or newer modification date. Skipping.",
         "2025-09-18T21:18:12.317919Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "catalog",
         "dr_accuweather",
         "create",
         "success",
         "Catalog `dr_accuweather` created or already exists.",
         "2025-09-18T21:18:11.674935Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/acme_w_fks.json",
         "skip",
         "success",
         "File already exists with same or newer modification date. Skipping.",
         "2025-09-18T21:18:11.45632Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/acme.json",
         "skip",
         "success",
         "File already exists with same or newer modification date. Skipping.",
         "2025-09-18T21:18:10.525822Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "schema",
         "dr_eo_amperity.silver",
         "create",
         "success",
         "Schema `silver` in catalog `dr_eo_amperity` created or already exists.",
         "2025-09-18T21:18:10.347031Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "schema",
         "_eo_amperity.information_schema",
         "skip",
         "success",
         "Skipping 'information_schema'.",
         "2025-09-18T21:18:09.313905Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "schema",
         "dr_eo_amperity.default",
         "create",
         "success",
         "Schema `default` in catalog `dr_eo_amperity` created or already exists.",
         "2025-09-18T21:18:07.7766Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "volume",
         "dr_eo_amperity.bronze.chuck",
         "create",
         "success",
         "Volume created or already exists.",
         "2025-09-18T21:18:07.763097Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "schema",
         "dr_eo_amperity.bronze",
         "create",
         "success",
         "Schema `bronze` in catalog `dr_eo_amperity` created or already exists.",
         "2025-09-18T21:18:06.087835Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "catalog",
         "dr_eo_amperity",
         "create",
         "success",
         "Catalog `dr_eo_amperity` created or already exists.",
         "2025-09-18T21:18:04.629207Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "info",
         "N/A",
         "start_sync",
         "success",
         "Starting Databricks Volume DR Sync.",
         "2025-09-18T21:18:03.164996Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "info",
         "N/A",
         "end_sync",
         "success",
         "Databricks Volume DR Sync completed.",
         "2025-09-18T21:16:47.807881Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/tiny.json",
         "copy",
         "success",
         "File copied successfully.",
         "2025-09-18T21:16:46.942367Z",
         [
          [
           "result",
           "True"
          ],
          [
           "modificationTime",
           "1745856608000"
          ],
          [
           "fileSize",
           "1914"
          ],
          [
           "source_path",
           "dbfs:/Volumes/_eo_amperity/bronze/chuck/tiny.json"
          ],
          [
           "target_path",
           "/Volumes/dr_eo_amperity/bronze/chuck"
          ]
         ]
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch.txt",
         "copy",
         "success",
         "File copied successfully.",
         "2025-09-18T21:16:45.861717Z",
         [
          [
           "result",
           "True"
          ],
          [
           "modificationTime",
           "1745534242000"
          ],
          [
           "fileSize",
           "17386"
          ],
          [
           "source_path",
           "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch.txt"
          ],
          [
           "target_path",
           "/Volumes/dr_eo_amperity/bronze/chuck"
          ]
         ]
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-06-11_10-46.json",
         "copy",
         "success",
         "File copied successfully.",
         "2025-09-18T21:16:44.765383Z",
         [
          [
           "result",
           "True"
          ],
          [
           "modificationTime",
           "1749663990000"
          ],
          [
           "fileSize",
           "9824"
          ],
          [
           "source_path",
           "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-06-11_10-46.json"
          ],
          [
           "target_path",
           "/Volumes/dr_eo_amperity/bronze/chuck"
          ]
         ]
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-06-10_11-26.json",
         "copy",
         "success",
         "File copied successfully.",
         "2025-09-18T21:16:43.561613Z",
         [
          [
           "result",
           "True"
          ],
          [
           "modificationTime",
           "1749580010000"
          ],
          [
           "fileSize",
           "9824"
          ],
          [
           "source_path",
           "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-06-10_11-26.json"
          ],
          [
           "target_path",
           "/Volumes/dr_eo_amperity/bronze/chuck"
          ]
         ]
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-06-08_18-02.json",
         "copy",
         "success",
         "File copied successfully.",
         "2025-09-18T21:16:42.501977Z",
         [
          [
           "result",
           "True"
          ],
          [
           "modificationTime",
           "1749430986000"
          ],
          [
           "fileSize",
           "9824"
          ],
          [
           "source_path",
           "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-06-08_18-02.json"
          ],
          [
           "target_path",
           "/Volumes/dr_eo_amperity/bronze/chuck"
          ]
         ]
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-06-08_15-57.json",
         "copy",
         "success",
         "File copied successfully.",
         "2025-09-18T21:16:41.422051Z",
         [
          [
           "result",
           "True"
          ],
          [
           "modificationTime",
           "1749423478000"
          ],
          [
           "fileSize",
           "9824"
          ],
          [
           "source_path",
           "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-06-08_15-57.json"
          ],
          [
           "target_path",
           "/Volumes/dr_eo_amperity/bronze/chuck"
          ]
         ]
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-06-05_13-39.json",
         "copy",
         "success",
         "File copied successfully.",
         "2025-09-18T21:16:40.246613Z",
         [
          [
           "result",
           "True"
          ],
          [
           "modificationTime",
           "1749155980000"
          ],
          [
           "fileSize",
           "9886"
          ],
          [
           "source_path",
           "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-06-05_13-39.json"
          ],
          [
           "target_path",
           "/Volumes/dr_eo_amperity/bronze/chuck"
          ]
         ]
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-06-04_18-14.json",
         "copy",
         "success",
         "File copied successfully.",
         "2025-09-18T21:16:39.19828Z",
         [
          [
           "result",
           "True"
          ],
          [
           "modificationTime",
           "1749086081000"
          ],
          [
           "fileSize",
           "9886"
          ],
          [
           "source_path",
           "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-06-04_18-14.json"
          ],
          [
           "target_path",
           "/Volumes/dr_eo_amperity/bronze/chuck"
          ]
         ]
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-06-04_17-57.json",
         "copy",
         "success",
         "File copied successfully.",
         "2025-09-18T21:16:38.109866Z",
         [
          [
           "result",
           "True"
          ],
          [
           "modificationTime",
           "1749085079000"
          ],
          [
           "fileSize",
           "9886"
          ],
          [
           "source_path",
           "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-06-04_17-57.json"
          ],
          [
           "target_path",
           "/Volumes/dr_eo_amperity/bronze/chuck"
          ]
         ]
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-06-04_09-48.json",
         "copy",
         "success",
         "File copied successfully.",
         "2025-09-18T21:16:37.027473Z",
         [
          [
           "result",
           "True"
          ],
          [
           "modificationTime",
           "1749055706000"
          ],
          [
           "fileSize",
           "9886"
          ],
          [
           "source_path",
           "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-06-04_09-48.json"
          ],
          [
           "target_path",
           "/Volumes/dr_eo_amperity/bronze/chuck"
          ]
         ]
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-06-04_07-39.json",
         "copy",
         "success",
         "File copied successfully.",
         "2025-09-18T21:16:35.905054Z",
         [
          [
           "result",
           "True"
          ],
          [
           "modificationTime",
           "1749047992000"
          ],
          [
           "fileSize",
           "9886"
          ],
          [
           "source_path",
           "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-06-04_07-39.json"
          ],
          [
           "target_path",
           "/Volumes/dr_eo_amperity/bronze/chuck"
          ]
         ]
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-27_11-32.json",
         "copy",
         "success",
         "File copied successfully.",
         "2025-09-18T21:16:34.797957Z",
         [
          [
           "result",
           "True"
          ],
          [
           "modificationTime",
           "1748370758000"
          ],
          [
           "fileSize",
           "16912"
          ],
          [
           "source_path",
           "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-27_11-32.json"
          ],
          [
           "target_path",
           "/Volumes/dr_eo_amperity/bronze/chuck"
          ]
         ]
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-27_11-31.json",
         "copy",
         "success",
         "File copied successfully.",
         "2025-09-18T21:16:33.603589Z",
         [
          [
           "result",
           "True"
          ],
          [
           "modificationTime",
           "1748370708000"
          ],
          [
           "fileSize",
           "16912"
          ],
          [
           "source_path",
           "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-27_11-31.json"
          ],
          [
           "target_path",
           "/Volumes/dr_eo_amperity/bronze/chuck"
          ]
         ]
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-27_09-44.json",
         "copy",
         "success",
         "File copied successfully.",
         "2025-09-18T21:16:32.465287Z",
         [
          [
           "result",
           "True"
          ],
          [
           "modificationTime",
           "1748364252000"
          ],
          [
           "fileSize",
           "16912"
          ],
          [
           "source_path",
           "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-27_09-44.json"
          ],
          [
           "target_path",
           "/Volumes/dr_eo_amperity/bronze/chuck"
          ]
         ]
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-27_09-43.json",
         "copy",
         "success",
         "File copied successfully.",
         "2025-09-18T21:16:31.386784Z",
         [
          [
           "result",
           "True"
          ],
          [
           "modificationTime",
           "1748364211000"
          ],
          [
           "fileSize",
           "14286"
          ],
          [
           "source_path",
           "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-27_09-43.json"
          ],
          [
           "target_path",
           "/Volumes/dr_eo_amperity/bronze/chuck"
          ]
         ]
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-22_14-06.json",
         "copy",
         "success",
         "File copied successfully.",
         "2025-09-18T21:16:30.283982Z",
         [
          [
           "result",
           "True"
          ],
          [
           "modificationTime",
           "1747937181000"
          ],
          [
           "fileSize",
           "16912"
          ],
          [
           "source_path",
           "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-22_14-06.json"
          ],
          [
           "target_path",
           "/Volumes/dr_eo_amperity/bronze/chuck"
          ]
         ]
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-22_07-41.json",
         "copy",
         "success",
         "File copied successfully.",
         "2025-09-18T21:16:29.218639Z",
         [
          [
           "result",
           "True"
          ],
          [
           "modificationTime",
           "1747924876000"
          ],
          [
           "fileSize",
           "16912"
          ],
          [
           "source_path",
           "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-22_07-41.json"
          ],
          [
           "target_path",
           "/Volumes/dr_eo_amperity/bronze/chuck"
          ]
         ]
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-22_06-42.json",
         "copy",
         "success",
         "File copied successfully.",
         "2025-09-18T21:16:27.997066Z",
         [
          [
           "result",
           "True"
          ],
          [
           "modificationTime",
           "1747910546000"
          ],
          [
           "fileSize",
           "16912"
          ],
          [
           "source_path",
           "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-22_06-42.json"
          ],
          [
           "target_path",
           "/Volumes/dr_eo_amperity/bronze/chuck"
          ]
         ]
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-20_10-25.json",
         "copy",
         "success",
         "File copied successfully.",
         "2025-09-18T21:16:26.967954Z",
         [
          [
           "result",
           "True"
          ],
          [
           "modificationTime",
           "1747761914000"
          ],
          [
           "fileSize",
           "17017"
          ],
          [
           "source_path",
           "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-20_10-25.json"
          ],
          [
           "target_path",
           "/Volumes/dr_eo_amperity/bronze/chuck"
          ]
         ]
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-20_09-27.json",
         "copy",
         "success",
         "File copied successfully.",
         "2025-09-18T21:16:25.908581Z",
         [
          [
           "result",
           "True"
          ],
          [
           "modificationTime",
           "1747758474000"
          ],
          [
           "fileSize",
           "16947"
          ],
          [
           "source_path",
           "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-20_09-27.json"
          ],
          [
           "target_path",
           "/Volumes/dr_eo_amperity/bronze/chuck"
          ]
         ]
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-19_15-16.json",
         "copy",
         "success",
         "File copied successfully.",
         "2025-09-18T21:16:24.54968Z",
         [
          [
           "result",
           "True"
          ],
          [
           "modificationTime",
           "1747693019000"
          ],
          [
           "fileSize",
           "16947"
          ],
          [
           "source_path",
           "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-19_15-16.json"
          ],
          [
           "target_path",
           "/Volumes/dr_eo_amperity/bronze/chuck"
          ]
         ]
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-15_15-16.json",
         "copy",
         "success",
         "File copied successfully.",
         "2025-09-18T21:16:23.362235Z",
         [
          [
           "result",
           "True"
          ],
          [
           "modificationTime",
           "1747347394000"
          ],
          [
           "fileSize",
           "16797"
          ],
          [
           "source_path",
           "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-15_15-16.json"
          ],
          [
           "target_path",
           "/Volumes/dr_eo_amperity/bronze/chuck"
          ]
         ]
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-15_14-55.json",
         "copy",
         "success",
         "File copied successfully.",
         "2025-09-18T21:16:22.28536Z",
         [
          [
           "result",
           "True"
          ],
          [
           "modificationTime",
           "1747346136000"
          ],
          [
           "fileSize",
           "16797"
          ],
          [
           "source_path",
           "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-15_14-55.json"
          ],
          [
           "target_path",
           "/Volumes/dr_eo_amperity/bronze/chuck"
          ]
         ]
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-15_14-54.json",
         "copy",
         "success",
         "File copied successfully.",
         "2025-09-18T21:16:21.226973Z",
         [
          [
           "result",
           "True"
          ],
          [
           "modificationTime",
           "1747346089000"
          ],
          [
           "fileSize",
           "16797"
          ],
          [
           "source_path",
           "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-15_14-54.json"
          ],
          [
           "target_path",
           "/Volumes/dr_eo_amperity/bronze/chuck"
          ]
         ]
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-15_14-31.json",
         "copy",
         "success",
         "File copied successfully.",
         "2025-09-18T21:16:20.153336Z",
         [
          [
           "result",
           "True"
          ],
          [
           "modificationTime",
           "1747344689000"
          ],
          [
           "fileSize",
           "16797"
          ],
          [
           "source_path",
           "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-15_14-31.json"
          ],
          [
           "target_path",
           "/Volumes/dr_eo_amperity/bronze/chuck"
          ]
         ]
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-15_14-22.json",
         "copy",
         "success",
         "File copied successfully.",
         "2025-09-18T21:16:19.086972Z",
         [
          [
           "result",
           "True"
          ],
          [
           "modificationTime",
           "1747344157000"
          ],
          [
           "fileSize",
           "16797"
          ],
          [
           "source_path",
           "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-15_14-22.json"
          ],
          [
           "target_path",
           "/Volumes/dr_eo_amperity/bronze/chuck"
          ]
         ]
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-15_14-21.json",
         "copy",
         "success",
         "File copied successfully.",
         "2025-09-18T21:16:17.940318Z",
         [
          [
           "result",
           "True"
          ],
          [
           "modificationTime",
           "1747344066000"
          ],
          [
           "fileSize",
           "16797"
          ],
          [
           "source_path",
           "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-15_14-21.json"
          ],
          [
           "target_path",
           "/Volumes/dr_eo_amperity/bronze/chuck"
          ]
         ]
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-15_13-55.json",
         "copy",
         "success",
         "File copied successfully.",
         "2025-09-18T21:16:16.833833Z",
         [
          [
           "result",
           "True"
          ],
          [
           "modificationTime",
           "1747342552000"
          ],
          [
           "fileSize",
           "16797"
          ],
          [
           "source_path",
           "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-15_13-55.json"
          ],
          [
           "target_path",
           "/Volumes/dr_eo_amperity/bronze/chuck"
          ]
         ]
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-15_13-38.json",
         "copy",
         "success",
         "File copied successfully.",
         "2025-09-18T21:16:15.708205Z",
         [
          [
           "result",
           "True"
          ],
          [
           "modificationTime",
           "1747341490000"
          ],
          [
           "fileSize",
           "16797"
          ],
          [
           "source_path",
           "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-15_13-38.json"
          ],
          [
           "target_path",
           "/Volumes/dr_eo_amperity/bronze/chuck"
          ]
         ]
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-15_13-30.json",
         "copy",
         "success",
         "File copied successfully.",
         "2025-09-18T21:16:14.606029Z",
         [
          [
           "result",
           "True"
          ],
          [
           "modificationTime",
           "1747341053000"
          ],
          [
           "fileSize",
           "16797"
          ],
          [
           "source_path",
           "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-15_13-30.json"
          ],
          [
           "target_path",
           "/Volumes/dr_eo_amperity/bronze/chuck"
          ]
         ]
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-15_12-54.json",
         "copy",
         "success",
         "File copied successfully.",
         "2025-09-18T21:16:13.526254Z",
         [
          [
           "result",
           "True"
          ],
          [
           "modificationTime",
           "1747338882000"
          ],
          [
           "fileSize",
           "16797"
          ],
          [
           "source_path",
           "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-15_12-54.json"
          ],
          [
           "target_path",
           "/Volumes/dr_eo_amperity/bronze/chuck"
          ]
         ]
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-15_12-10.json",
         "copy",
         "success",
         "File copied successfully.",
         "2025-09-18T21:16:12.372196Z",
         [
          [
           "result",
           "True"
          ],
          [
           "modificationTime",
           "1747336203000"
          ],
          [
           "fileSize",
           "16902"
          ],
          [
           "source_path",
           "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-15_12-10.json"
          ],
          [
           "target_path",
           "/Volumes/dr_eo_amperity/bronze/chuck"
          ]
         ]
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-15_11-55.json",
         "copy",
         "success",
         "File copied successfully.",
         "2025-09-18T21:16:11.289145Z",
         [
          [
           "result",
           "True"
          ],
          [
           "modificationTime",
           "1747335356000"
          ],
          [
           "fileSize",
           "16902"
          ],
          [
           "source_path",
           "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-15_11-55.json"
          ],
          [
           "target_path",
           "/Volumes/dr_eo_amperity/bronze/chuck"
          ]
         ]
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-15_11-38.json",
         "copy",
         "success",
         "File copied successfully.",
         "2025-09-18T21:16:10.204977Z",
         [
          [
           "result",
           "True"
          ],
          [
           "modificationTime",
           "1747334313000"
          ],
          [
           "fileSize",
           "16902"
          ],
          [
           "source_path",
           "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-15_11-38.json"
          ],
          [
           "target_path",
           "/Volumes/dr_eo_amperity/bronze/chuck"
          ]
         ]
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-15_11-33.json",
         "copy",
         "success",
         "File copied successfully.",
         "2025-09-18T21:16:09.084289Z",
         [
          [
           "result",
           "True"
          ],
          [
           "modificationTime",
           "1747333983000"
          ],
          [
           "fileSize",
           "16902"
          ],
          [
           "source_path",
           "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-15_11-33.json"
          ],
          [
           "target_path",
           "/Volumes/dr_eo_amperity/bronze/chuck"
          ]
         ]
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-15_11-31.json",
         "copy",
         "success",
         "File copied successfully.",
         "2025-09-18T21:16:08.0124Z",
         [
          [
           "result",
           "True"
          ],
          [
           "modificationTime",
           "1747333883000"
          ],
          [
           "fileSize",
           "299"
          ],
          [
           "source_path",
           "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-15_11-31.json"
          ],
          [
           "target_path",
           "/Volumes/dr_eo_amperity/bronze/chuck"
          ]
         ]
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-15_10-17.json",
         "copy",
         "success",
         "File copied successfully.",
         "2025-09-18T21:16:06.888347Z",
         [
          [
           "result",
           "True"
          ],
          [
           "modificationTime",
           "1747329464000"
          ],
          [
           "fileSize",
           "18012"
          ],
          [
           "source_path",
           "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-15_10-17.json"
          ],
          [
           "target_path",
           "/Volumes/dr_eo_amperity/bronze/chuck"
          ]
         ]
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-14_16-23.json",
         "copy",
         "success",
         "File copied successfully.",
         "2025-09-18T21:16:05.775096Z",
         [
          [
           "result",
           "True"
          ],
          [
           "modificationTime",
           "1747264990000"
          ],
          [
           "fileSize",
           "16902"
          ],
          [
           "source_path",
           "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-14_16-23.json"
          ],
          [
           "target_path",
           "/Volumes/dr_eo_amperity/bronze/chuck"
          ]
         ]
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-14_15-44.json",
         "copy",
         "success",
         "File copied successfully.",
         "2025-09-18T21:16:04.759318Z",
         [
          [
           "result",
           "True"
          ],
          [
           "modificationTime",
           "1747262693000"
          ],
          [
           "fileSize",
           "16902"
          ],
          [
           "source_path",
           "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-14_15-44.json"
          ],
          [
           "target_path",
           "/Volumes/dr_eo_amperity/bronze/chuck"
          ]
         ]
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-14_15-39.json",
         "copy",
         "success",
         "File copied successfully.",
         "2025-09-18T21:16:03.698591Z",
         [
          [
           "result",
           "True"
          ],
          [
           "modificationTime",
           "1747262388000"
          ],
          [
           "fileSize",
           "16832"
          ],
          [
           "source_path",
           "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-14_15-39.json"
          ],
          [
           "target_path",
           "/Volumes/dr_eo_amperity/bronze/chuck"
          ]
         ]
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-14_15-34.json",
         "copy",
         "success",
         "File copied successfully.",
         "2025-09-18T21:16:02.693571Z",
         [
          [
           "result",
           "True"
          ],
          [
           "modificationTime",
           "1747262078000"
          ],
          [
           "fileSize",
           "17893"
          ],
          [
           "source_path",
           "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-14_15-34.json"
          ],
          [
           "target_path",
           "/Volumes/dr_eo_amperity/bronze/chuck"
          ]
         ]
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-14_15-32.json",
         "copy",
         "success",
         "File copied successfully.",
         "2025-09-18T21:16:01.538427Z",
         [
          [
           "result",
           "True"
          ],
          [
           "modificationTime",
           "1747261965000"
          ],
          [
           "fileSize",
           "18012"
          ],
          [
           "source_path",
           "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-14_15-32.json"
          ],
          [
           "target_path",
           "/Volumes/dr_eo_amperity/bronze/chuck"
          ]
         ]
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-14_14-55.json",
         "copy",
         "success",
         "File copied successfully.",
         "2025-09-18T21:16:00.478159Z",
         [
          [
           "result",
           "True"
          ],
          [
           "modificationTime",
           "1747259736000"
          ],
          [
           "fileSize",
           "18012"
          ],
          [
           "source_path",
           "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-14_14-55.json"
          ],
          [
           "target_path",
           "/Volumes/dr_eo_amperity/bronze/chuck"
          ]
         ]
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-14_14-45.json",
         "copy",
         "success",
         "File copied successfully.",
         "2025-09-18T21:15:59.39073Z",
         [
          [
           "result",
           "True"
          ],
          [
           "modificationTime",
           "1747259154000"
          ],
          [
           "fileSize",
           "18012"
          ],
          [
           "source_path",
           "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-14_14-45.json"
          ],
          [
           "target_path",
           "/Volumes/dr_eo_amperity/bronze/chuck"
          ]
         ]
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-14_13-47.json",
         "copy",
         "success",
         "File copied successfully.",
         "2025-09-18T21:15:58.178338Z",
         [
          [
           "result",
           "True"
          ],
          [
           "modificationTime",
           "1747255644000"
          ],
          [
           "fileSize",
           "18012"
          ],
          [
           "source_path",
           "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-05-14_13-47.json"
          ],
          [
           "target_path",
           "/Volumes/dr_eo_amperity/bronze/chuck"
          ]
         ]
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-29_12-32.json",
         "copy",
         "success",
         "File copied successfully.",
         "2025-09-18T21:15:57.078919Z",
         [
          [
           "result",
           "True"
          ],
          [
           "modificationTime",
           "1745955178000"
          ],
          [
           "fileSize",
           "18047"
          ],
          [
           "source_path",
           "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-29_12-32.json"
          ],
          [
           "target_path",
           "/Volumes/dr_eo_amperity/bronze/chuck"
          ]
         ]
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_21-09.json",
         "copy",
         "success",
         "File copied successfully.",
         "2025-09-18T21:15:56.00826Z",
         [
          [
           "result",
           "True"
          ],
          [
           "modificationTime",
           "1745899795000"
          ],
          [
           "fileSize",
           "18047"
          ],
          [
           "source_path",
           "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_21-09.json"
          ],
          [
           "target_path",
           "/Volumes/dr_eo_amperity/bronze/chuck"
          ]
         ]
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_20-52.json",
         "copy",
         "success",
         "File copied successfully.",
         "2025-09-18T21:15:54.901841Z",
         [
          [
           "result",
           "True"
          ],
          [
           "modificationTime",
           "1745898778000"
          ],
          [
           "fileSize",
           "18047"
          ],
          [
           "source_path",
           "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_20-52.json"
          ],
          [
           "target_path",
           "/Volumes/dr_eo_amperity/bronze/chuck"
          ]
         ]
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_20-48.json",
         "copy",
         "success",
         "File copied successfully.",
         "2025-09-18T21:15:53.788665Z",
         [
          [
           "result",
           "True"
          ],
          [
           "modificationTime",
           "1745898532000"
          ],
          [
           "fileSize",
           "18047"
          ],
          [
           "source_path",
           "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_20-48.json"
          ],
          [
           "target_path",
           "/Volumes/dr_eo_amperity/bronze/chuck"
          ]
         ]
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_20-44.json",
         "copy",
         "success",
         "File copied successfully.",
         "2025-09-18T21:15:52.696591Z",
         [
          [
           "result",
           "True"
          ],
          [
           "modificationTime",
           "1745898284000"
          ],
          [
           "fileSize",
           "18047"
          ],
          [
           "source_path",
           "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_20-44.json"
          ],
          [
           "target_path",
           "/Volumes/dr_eo_amperity/bronze/chuck"
          ]
         ]
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_20-38.json",
         "copy",
         "success",
         "File copied successfully.",
         "2025-09-18T21:15:51.479603Z",
         [
          [
           "result",
           "True"
          ],
          [
           "modificationTime",
           "1745897893000"
          ],
          [
           "fileSize",
           "18047"
          ],
          [
           "source_path",
           "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_20-38.json"
          ],
          [
           "target_path",
           "/Volumes/dr_eo_amperity/bronze/chuck"
          ]
         ]
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_20-26.json",
         "copy",
         "success",
         "File copied successfully.",
         "2025-09-18T21:15:50.497189Z",
         [
          [
           "result",
           "True"
          ],
          [
           "modificationTime",
           "1745897189000"
          ],
          [
           "fileSize",
           "18047"
          ],
          [
           "source_path",
           "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_20-26.json"
          ],
          [
           "target_path",
           "/Volumes/dr_eo_amperity/bronze/chuck"
          ]
         ]
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_20-18.json",
         "copy",
         "success",
         "File copied successfully.",
         "2025-09-18T21:15:49.277021Z",
         [
          [
           "result",
           "True"
          ],
          [
           "modificationTime",
           "1745896726000"
          ],
          [
           "fileSize",
           "18047"
          ],
          [
           "source_path",
           "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_20-18.json"
          ],
          [
           "target_path",
           "/Volumes/dr_eo_amperity/bronze/chuck"
          ]
         ]
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_19-59.json",
         "copy",
         "success",
         "File copied successfully.",
         "2025-09-18T21:15:48.137651Z",
         [
          [
           "result",
           "True"
          ],
          [
           "modificationTime",
           "1745895594000"
          ],
          [
           "fileSize",
           "18047"
          ],
          [
           "source_path",
           "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_19-59.json"
          ],
          [
           "target_path",
           "/Volumes/dr_eo_amperity/bronze/chuck"
          ]
         ]
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_19-56.json",
         "copy",
         "success",
         "File copied successfully.",
         "2025-09-18T21:15:46.931652Z",
         [
          [
           "result",
           "True"
          ],
          [
           "modificationTime",
           "1745895417000"
          ],
          [
           "fileSize",
           "18047"
          ],
          [
           "source_path",
           "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_19-56.json"
          ],
          [
           "target_path",
           "/Volumes/dr_eo_amperity/bronze/chuck"
          ]
         ]
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_19-37.json",
         "copy",
         "success",
         "File copied successfully.",
         "2025-09-18T21:15:45.756765Z",
         [
          [
           "result",
           "True"
          ],
          [
           "modificationTime",
           "1745894253000"
          ],
          [
           "fileSize",
           "17194"
          ],
          [
           "source_path",
           "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_19-37.json"
          ],
          [
           "target_path",
           "/Volumes/dr_eo_amperity/bronze/chuck"
          ]
         ]
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_19-31.json",
         "copy",
         "success",
         "File copied successfully.",
         "2025-09-18T21:15:44.681708Z",
         [
          [
           "result",
           "True"
          ],
          [
           "modificationTime",
           "1745893886000"
          ],
          [
           "fileSize",
           "17194"
          ],
          [
           "source_path",
           "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_19-31.json"
          ],
          [
           "target_path",
           "/Volumes/dr_eo_amperity/bronze/chuck"
          ]
         ]
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_18-42.json",
         "copy",
         "success",
         "File copied successfully.",
         "2025-09-18T21:15:43.576033Z",
         [
          [
           "result",
           "True"
          ],
          [
           "modificationTime",
           "1745890946000"
          ],
          [
           "fileSize",
           "17194"
          ],
          [
           "source_path",
           "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_18-42.json"
          ],
          [
           "target_path",
           "/Volumes/dr_eo_amperity/bronze/chuck"
          ]
         ]
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_18-41.json",
         "copy",
         "success",
         "File copied successfully.",
         "2025-09-18T21:15:42.513374Z",
         [
          [
           "result",
           "True"
          ],
          [
           "modificationTime",
           "1745890894000"
          ],
          [
           "fileSize",
           "17194"
          ],
          [
           "source_path",
           "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_18-41.json"
          ],
          [
           "target_path",
           "/Volumes/dr_eo_amperity/bronze/chuck"
          ]
         ]
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_18-40.json",
         "copy",
         "success",
         "File copied successfully.",
         "2025-09-18T21:15:41.378613Z",
         [
          [
           "result",
           "True"
          ],
          [
           "modificationTime",
           "1745890854000"
          ],
          [
           "fileSize",
           "17194"
          ],
          [
           "source_path",
           "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_18-40.json"
          ],
          [
           "target_path",
           "/Volumes/dr_eo_amperity/bronze/chuck"
          ]
         ]
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_18-33.json",
         "copy",
         "success",
         "File copied successfully.",
         "2025-09-18T21:15:40.306935Z",
         [
          [
           "result",
           "True"
          ],
          [
           "modificationTime",
           "1745890411000"
          ],
          [
           "fileSize",
           "17194"
          ],
          [
           "source_path",
           "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_18-33.json"
          ],
          [
           "target_path",
           "/Volumes/dr_eo_amperity/bronze/chuck"
          ]
         ]
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_18-30.json",
         "copy",
         "success",
         "File copied successfully.",
         "2025-09-18T21:15:39.118781Z",
         [
          [
           "result",
           "True"
          ],
          [
           "modificationTime",
           "1745890252000"
          ],
          [
           "fileSize",
           "17194"
          ],
          [
           "source_path",
           "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_18-30.json"
          ],
          [
           "target_path",
           "/Volumes/dr_eo_amperity/bronze/chuck"
          ]
         ]
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_18-26.json",
         "copy",
         "success",
         "File copied successfully.",
         "2025-09-18T21:15:38.047954Z",
         [
          [
           "result",
           "True"
          ],
          [
           "modificationTime",
           "1745890004000"
          ],
          [
           "fileSize",
           "17194"
          ],
          [
           "source_path",
           "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_18-26.json"
          ],
          [
           "target_path",
           "/Volumes/dr_eo_amperity/bronze/chuck"
          ]
         ]
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_18-15.json",
         "copy",
         "success",
         "File copied successfully.",
         "2025-09-18T21:15:36.937003Z",
         [
          [
           "result",
           "True"
          ],
          [
           "modificationTime",
           "1745889301000"
          ],
          [
           "fileSize",
           "17194"
          ],
          [
           "source_path",
           "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_18-15.json"
          ],
          [
           "target_path",
           "/Volumes/dr_eo_amperity/bronze/chuck"
          ]
         ]
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_18-14.json",
         "copy",
         "success",
         "File copied successfully.",
         "2025-09-18T21:15:35.820544Z",
         [
          [
           "result",
           "True"
          ],
          [
           "modificationTime",
           "1745889261000"
          ],
          [
           "fileSize",
           "55"
          ],
          [
           "source_path",
           "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_18-14.json"
          ],
          [
           "target_path",
           "/Volumes/dr_eo_amperity/bronze/chuck"
          ]
         ]
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_18-11.json",
         "copy",
         "success",
         "File copied successfully.",
         "2025-09-18T21:15:34.744886Z",
         [
          [
           "result",
           "True"
          ],
          [
           "modificationTime",
           "1745889083000"
          ],
          [
           "fileSize",
           "55"
          ],
          [
           "source_path",
           "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_18-11.json"
          ],
          [
           "target_path",
           "/Volumes/dr_eo_amperity/bronze/chuck"
          ]
         ]
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_18-10.json",
         "copy",
         "success",
         "File copied successfully.",
         "2025-09-18T21:15:33.681794Z",
         [
          [
           "result",
           "True"
          ],
          [
           "modificationTime",
           "1745889054000"
          ],
          [
           "fileSize",
           "55"
          ],
          [
           "source_path",
           "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_18-10.json"
          ],
          [
           "target_path",
           "/Volumes/dr_eo_amperity/bronze/chuck"
          ]
         ]
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_18-09.json",
         "copy",
         "success",
         "File copied successfully.",
         "2025-09-18T21:15:32.656915Z",
         [
          [
           "result",
           "True"
          ],
          [
           "modificationTime",
           "1745888998000"
          ],
          [
           "fileSize",
           "55"
          ],
          [
           "source_path",
           "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_18-09.json"
          ],
          [
           "target_path",
           "/Volumes/dr_eo_amperity/bronze/chuck"
          ]
         ]
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_18-08.json",
         "copy",
         "success",
         "File copied successfully.",
         "2025-09-18T21:15:31.632977Z",
         [
          [
           "result",
           "True"
          ],
          [
           "modificationTime",
           "1745888923000"
          ],
          [
           "fileSize",
           "55"
          ],
          [
           "source_path",
           "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_18-08.json"
          ],
          [
           "target_path",
           "/Volumes/dr_eo_amperity/bronze/chuck"
          ]
         ]
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_18-03.json",
         "copy",
         "success",
         "File copied successfully.",
         "2025-09-18T21:15:30.592632Z",
         [
          [
           "result",
           "True"
          ],
          [
           "modificationTime",
           "1745888602000"
          ],
          [
           "fileSize",
           "55"
          ],
          [
           "source_path",
           "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_18-03.json"
          ],
          [
           "target_path",
           "/Volumes/dr_eo_amperity/bronze/chuck"
          ]
         ]
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_18-02.json",
         "copy",
         "success",
         "File copied successfully.",
         "2025-09-18T21:15:29.515902Z",
         [
          [
           "result",
           "True"
          ],
          [
           "modificationTime",
           "1745888569000"
          ],
          [
           "fileSize",
           "55"
          ],
          [
           "source_path",
           "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_18-02.json"
          ],
          [
           "target_path",
           "/Volumes/dr_eo_amperity/bronze/chuck"
          ]
         ]
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_17-58.json",
         "copy",
         "success",
         "File copied successfully.",
         "2025-09-18T21:15:28.515036Z",
         [
          [
           "result",
           "True"
          ],
          [
           "modificationTime",
           "1745888285000"
          ],
          [
           "fileSize",
           "55"
          ],
          [
           "source_path",
           "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_17-58.json"
          ],
          [
           "target_path",
           "/Volumes/dr_eo_amperity/bronze/chuck"
          ]
         ]
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_17-55.json",
         "copy",
         "success",
         "File copied successfully.",
         "2025-09-18T21:15:27.407532Z",
         [
          [
           "result",
           "True"
          ],
          [
           "modificationTime",
           "1745888126000"
          ],
          [
           "fileSize",
           "55"
          ],
          [
           "source_path",
           "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_17-55.json"
          ],
          [
           "target_path",
           "/Volumes/dr_eo_amperity/bronze/chuck"
          ]
         ]
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_17-53.json",
         "copy",
         "success",
         "File copied successfully.",
         "2025-09-18T21:15:26.301526Z",
         [
          [
           "result",
           "True"
          ],
          [
           "modificationTime",
           "1745888001000"
          ],
          [
           "fileSize",
           "55"
          ],
          [
           "source_path",
           "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_17-53.json"
          ],
          [
           "target_path",
           "/Volumes/dr_eo_amperity/bronze/chuck"
          ]
         ]
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_17-31.json",
         "copy",
         "success",
         "File copied successfully.",
         "2025-09-18T21:15:25.243905Z",
         [
          [
           "result",
           "True"
          ],
          [
           "modificationTime",
           "1745886718000"
          ],
          [
           "fileSize",
           "55"
          ],
          [
           "source_path",
           "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_17-31.json"
          ],
          [
           "target_path",
           "/Volumes/dr_eo_amperity/bronze/chuck"
          ]
         ]
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_16-23.json",
         "copy",
         "success",
         "File copied successfully.",
         "2025-09-18T21:15:24.092939Z",
         [
          [
           "result",
           "True"
          ],
          [
           "modificationTime",
           "1745882597000"
          ],
          [
           "fileSize",
           "55"
          ],
          [
           "source_path",
           "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_16-23.json"
          ],
          [
           "target_path",
           "/Volumes/dr_eo_amperity/bronze/chuck"
          ]
         ]
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_16-22.json",
         "copy",
         "success",
         "File copied successfully.",
         "2025-09-18T21:15:22.974103Z",
         [
          [
           "result",
           "True"
          ],
          [
           "modificationTime",
           "1745882546000"
          ],
          [
           "fileSize",
           "55"
          ],
          [
           "source_path",
           "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_16-22.json"
          ],
          [
           "target_path",
           "/Volumes/dr_eo_amperity/bronze/chuck"
          ]
         ]
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_16-21.json",
         "copy",
         "success",
         "File copied successfully.",
         "2025-09-18T21:15:21.909218Z",
         [
          [
           "result",
           "True"
          ],
          [
           "modificationTime",
           "1745882513000"
          ],
          [
           "fileSize",
           "55"
          ],
          [
           "source_path",
           "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_16-21.json"
          ],
          [
           "target_path",
           "/Volumes/dr_eo_amperity/bronze/chuck"
          ]
         ]
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_15-54.json",
         "copy",
         "success",
         "File copied successfully.",
         "2025-09-18T21:15:20.851122Z",
         [
          [
           "result",
           "True"
          ],
          [
           "modificationTime",
           "1745880870000"
          ],
          [
           "fileSize",
           "55"
          ],
          [
           "source_path",
           "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_15-54.json"
          ],
          [
           "target_path",
           "/Volumes/dr_eo_amperity/bronze/chuck"
          ]
         ]
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_15-53.json",
         "copy",
         "success",
         "File copied successfully.",
         "2025-09-18T21:15:19.757661Z",
         [
          [
           "result",
           "True"
          ],
          [
           "modificationTime",
           "1745880836000"
          ],
          [
           "fileSize",
           "55"
          ],
          [
           "source_path",
           "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_15-53.json"
          ],
          [
           "target_path",
           "/Volumes/dr_eo_amperity/bronze/chuck"
          ]
         ]
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_15-48.json",
         "copy",
         "success",
         "File copied successfully.",
         "2025-09-18T21:15:18.681895Z",
         [
          [
           "result",
           "True"
          ],
          [
           "modificationTime",
           "1745880531000"
          ],
          [
           "fileSize",
           "55"
          ],
          [
           "source_path",
           "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_15-48.json"
          ],
          [
           "target_path",
           "/Volumes/dr_eo_amperity/bronze/chuck"
          ]
         ]
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_15-47.json",
         "copy",
         "success",
         "File copied successfully.",
         "2025-09-18T21:15:17.624036Z",
         [
          [
           "result",
           "True"
          ],
          [
           "modificationTime",
           "1745880480000"
          ],
          [
           "fileSize",
           "55"
          ],
          [
           "source_path",
           "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_15-47.json"
          ],
          [
           "target_path",
           "/Volumes/dr_eo_amperity/bronze/chuck"
          ]
         ]
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_15-46.json",
         "copy",
         "success",
         "File copied successfully.",
         "2025-09-18T21:15:16.408467Z",
         [
          [
           "result",
           "True"
          ],
          [
           "modificationTime",
           "1745880398000"
          ],
          [
           "fileSize",
           "55"
          ],
          [
           "source_path",
           "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_15-46.json"
          ],
          [
           "target_path",
           "/Volumes/dr_eo_amperity/bronze/chuck"
          ]
         ]
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_15-42.json",
         "copy",
         "success",
         "File copied successfully.",
         "2025-09-18T21:15:15.163876Z",
         [
          [
           "result",
           "True"
          ],
          [
           "modificationTime",
           "1745880146000"
          ],
          [
           "fileSize",
           "55"
          ],
          [
           "source_path",
           "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_15-42.json"
          ],
          [
           "target_path",
           "/Volumes/dr_eo_amperity/bronze/chuck"
          ]
         ]
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_15-29.json",
         "copy",
         "success",
         "File copied successfully.",
         "2025-09-18T21:15:14.107296Z",
         [
          [
           "result",
           "True"
          ],
          [
           "modificationTime",
           "1745879389000"
          ],
          [
           "fileSize",
           "55"
          ],
          [
           "source_path",
           "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_15-29.json"
          ],
          [
           "target_path",
           "/Volumes/dr_eo_amperity/bronze/chuck"
          ]
         ]
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_15-23.json",
         "copy",
         "success",
         "File copied successfully.",
         "2025-09-18T21:15:13.083335Z",
         [
          [
           "result",
           "True"
          ],
          [
           "modificationTime",
           "1745879035000"
          ],
          [
           "fileSize",
           "55"
          ],
          [
           "source_path",
           "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_15-23.json"
          ],
          [
           "target_path",
           "/Volumes/dr_eo_amperity/bronze/chuck"
          ]
         ]
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_12-40.json",
         "copy",
         "success",
         "File copied successfully.",
         "2025-09-18T21:15:12.07184Z",
         [
          [
           "result",
           "True"
          ],
          [
           "modificationTime",
           "1745869227000"
          ],
          [
           "fileSize",
           "17698"
          ],
          [
           "source_path",
           "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_12-40.json"
          ],
          [
           "target_path",
           "/Volumes/dr_eo_amperity/bronze/chuck"
          ]
         ]
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_12-01.json",
         "copy",
         "success",
         "File copied successfully.",
         "2025-09-18T21:15:11.030927Z",
         [
          [
           "result",
           "True"
          ],
          [
           "modificationTime",
           "1745866879000"
          ],
          [
           "fileSize",
           "17698"
          ],
          [
           "source_path",
           "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_12-01.json"
          ],
          [
           "target_path",
           "/Volumes/dr_eo_amperity/bronze/chuck"
          ]
         ]
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_10-57.json",
         "copy",
         "success",
         "File copied successfully.",
         "2025-09-18T21:15:09.995028Z",
         [
          [
           "result",
           "True"
          ],
          [
           "modificationTime",
           "1745863055000"
          ],
          [
           "fileSize",
           "30370"
          ],
          [
           "source_path",
           "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-28_10-57.json"
          ],
          [
           "target_path",
           "/Volumes/dr_eo_amperity/bronze/chuck"
          ]
         ]
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-25_17-38.json",
         "copy",
         "success",
         "File copied successfully.",
         "2025-09-18T21:15:08.982163Z",
         [
          [
           "result",
           "True"
          ],
          [
           "modificationTime",
           "1745627907000"
          ],
          [
           "fileSize",
           "30370"
          ],
          [
           "source_path",
           "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-25_17-38.json"
          ],
          [
           "target_path",
           "/Volumes/dr_eo_amperity/bronze/chuck"
          ]
         ]
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-25_17-34.json",
         "copy",
         "success",
         "File copied successfully.",
         "2025-09-18T21:15:07.957553Z",
         [
          [
           "result",
           "True"
          ],
          [
           "modificationTime",
           "1745627668000"
          ],
          [
           "fileSize",
           "30370"
          ],
          [
           "source_path",
           "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-25_17-34.json"
          ],
          [
           "target_path",
           "/Volumes/dr_eo_amperity/bronze/chuck"
          ]
         ]
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-25_17-20.json",
         "copy",
         "success",
         "File copied successfully.",
         "2025-09-18T21:15:06.479554Z",
         [
          [
           "result",
           "True"
          ],
          [
           "modificationTime",
           "1745626856000"
          ],
          [
           "fileSize",
           "30370"
          ],
          [
           "source_path",
           "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-25_17-20.json"
          ],
          [
           "target_path",
           "/Volumes/dr_eo_amperity/bronze/chuck"
          ]
         ]
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-25_13-29.json",
         "copy",
         "success",
         "File copied successfully.",
         "2025-09-18T21:15:05.314691Z",
         [
          [
           "result",
           "True"
          ],
          [
           "modificationTime",
           "1745613000000"
          ],
          [
           "fileSize",
           "30370"
          ],
          [
           "source_path",
           "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-25_13-29.json"
          ],
          [
           "target_path",
           "/Volumes/dr_eo_amperity/bronze/chuck"
          ]
         ]
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-25_12-04.json",
         "copy",
         "success",
         "File copied successfully.",
         "2025-09-18T21:15:04.280904Z",
         [
          [
           "result",
           "True"
          ],
          [
           "modificationTime",
           "1745607856000"
          ],
          [
           "fileSize",
           "30370"
          ],
          [
           "source_path",
           "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-25_12-04.json"
          ],
          [
           "target_path",
           "/Volumes/dr_eo_amperity/bronze/chuck"
          ]
         ]
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-24_16-31.json",
         "copy",
         "success",
         "File copied successfully.",
         "2025-09-18T21:15:03.222782Z",
         [
          [
           "result",
           "True"
          ],
          [
           "modificationTime",
           "1745537510000"
          ],
          [
           "fileSize",
           "24570"
          ],
          [
           "source_path",
           "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-24_16-31.json"
          ],
          [
           "target_path",
           "/Volumes/dr_eo_amperity/bronze/chuck"
          ]
         ]
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-24_16-28.json",
         "copy",
         "success",
         "File copied successfully.",
         "2025-09-18T21:15:02.165194Z",
         [
          [
           "result",
           "True"
          ],
          [
           "modificationTime",
           "1745537285000"
          ],
          [
           "fileSize",
           "24570"
          ],
          [
           "source_path",
           "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-24_16-28.json"
          ],
          [
           "target_path",
           "/Volumes/dr_eo_amperity/bronze/chuck"
          ]
         ]
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-24_16-16.json",
         "copy",
         "success",
         "File copied successfully.",
         "2025-09-18T21:15:00.997723Z",
         [
          [
           "result",
           "True"
          ],
          [
           "modificationTime",
           "1745536604000"
          ],
          [
           "fileSize",
           "23764"
          ],
          [
           "source_path",
           "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-24_16-16.json"
          ],
          [
           "target_path",
           "/Volumes/dr_eo_amperity/bronze/chuck"
          ]
         ]
        ],
        [
         "DR_Volume_data_temp",
         "directory",
         "dbfs:/Volumes/satya_share/satyendranath_sure/satya_volume/test/test1/",
         "list",
         "success",
         "Found directory to traverse.",
         "2025-09-18T21:14:59.946687Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-24_15-46.json",
         "copy",
         "success",
         "File copied successfully.",
         "2025-09-18T21:14:59.879595Z",
         [
          [
           "result",
           "True"
          ],
          [
           "modificationTime",
           "1745534811000"
          ],
          [
           "fileSize",
           "12523"
          ],
          [
           "source_path",
           "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-24_15-46.json"
          ],
          [
           "target_path",
           "/Volumes/dr_eo_amperity/bronze/chuck"
          ]
         ]
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/satya_share/satyendranath_sure/satya_volume/test/file_str11.0.parquet",
         "copy",
         "success",
         "File copied successfully.",
         "2025-09-18T21:14:58.940053Z",
         [
          [
           "result",
           "True"
          ],
          [
           "modificationTime",
           "1758221596000"
          ],
          [
           "fileSize",
           "6302661"
          ],
          [
           "source_path",
           "dbfs:/Volumes/satya_share/satyendranath_sure/satya_volume/test/file_str11.0.parquet"
          ],
          [
           "target_path",
           "/Volumes/satya_dr_catalog/satyendranath_sure/satya_volume/test/"
          ]
         ]
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-24_15-42.json",
         "copy",
         "success",
         "File copied successfully.",
         "2025-09-18T21:14:58.823548Z",
         [
          [
           "result",
           "True"
          ],
          [
           "modificationTime",
           "1745534570000"
          ],
          [
           "fileSize",
           "12523"
          ],
          [
           "source_path",
           "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-24_15-42.json"
          ],
          [
           "target_path",
           "/Volumes/dr_eo_amperity/bronze/chuck"
          ]
         ]
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-24_15-28.txt",
         "copy",
         "success",
         "File copied successfully.",
         "2025-09-18T21:14:57.704662Z",
         [
          [
           "result",
           "True"
          ],
          [
           "modificationTime",
           "1745533788000"
          ],
          [
           "fileSize",
           "10424"
          ],
          [
           "source_path",
           "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-24_15-28.txt"
          ],
          [
           "target_path",
           "/Volumes/dr_eo_amperity/bronze/chuck"
          ]
         ]
        ],
        [
         "DR_Volume_data_temp",
         "directory",
         "dbfs:/Volumes/satya_share/satyendranath_sure/satya_volume/test/",
         "list",
         "success",
         "Found directory to traverse.",
         "2025-09-18T21:14:56.78221Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-24_15-23.txt",
         "copy",
         "success",
         "File copied successfully.",
         "2025-09-18T21:14:55.882649Z",
         [
          [
           "result",
           "True"
          ],
          [
           "modificationTime",
           "1745533417000"
          ],
          [
           "fileSize",
           "10397"
          ],
          [
           "source_path",
           "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-24_15-23.txt"
          ],
          [
           "target_path",
           "/Volumes/dr_eo_amperity/bronze/chuck"
          ]
         ]
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/satya_share/satyendranath_sure/satya_volume/file_str13.0.parquet",
         "copy",
         "success",
         "File copied successfully.",
         "2025-09-18T21:14:55.474236Z",
         [
          [
           "result",
           "True"
          ],
          [
           "modificationTime",
           "1758208344000"
          ],
          [
           "fileSize",
           "6403985"
          ],
          [
           "source_path",
           "dbfs:/Volumes/satya_share/satyendranath_sure/satya_volume/file_str13.0.parquet"
          ],
          [
           "target_path",
           "/Volumes/satya_dr_catalog/satyendranath_sure/satya_volume"
          ]
         ]
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/satya_share/satyendranath_sure/satya_external_volume/file_str14.0.parquet",
         "copy",
         "success",
         "File copied successfully.",
         "2025-09-18T21:14:54.991841Z",
         [
          [
           "result",
           "True"
          ],
          [
           "modificationTime",
           "1758207658000"
          ],
          [
           "fileSize",
           "6232030"
          ],
          [
           "source_path",
           "dbfs:/Volumes/satya_share/satyendranath_sure/satya_external_volume/file_str14.0.parquet"
          ],
          [
           "target_path",
           "/Volumes/satya_dr_catalog/satyendranath_sure/satya_external_volume"
          ]
         ]
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-24_15-14.txt",
         "copy",
         "success",
         "File copied successfully.",
         "2025-09-18T21:14:54.590562Z",
         [
          [
           "result",
           "True"
          ],
          [
           "modificationTime",
           "1745532874000"
          ],
          [
           "fileSize",
           "9705"
          ],
          [
           "source_path",
           "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-24_15-14.txt"
          ],
          [
           "target_path",
           "/Volumes/dr_eo_amperity/bronze/chuck"
          ]
         ]
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/satya_share/satyendranath_sure/satya_volume/file_str11.0.parquet",
         "copy",
         "success",
         "File copied successfully.",
         "2025-09-18T21:14:53.707176Z",
         [
          [
           "result",
           "True"
          ],
          [
           "modificationTime",
           "1758227993000"
          ],
          [
           "fileSize",
           "6302661"
          ],
          [
           "source_path",
           "dbfs:/Volumes/satya_share/satyendranath_sure/satya_volume/file_str11.0.parquet"
          ],
          [
           "target_path",
           "/Volumes/satya_dr_catalog/satyendranath_sure/satya_volume"
          ]
         ]
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-24_15-12.json",
         "copy",
         "success",
         "File copied successfully.",
         "2025-09-18T21:14:53.174909Z",
         [
          [
           "result",
           "True"
          ],
          [
           "modificationTime",
           "1745532746000"
          ],
          [
           "fileSize",
           "9705"
          ],
          [
           "source_path",
           "dbfs:/Volumes/_eo_amperity/bronze/chuck/stitch-2025-04-24_15-12.json"
          ],
          [
           "target_path",
           "/Volumes/dr_eo_amperity/bronze/chuck"
          ]
         ]
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/satya_share/satyendranath_sure/satya_external_volume/file_str13.0.parquet",
         "copy",
         "success",
         "File copied successfully.",
         "2025-09-18T21:14:52.743873Z",
         [
          [
           "result",
           "True"
          ],
          [
           "modificationTime",
           "1758207658000"
          ],
          [
           "fileSize",
           "6403985"
          ],
          [
           "source_path",
           "dbfs:/Volumes/satya_share/satyendranath_sure/satya_external_volume/file_str13.0.parquet"
          ],
          [
           "target_path",
           "/Volumes/satya_dr_catalog/satyendranath_sure/satya_external_volume"
          ]
         ]
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/joe-stitch-2025-05-29_10-51.json",
         "copy",
         "success",
         "File copied successfully.",
         "2025-09-18T21:14:51.254898Z",
         [
          [
           "result",
           "True"
          ],
          [
           "modificationTime",
           "1748542172000"
          ],
          [
           "fileSize",
           "4874"
          ],
          [
           "source_path",
           "dbfs:/Volumes/_eo_amperity/bronze/chuck/joe-stitch-2025-05-29_10-51.json"
          ],
          [
           "target_path",
           "/Volumes/dr_eo_amperity/bronze/chuck"
          ]
         ]
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/satya_share/satyendranath_sure/satya_volume/Group_access_revoke.csv",
         "copy",
         "success",
         "File copied successfully.",
         "2025-09-18T21:14:50.788988Z",
         [
          [
           "result",
           "True"
          ],
          [
           "modificationTime",
           "1754924360000"
          ],
          [
           "fileSize",
           "260"
          ],
          [
           "source_path",
           "dbfs:/Volumes/satya_share/satyendranath_sure/satya_volume/Group_access_revoke.csv"
          ],
          [
           "target_path",
           "/Volumes/satya_dr_catalog/satyendranath_sure/satya_volume"
          ]
         ]
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/satya_share/satyendranath_sure/satya_external_volume/file_str11.0.parquet",
         "copy",
         "success",
         "File copied successfully.",
         "2025-09-18T21:14:49.917056Z",
         [
          [
           "result",
           "True"
          ],
          [
           "modificationTime",
           "1758207658000"
          ],
          [
           "fileSize",
           "6302661"
          ],
          [
           "source_path",
           "dbfs:/Volumes/satya_share/satyendranath_sure/satya_external_volume/file_str11.0.parquet"
          ],
          [
           "target_path",
           "/Volumes/satya_dr_catalog/satyendranath_sure/satya_external_volume"
          ]
         ]
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/file.txt",
         "copy",
         "success",
         "File copied successfully.",
         "2025-09-18T21:14:49.403334Z",
         [
          [
           "result",
           "True"
          ],
          [
           "modificationTime",
           "1745523282000"
          ],
          [
           "fileSize",
           "2"
          ],
          [
           "source_path",
           "dbfs:/Volumes/_eo_amperity/bronze/chuck/file.txt"
          ],
          [
           "target_path",
           "/Volumes/dr_eo_amperity/bronze/chuck"
          ]
         ]
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/satya_share/satyendranath_sure/satya_volume/DR FY26 SOP.pdf",
         "copy",
         "success",
         "File copied successfully.",
         "2025-09-18T21:14:48.899447Z",
         [
          [
           "result",
           "True"
          ],
          [
           "modificationTime",
           "1754924251000"
          ],
          [
           "fileSize",
           "3595169"
          ],
          [
           "source_path",
           "dbfs:/Volumes/satya_share/satyendranath_sure/satya_volume/DR FY26 SOP.pdf"
          ],
          [
           "target_path",
           "/Volumes/satya_dr_catalog/satyendranath_sure/satya_volume"
          ]
         ]
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/democracy-manifest.json",
         "copy",
         "success",
         "File copied successfully.",
         "2025-09-18T21:14:48.414612Z",
         [
          [
           "result",
           "True"
          ],
          [
           "modificationTime",
           "1745605748000"
          ],
          [
           "fileSize",
           "1681"
          ],
          [
           "source_path",
           "dbfs:/Volumes/_eo_amperity/bronze/chuck/democracy-manifest.json"
          ],
          [
           "target_path",
           "/Volumes/dr_eo_amperity/bronze/chuck"
          ]
         ]
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/cluster_init.sh",
         "copy",
         "success",
         "File copied successfully.",
         "2025-09-18T21:14:46.879889Z",
         [
          [
           "result",
           "True"
          ],
          [
           "modificationTime",
           "1748370759000"
          ],
          [
           "fileSize",
           "631"
          ],
          [
           "source_path",
           "dbfs:/Volumes/_eo_amperity/bronze/chuck/cluster_init.sh"
          ],
          [
           "target_path",
           "/Volumes/dr_eo_amperity/bronze/chuck"
          ]
         ]
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/cluster_init-2025-06-11_10-46.sh",
         "copy",
         "success",
         "File copied successfully.",
         "2025-09-18T21:14:44.797171Z",
         [
          [
           "result",
           "True"
          ],
          [
           "modificationTime",
           "1749663991000"
          ],
          [
           "fileSize",
           "661"
          ],
          [
           "source_path",
           "dbfs:/Volumes/_eo_amperity/bronze/chuck/cluster_init-2025-06-11_10-46.sh"
          ],
          [
           "target_path",
           "/Volumes/dr_eo_amperity/bronze/chuck"
          ]
         ]
        ],
        [
         "DR_Volume_data_temp",
         "volume",
         "satya_dr_catalog.satyendranath_sure.satya_volume",
         "create",
         "success",
         "Volume created or already exists.",
         "2025-09-18T21:14:44.400831Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "volume",
         "satya_dr_catalog.satyendranath_sure.satya_external_volume",
         "create",
         "success",
         "Volume created or already exists.",
         "2025-09-18T21:14:44.39101Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/cluster_init-2025-06-10_11-26.sh",
         "copy",
         "success",
         "File copied successfully.",
         "2025-09-18T21:14:43.651562Z",
         [
          [
           "result",
           "True"
          ],
          [
           "modificationTime",
           "1749580011000"
          ],
          [
           "fileSize",
           "661"
          ],
          [
           "source_path",
           "dbfs:/Volumes/_eo_amperity/bronze/chuck/cluster_init-2025-06-10_11-26.sh"
          ],
          [
           "target_path",
           "/Volumes/dr_eo_amperity/bronze/chuck"
          ]
         ]
        ],
        [
         "DR_Volume_data_temp",
         "schema",
         "satya_dr_catalog.satyendranath_sure",
         "create",
         "success",
         "Schema `satyendranath_sure` in catalog `satya_dr_catalog` created or already exists.",
         "2025-09-18T21:14:42.676031Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/cluster_init-2025-06-08_18-02.sh",
         "copy",
         "success",
         "File copied successfully.",
         "2025-09-18T21:14:42.032683Z",
         [
          [
           "result",
           "True"
          ],
          [
           "modificationTime",
           "1749430987000"
          ],
          [
           "fileSize",
           "661"
          ],
          [
           "source_path",
           "dbfs:/Volumes/_eo_amperity/bronze/chuck/cluster_init-2025-06-08_18-02.sh"
          ],
          [
           "target_path",
           "/Volumes/dr_eo_amperity/bronze/chuck"
          ]
         ]
        ],
        [
         "DR_Volume_data_temp",
         "schema",
         "satya_share.information_schema",
         "skip",
         "success",
         "Skipping 'information_schema'.",
         "2025-09-18T21:14:41.71179Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/cluster_init-2025-06-08_15-57.sh",
         "copy",
         "success",
         "File copied successfully.",
         "2025-09-18T21:14:40.41959Z",
         [
          [
           "result",
           "True"
          ],
          [
           "modificationTime",
           "1749423479000"
          ],
          [
           "fileSize",
           "661"
          ],
          [
           "source_path",
           "dbfs:/Volumes/_eo_amperity/bronze/chuck/cluster_init-2025-06-08_15-57.sh"
          ],
          [
           "target_path",
           "/Volumes/dr_eo_amperity/bronze/chuck"
          ]
         ]
        ],
        [
         "DR_Volume_data_temp",
         "catalog",
         "satya_dr_catalog",
         "create",
         "success",
         "Catalog `satya_dr_catalog` created or already exists.",
         "2025-09-18T21:14:39.593466Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/cluster_init-2025-06-05_13-39.sh",
         "copy",
         "success",
         "File copied successfully.",
         "2025-09-18T21:14:39.217178Z",
         [
          [
           "result",
           "True"
          ],
          [
           "modificationTime",
           "1749155980000"
          ],
          [
           "fileSize",
           "661"
          ],
          [
           "source_path",
           "dbfs:/Volumes/_eo_amperity/bronze/chuck/cluster_init-2025-06-05_13-39.sh"
          ],
          [
           "target_path",
           "/Volumes/dr_eo_amperity/bronze/chuck"
          ]
         ]
        ],
        [
         "DR_Volume_data_temp",
         "catalog",
         "test_not",
         "process",
         "error",
         "An error occurred while processing catalog pair. Error: [NO_SUCH_CATALOG_EXCEPTION] Catalog 'test_not' was not found. Please verify the catalog name and then retry the query or command again. SQLSTATE: 42704\n\nJVM stacktrace:\norg.apache.spark.sql.catalyst.analysis.NoSuchCatalogException\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$listSchemasProto$1(ManagedCatalogClientImpl.scala:1249)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$recordAndWrapExceptionBase$2(ManagedCatalogClientImpl.scala:7501)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:113)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:159)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:104)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$recordAndWrapExceptionBase$1(ManagedCatalogClientImpl.scala:7500)\n\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:74)\n\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:66)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:268)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapExceptionBase(ManagedCatalogClientImpl.scala:7481)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:7467)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.listSchemasProto(ManagedCatalogClientImpl.scala:1215)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.listSchemas(ManagedCatalogClientImpl.scala:1201)\n\tat com.databricks.sql.managedcatalog.ManagedCatalogCommon.listSchemas(ManagedCatalogCommon.scala:824)\n\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.$anonfun$listSchemas$1(ProfiledManagedCatalog.scala:173)\n\tat org.apache.spark.sql.catalyst.MetricKeyUtils$.measure(MetricKey.scala:1836)\n\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.$anonfun$profile$1(ProfiledManagedCatalog.scala:64)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:113)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:159)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:104)\n\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.profile(ProfiledManagedCatalog.scala:63)\n\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.listSchemas(ProfiledManagedCatalog.scala:173)\n\tat com.databricks.sql.managedcatalog.ManagedCatalogSessionCatalog.listDatabasesWithCatalog(ManagedCatalogSessionCatalog.scala:928)\n\tat com.databricks.sql.managedcatalog.UnityCatalogV2Proxy.listNamespaces(UnityCatalogV2Proxy.scala:141)\n\tat org.apache.spark.sql.execution.command.ShowNamespacesCommand.run(ShowNamespacesCommand.scala:44)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$2(commands.scala:87)\n\tat org.apache.spark.sql.execution.SparkPlan.runCommandInAetherOrSpark(SparkPlan.scala:195)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$1(commands.scala:87)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:113)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:159)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:104)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:84)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:83)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:97)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$5(QueryExecution.scala:507)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$4(QueryExecution.scala:507)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:253)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$3(QueryExecution.scala:506)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$13(SQLExecution.scala:536)\n\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$12(SQLExecution.scala:458)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:834)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$11(SQLExecution.scala:386)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:121)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:115)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:120)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$10(SQLExecution.scala:386)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:863)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:385)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:238)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:787)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:502)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1449)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:498)\n\tat org.apache.spark.sql.execution.QueryExecution.withMVTagsIfNecessary(QueryExecution.scala:418)\n\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:496)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:578)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:570)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:529)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:529)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:361)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:357)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:505)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$8(QueryExecution.scala:570)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:570)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:374)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1686)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1747)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:75)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:379)\n\tat org.apache.spark.sql.classic.Dataset.<init>(Dataset.scala:432)\n\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$3(Dataset.scala:155)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.classic.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1071)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:113)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:159)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:104)\n\tat org.apache.spark.sql.classic.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1071)\n\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:146)\n\tat org.apache.spark.sql.classic.SparkSession.$anonfun$sql$4(SparkSession.scala:850)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.classic.SparkSession.sql(SparkSession.scala:813)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.executeSQL(SparkConnectPlanner.scala:3500)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleSqlCommand(SparkConnectPlanner.scala:3330)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3207)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:385)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:282)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:238)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:466)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:466)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:121)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:115)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:120)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:465)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:238)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$execute$1(ExecuteThreadRunner.scala:141)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries(UtilizationMetrics.scala:43)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries$(UtilizationMetrics.scala:40)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.recordActiveQueries(ExecuteThreadRunner.scala:53)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:139)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:586)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n\tat scala.util.Using$.resource(Using.scala:296)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:586)",
         "2025-09-18T21:14:38.634491Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/cluster_init-2025-06-04_18-14.sh",
         "copy",
         "success",
         "File copied successfully.",
         "2025-09-18T21:14:38.124731Z",
         [
          [
           "result",
           "True"
          ],
          [
           "modificationTime",
           "1749086082000"
          ],
          [
           "fileSize",
           "661"
          ],
          [
           "source_path",
           "dbfs:/Volumes/_eo_amperity/bronze/chuck/cluster_init-2025-06-04_18-14.sh"
          ],
          [
           "target_path",
           "/Volumes/dr_eo_amperity/bronze/chuck"
          ]
         ]
        ],
        [
         "DR_Volume_data_temp",
         "catalog",
         "dr_test_not",
         "create",
         "success",
         "Catalog `dr_test_not` created or already exists.",
         "2025-09-18T21:14:37.667473Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/cluster_init-2025-06-04_17-57.sh",
         "copy",
         "success",
         "File copied successfully.",
         "2025-09-18T21:14:36.895568Z",
         [
          [
           "result",
           "True"
          ],
          [
           "modificationTime",
           "1749085080000"
          ],
          [
           "fileSize",
           "661"
          ],
          [
           "source_path",
           "dbfs:/Volumes/_eo_amperity/bronze/chuck/cluster_init-2025-06-04_17-57.sh"
          ],
          [
           "target_path",
           "/Volumes/dr_eo_amperity/bronze/chuck"
          ]
         ]
        ],
        [
         "DR_Volume_data_temp",
         "schema",
         "accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample.information_schema",
         "skip",
         "success",
         "Skipping 'information_schema'.",
         "2025-09-18T21:14:36.70498Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/cluster_init-2025-06-04_09-48.sh",
         "copy",
         "success",
         "File copied successfully.",
         "2025-09-18T21:14:35.510614Z",
         [
          [
           "result",
           "True"
          ],
          [
           "modificationTime",
           "1749055706000"
          ],
          [
           "fileSize",
           "661"
          ],
          [
           "source_path",
           "dbfs:/Volumes/_eo_amperity/bronze/chuck/cluster_init-2025-06-04_09-48.sh"
          ],
          [
           "target_path",
           "/Volumes/dr_eo_amperity/bronze/chuck"
          ]
         ]
        ],
        [
         "DR_Volume_data_temp",
         "schema",
         "dr_accuweather.forecast",
         "create",
         "success",
         "Schema `forecast` in catalog `dr_accuweather` created or already exists.",
         "2025-09-18T21:14:34.710307Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/cluster_init-2025-06-04_07-39.sh",
         "copy",
         "success",
         "File copied successfully.",
         "2025-09-18T21:14:34.364724Z",
         [
          [
           "result",
           "True"
          ],
          [
           "modificationTime",
           "1749047992000"
          ],
          [
           "fileSize",
           "661"
          ],
          [
           "source_path",
           "dbfs:/Volumes/_eo_amperity/bronze/chuck/cluster_init-2025-06-04_07-39.sh"
          ],
          [
           "target_path",
           "/Volumes/dr_eo_amperity/bronze/chuck"
          ]
         ]
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/cluster_init-2025-06-02_20-37.sh",
         "copy",
         "success",
         "File copied successfully.",
         "2025-09-18T21:14:33.123612Z",
         [
          [
           "result",
           "True"
          ],
          [
           "modificationTime",
           "1748921868000"
          ],
          [
           "fileSize",
           "745"
          ],
          [
           "source_path",
           "dbfs:/Volumes/_eo_amperity/bronze/chuck/cluster_init-2025-06-02_20-37.sh"
          ],
          [
           "target_path",
           "/Volumes/dr_eo_amperity/bronze/chuck"
          ]
         ]
        ],
        [
         "DR_Volume_data_temp",
         "catalog",
         "dr_accuweather",
         "create",
         "success",
         "Catalog `dr_accuweather` created or already exists.",
         "2025-09-18T21:14:33.068882Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "schema",
         "dr_eo_amperity.silver",
         "create",
         "success",
         "Schema `silver` in catalog `dr_eo_amperity` created or already exists.",
         "2025-09-18T21:14:31.714671Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/chinese-meal.json",
         "copy",
         "success",
         "File copied successfully.",
         "2025-09-18T21:14:31.700535Z",
         [
          [
           "result",
           "True"
          ],
          [
           "modificationTime",
           "1745606447000"
          ],
          [
           "fileSize",
           "1323"
          ],
          [
           "source_path",
           "dbfs:/Volumes/_eo_amperity/bronze/chuck/chinese-meal.json"
          ],
          [
           "target_path",
           "/Volumes/dr_eo_amperity/bronze/chuck"
          ]
         ]
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/acme_w_fks.json",
         "copy",
         "success",
         "File copied successfully.",
         "2025-09-18T21:14:30.575023Z",
         [
          [
           "result",
           "True"
          ],
          [
           "modificationTime",
           "1745878932000"
          ],
          [
           "fileSize",
           "9936"
          ],
          [
           "source_path",
           "dbfs:/Volumes/_eo_amperity/bronze/chuck/acme_w_fks.json"
          ],
          [
           "target_path",
           "/Volumes/dr_eo_amperity/bronze/chuck"
          ]
         ]
        ],
        [
         "DR_Volume_data_temp",
         "file",
         "dbfs:/Volumes/_eo_amperity/bronze/chuck/acme.json",
         "copy",
         "success",
         "File copied successfully.",
         "2025-09-18T21:14:28.798117Z",
         [
          [
           "result",
           "True"
          ],
          [
           "modificationTime",
           "1745882459000"
          ],
          [
           "fileSize",
           "9891"
          ],
          [
           "source_path",
           "dbfs:/Volumes/_eo_amperity/bronze/chuck/acme.json"
          ],
          [
           "target_path",
           "/Volumes/dr_eo_amperity/bronze/chuck"
          ]
         ]
        ],
        [
         "DR_Volume_data_temp",
         "schema",
         "_eo_amperity.information_schema",
         "skip",
         "success",
         "Skipping 'information_schema'.",
         "2025-09-18T21:14:28.077174Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "schema",
         "dr_eo_amperity.default",
         "create",
         "success",
         "Schema `default` in catalog `dr_eo_amperity` created or already exists.",
         "2025-09-18T21:14:26.535566Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "volume",
         "dr_eo_amperity.bronze.chuck",
         "create",
         "success",
         "Volume created or already exists.",
         "2025-09-18T21:14:26.496356Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "schema",
         "dr_eo_amperity.bronze",
         "create",
         "success",
         "Schema `bronze` in catalog `dr_eo_amperity` created or already exists.",
         "2025-09-18T21:14:25.107757Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "catalog",
         "dr_eo_amperity",
         "create",
         "success",
         "Catalog `dr_eo_amperity` created or already exists.",
         "2025-09-18T21:14:23.76101Z",
         []
        ],
        [
         "DR_Volume_data_temp",
         "info",
         "N/A",
         "start_sync",
         "success",
         "Starting Databricks Volume DR Sync.",
         "2025-09-18T21:14:22.650586Z",
         []
        ],
        [
         "DR_Model_data",
         "schema",
         "`satya_dr_catalog`.`satyendranath_sure`",
         "create",
         "success",
         "Schema `satya_dr_catalog`.`satyendranath_sure` created or already exists.",
         "2025-08-30T15:08:52.504525Z",
         []
        ],
        [
         "DR_Model_data",
         "catalog",
         "`satya_dr_catalog`",
         "create",
         "success",
         "Catalog `satya_dr_catalog` created or already exists.",
         "2025-08-30T15:08:50.532938Z",
         []
        ],
        [
         "DR_Model_data",
         "catalog_pair",
         "Processing of test_not:dr_test_not",
         "unhandled_error",
         "error",
         "An unhandled error occurred: [NO_SUCH_CATALOG_EXCEPTION] Catalog 'test_not' was not found. Please verify the catalog name and then retry the query or command again. SQLSTATE: 42704\n\nJVM stacktrace:\norg.apache.spark.sql.catalyst.analysis.NoSuchCatalogException\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$listSchemasProto$1(ManagedCatalogClientImpl.scala:1328)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$recordAndWrapExceptionBase$2(ManagedCatalogClientImpl.scala:7398)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$recordAndWrapExceptionBase$1(ManagedCatalogClientImpl.scala:7397)\n\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:66)\n\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:63)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:251)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapExceptionBase(ManagedCatalogClientImpl.scala:7378)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:7364)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.listSchemasProto(ManagedCatalogClientImpl.scala:1293)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.listSchemas(ManagedCatalogClientImpl.scala:1279)\n\tat com.databricks.sql.managedcatalog.ManagedCatalogCommon.listSchemas(ManagedCatalogCommon.scala:827)\n\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.$anonfun$listSchemas$1(ProfiledManagedCatalog.scala:168)\n\tat org.apache.spark.sql.catalyst.MetricKeyUtils$.measure(MetricKey.scala:1709)\n\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.$anonfun$profile$1(ProfiledManagedCatalog.scala:64)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.profile(ProfiledManagedCatalog.scala:63)\n\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.listSchemas(ProfiledManagedCatalog.scala:168)\n\tat com.databricks.sql.managedcatalog.ManagedCatalogSessionCatalog.listDatabasesWithCatalog(ManagedCatalogSessionCatalog.scala:908)\n\tat com.databricks.sql.managedcatalog.UnityCatalogV2Proxy.listNamespaces(UnityCatalogV2Proxy.scala:141)\n\tat org.apache.spark.sql.execution.datasources.v2.ShowNamespacesExec.run(ShowNamespacesExec.scala:42)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.$anonfun$result$2(V2CommandExec.scala:48)\n\tat org.apache.spark.sql.execution.SparkPlan.runCommandInAetherOrSpark(SparkPlan.scala:194)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.$anonfun$result$1(V2CommandExec.scala:48)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:47)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:45)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:56)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$5(QueryExecution.scala:469)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$4(QueryExecution.scala:469)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:219)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$3(QueryExecution.scala:468)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$11(SQLExecution.scala:479)\n\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$10(SQLExecution.scala:415)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:779)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:345)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:214)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:716)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:464)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1342)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:460)\n\tat org.apache.spark.sql.execution.QueryExecution.withMVTagsIfNecessary(QueryExecution.scala:389)\n\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:458)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:537)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:532)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:521)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:521)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:361)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:357)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:497)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$8(QueryExecution.scala:532)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:532)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:345)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:350)\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:406)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$3(Dataset.scala:150)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n\tat org.apache.spark.sql.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1469)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1469)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:141)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:1142)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:1094)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.executeSQL(SparkConnectPlanner.scala:3606)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleSqlCommand(SparkConnectPlanner.scala:3435)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3370)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:413)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:312)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:233)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:464)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:464)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:90)\n\tat org.apache.spark.util.Utils$.withContextClassLoader(Utils.scala:241)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:89)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:463)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:233)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:139)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:614)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n\tat scala.util.Using$.resource(Using.scala:269)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:614)\nTraceback (most recent call last):\n  File \"/home/spark-fc8c1a42-477f-476e-91f5-48/.ipykernel/1920/command-8748648698932923-2826314312\", line 295, in main\n    source_schemas_df = spark.sql(f\"SHOW SCHEMAS IN `{source_catalog}`\")\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/databricks/spark/python/pyspark/sql/connect/session.py\", line 821, in sql\n    data, properties, ei = self.client.execute_command(cmd.command(self._client))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/databricks/spark/python/pyspark/sql/connect/client/core.py\", line 1481, in execute_command\n    data, _, metrics, observed_metrics, properties = self._execute_and_fetch(\n                                                     ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/databricks/spark/python/pyspark/sql/connect/client/core.py\", line 1970, in _execute_and_fetch\n    for response in self._execute_and_fetch_as_iterator(\n  File \"/databricks/spark/python/pyspark/sql/connect/client/core.py\", line 1946, in _execute_and_fetch_as_iterator\n    self._handle_error(error)\n  File \"/databricks/spark/python/pyspark/sql/connect/client/core.py\", line 2266, in _handle_error\n    self._handle_rpc_error(error)\n  File \"/databricks/spark/python/pyspark/sql/connect/client/core.py\", line 2347, in _handle_rpc_error\n    raise convert_exception(\npyspark.errors.exceptions.connect.AnalysisException: [NO_SUCH_CATALOG_EXCEPTION] Catalog 'test_not' was not found. Please verify the catalog name and then retry the query or command again. SQLSTATE: 42704\n\nJVM stacktrace:\norg.apache.spark.sql.catalyst.analysis.NoSuchCatalogException\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$listSchemasProto$1(ManagedCatalogClientImpl.scala:1328)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$recordAndWrapExceptionBase$2(ManagedCatalogClientImpl.scala:7398)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$recordAndWrapExceptionBase$1(ManagedCatalogClientImpl.scala:7397)\n\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:66)\n\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:63)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:251)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapExceptionBase(ManagedCatalogClientImpl.scala:7378)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:7364)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.listSchemasProto(ManagedCatalogClientImpl.scala:1293)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.listSchemas(ManagedCatalogClientImpl.scala:1279)\n\tat com.databricks.sql.managedcatalog.ManagedCatalogCommon.listSchemas(ManagedCatalogCommon.scala:827)\n\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.$anonfun$listSchemas$1(ProfiledManagedCatalog.scala:168)\n\tat org.apache.spark.sql.catalyst.MetricKeyUtils$.measure(MetricKey.scala:1709)\n\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.$anonfun$profile$1(ProfiledManagedCatalog.scala:64)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.profile(ProfiledManagedCatalog.scala:63)\n\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.listSchemas(ProfiledManagedCatalog.scala:168)\n\tat com.databricks.sql.managedcatalog.ManagedCatalogSessionCatalog.listDatabasesWithCatalog(ManagedCatalogSessionCatalog.scala:908)\n\tat com.databricks.sql.managedcatalog.UnityCatalogV2Proxy.listNamespaces(UnityCatalogV2Proxy.scala:141)\n\tat org.apache.spark.sql.execution.datasources.v2.ShowNamespacesExec.run(ShowNamespacesExec.scala:42)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.$anonfun$result$2(V2CommandExec.scala:48)\n\tat org.apache.spark.sql.execution.SparkPlan.runCommandInAetherOrSpark(SparkPlan.scala:194)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.$anonfun$result$1(V2CommandExec.scala:48)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:47)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:45)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:56)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$5(QueryExecution.scala:469)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$4(QueryExecution.scala:469)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:219)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$3(QueryExecution.scala:468)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$11(SQLExecution.scala:479)\n\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$10(SQLExecution.scala:415)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:779)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:345)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:214)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:716)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:464)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1342)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:460)\n\tat org.apache.spark.sql.execution.QueryExecution.withMVTagsIfNecessary(QueryExecution.scala:389)\n\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:458)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:537)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:532)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:521)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:521)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:361)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:357)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:497)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$8(QueryExecution.scala:532)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:532)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:345)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:350)\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:406)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$3(Dataset.scala:150)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n\tat org.apache.spark.sql.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1469)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1469)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:141)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:1142)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:1094)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.executeSQL(SparkConnectPlanner.scala:3606)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleSqlCommand(SparkConnectPlanner.scala:3435)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3370)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:413)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:312)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:233)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:464)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:464)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:90)\n\tat org.apache.spark.util.Utils$.withContextClassLoader(Utils.scala:241)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:89)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:463)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:233)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:139)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:614)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n\tat scala.util.Using$.resource(Using.scala:269)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:614)\n",
         "2025-08-30T15:08:49.180449Z",
         []
        ],
        [
         "DR_Model_data",
         "catalog",
         "`dr_test_not`",
         "create",
         "success",
         "Catalog `dr_test_not` created or already exists.",
         "2025-08-30T15:08:47.839358Z",
         []
        ],
        [
         "DR_Model_data",
         "schema",
         "`dr_accuweather`.`forecast`",
         "create",
         "success",
         "Schema `dr_accuweather`.`forecast` created or already exists.",
         "2025-08-30T15:08:46.45627Z",
         []
        ],
        [
         "DR_Model_data",
         "catalog",
         "`dr_accuweather`",
         "create",
         "success",
         "Catalog `dr_accuweather` created or already exists.",
         "2025-08-30T15:08:33.118839Z",
         []
        ],
        [
         "DR_Model_data",
         "schema",
         "`dr_eo_amperity`.`silver`",
         "create",
         "success",
         "Schema `dr_eo_amperity`.`silver` created or already exists.",
         "2025-08-30T15:08:31.722105Z",
         []
        ],
        [
         "DR_Model_data",
         "schema",
         "`dr_eo_amperity`.`default`",
         "create",
         "success",
         "Schema `dr_eo_amperity`.`default` created or already exists.",
         "2025-08-30T15:08:30.330966Z",
         []
        ],
        [
         "DR_Model_data",
         "schema",
         "`dr_eo_amperity`.`bronze`",
         "create",
         "success",
         "Schema `dr_eo_amperity`.`bronze` created or already exists.",
         "2025-08-30T15:08:28.698182Z",
         []
        ],
        [
         "DR_Model_data",
         "catalog",
         "`dr_eo_amperity`",
         "create",
         "success",
         "Catalog `dr_eo_amperity` created or already exists.",
         "2025-08-30T15:08:04.983538Z",
         []
        ],
        [
         "DR_Model_data",
         "schema",
         "`satya_dr_catalog`.`satyendranath_sure`",
         "create",
         "success",
         "Schema `satya_dr_catalog`.`satyendranath_sure` created or already exists.",
         "2025-08-29T18:34:43.994454Z",
         []
        ],
        [
         "DR_Model_data",
         "catalog",
         "`satya_dr_catalog`",
         "create",
         "success",
         "Catalog `satya_dr_catalog` created or already exists.",
         "2025-08-29T18:34:41.963747Z",
         []
        ],
        [
         "DR_Model_data",
         "catalog_pair",
         "Processing of test_not:dr_test_not",
         "unhandled_error",
         "error",
         "An unhandled error occurred: [NO_SUCH_CATALOG_EXCEPTION] Catalog 'test_not' was not found. Please verify the catalog name and then retry the query or command again. SQLSTATE: 42704\n\nJVM stacktrace:\norg.apache.spark.sql.catalyst.analysis.NoSuchCatalogException\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$listSchemasProto$1(ManagedCatalogClientImpl.scala:1328)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$recordAndWrapExceptionBase$2(ManagedCatalogClientImpl.scala:7398)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$recordAndWrapExceptionBase$1(ManagedCatalogClientImpl.scala:7397)\n\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:66)\n\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:63)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:251)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapExceptionBase(ManagedCatalogClientImpl.scala:7378)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:7364)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.listSchemasProto(ManagedCatalogClientImpl.scala:1293)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.listSchemas(ManagedCatalogClientImpl.scala:1279)\n\tat com.databricks.sql.managedcatalog.ManagedCatalogCommon.listSchemas(ManagedCatalogCommon.scala:827)\n\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.$anonfun$listSchemas$1(ProfiledManagedCatalog.scala:168)\n\tat org.apache.spark.sql.catalyst.MetricKeyUtils$.measure(MetricKey.scala:1709)\n\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.$anonfun$profile$1(ProfiledManagedCatalog.scala:64)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.profile(ProfiledManagedCatalog.scala:63)\n\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.listSchemas(ProfiledManagedCatalog.scala:168)\n\tat com.databricks.sql.managedcatalog.ManagedCatalogSessionCatalog.listDatabasesWithCatalog(ManagedCatalogSessionCatalog.scala:908)\n\tat com.databricks.sql.managedcatalog.UnityCatalogV2Proxy.listNamespaces(UnityCatalogV2Proxy.scala:141)\n\tat org.apache.spark.sql.execution.datasources.v2.ShowNamespacesExec.run(ShowNamespacesExec.scala:42)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.$anonfun$result$2(V2CommandExec.scala:48)\n\tat org.apache.spark.sql.execution.SparkPlan.runCommandInAetherOrSpark(SparkPlan.scala:194)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.$anonfun$result$1(V2CommandExec.scala:48)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:47)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:45)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:56)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$5(QueryExecution.scala:469)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$4(QueryExecution.scala:469)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:219)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$3(QueryExecution.scala:468)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$11(SQLExecution.scala:479)\n\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$10(SQLExecution.scala:415)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:779)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:345)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:214)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:716)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:464)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1342)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:460)\n\tat org.apache.spark.sql.execution.QueryExecution.withMVTagsIfNecessary(QueryExecution.scala:389)\n\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:458)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:537)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:532)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:521)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:521)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:361)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:357)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:497)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$8(QueryExecution.scala:532)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:532)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:345)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:350)\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:406)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$3(Dataset.scala:150)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n\tat org.apache.spark.sql.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1469)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1469)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:141)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:1142)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:1094)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.executeSQL(SparkConnectPlanner.scala:3606)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleSqlCommand(SparkConnectPlanner.scala:3435)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3370)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:413)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:312)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:233)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:464)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:464)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:90)\n\tat org.apache.spark.util.Utils$.withContextClassLoader(Utils.scala:241)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:89)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:463)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:233)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:139)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:614)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n\tat scala.util.Using$.resource(Using.scala:269)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:614)\nTraceback (most recent call last):\n  File \"/home/spark-45bdfff6-00b1-4911-af55-ac/.ipykernel/1943/command-8748648698932923-2826314312\", line 295, in main\n    source_schemas_df = spark.sql(f\"SHOW SCHEMAS IN `{source_catalog}`\")\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/databricks/spark/python/pyspark/sql/connect/session.py\", line 821, in sql\n    data, properties, ei = self.client.execute_command(cmd.command(self._client))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/databricks/spark/python/pyspark/sql/connect/client/core.py\", line 1481, in execute_command\n    data, _, metrics, observed_metrics, properties = self._execute_and_fetch(\n                                                     ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/databricks/spark/python/pyspark/sql/connect/client/core.py\", line 1970, in _execute_and_fetch\n    for response in self._execute_and_fetch_as_iterator(\n  File \"/databricks/spark/python/pyspark/sql/connect/client/core.py\", line 1946, in _execute_and_fetch_as_iterator\n    self._handle_error(error)\n  File \"/databricks/spark/python/pyspark/sql/connect/client/core.py\", line 2266, in _handle_error\n    self._handle_rpc_error(error)\n  File \"/databricks/spark/python/pyspark/sql/connect/client/core.py\", line 2347, in _handle_rpc_error\n    raise convert_exception(\npyspark.errors.exceptions.connect.AnalysisException: [NO_SUCH_CATALOG_EXCEPTION] Catalog 'test_not' was not found. Please verify the catalog name and then retry the query or command again. SQLSTATE: 42704\n\nJVM stacktrace:\norg.apache.spark.sql.catalyst.analysis.NoSuchCatalogException\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$listSchemasProto$1(ManagedCatalogClientImpl.scala:1328)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$recordAndWrapExceptionBase$2(ManagedCatalogClientImpl.scala:7398)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$recordAndWrapExceptionBase$1(ManagedCatalogClientImpl.scala:7397)\n\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:66)\n\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:63)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:251)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapExceptionBase(ManagedCatalogClientImpl.scala:7378)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:7364)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.listSchemasProto(ManagedCatalogClientImpl.scala:1293)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.listSchemas(ManagedCatalogClientImpl.scala:1279)\n\tat com.databricks.sql.managedcatalog.ManagedCatalogCommon.listSchemas(ManagedCatalogCommon.scala:827)\n\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.$anonfun$listSchemas$1(ProfiledManagedCatalog.scala:168)\n\tat org.apache.spark.sql.catalyst.MetricKeyUtils$.measure(MetricKey.scala:1709)\n\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.$anonfun$profile$1(ProfiledManagedCatalog.scala:64)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.profile(ProfiledManagedCatalog.scala:63)\n\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.listSchemas(ProfiledManagedCatalog.scala:168)\n\tat com.databricks.sql.managedcatalog.ManagedCatalogSessionCatalog.listDatabasesWithCatalog(ManagedCatalogSessionCatalog.scala:908)\n\tat com.databricks.sql.managedcatalog.UnityCatalogV2Proxy.listNamespaces(UnityCatalogV2Proxy.scala:141)\n\tat org.apache.spark.sql.execution.datasources.v2.ShowNamespacesExec.run(ShowNamespacesExec.scala:42)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.$anonfun$result$2(V2CommandExec.scala:48)\n\tat org.apache.spark.sql.execution.SparkPlan.runCommandInAetherOrSpark(SparkPlan.scala:194)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.$anonfun$result$1(V2CommandExec.scala:48)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:47)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:45)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:56)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$5(QueryExecution.scala:469)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$4(QueryExecution.scala:469)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:219)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$3(QueryExecution.scala:468)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$11(SQLExecution.scala:479)\n\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$10(SQLExecution.scala:415)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:779)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:345)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:214)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:716)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:464)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1342)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:460)\n\tat org.apache.spark.sql.execution.QueryExecution.withMVTagsIfNecessary(QueryExecution.scala:389)\n\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:458)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:537)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:532)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:521)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:521)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:361)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:357)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:497)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$8(QueryExecution.scala:532)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:532)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:345)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:350)\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:406)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$3(Dataset.scala:150)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n\tat org.apache.spark.sql.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1469)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1469)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:141)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:1142)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:1094)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.executeSQL(SparkConnectPlanner.scala:3606)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleSqlCommand(SparkConnectPlanner.scala:3435)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3370)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:413)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:312)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:233)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:464)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:464)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:90)\n\tat org.apache.spark.util.Utils$.withContextClassLoader(Utils.scala:241)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:89)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:463)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:233)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:139)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:614)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n\tat scala.util.Using$.resource(Using.scala:269)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:614)\n",
         "2025-08-29T18:34:40.618934Z",
         []
        ],
        [
         "DR_Model_data",
         "catalog",
         "`dr_test_not`",
         "create",
         "success",
         "Catalog `dr_test_not` created or already exists.",
         "2025-08-29T18:34:38.912022Z",
         []
        ],
        [
         "DR_Model_data",
         "schema",
         "`dr_accuweather`.`forecast`",
         "create",
         "success",
         "Schema `dr_accuweather`.`forecast` created or already exists.",
         "2025-08-29T18:34:37.40092Z",
         []
        ],
        [
         "DR_Model_data",
         "catalog",
         "`dr_accuweather`",
         "create",
         "success",
         "Catalog `dr_accuweather` created or already exists.",
         "2025-08-29T18:34:24.325473Z",
         []
        ],
        [
         "DR_Model_data",
         "schema",
         "`dr_eo_amperity`.`silver`",
         "create",
         "success",
         "Schema `dr_eo_amperity`.`silver` created or already exists.",
         "2025-08-29T18:34:22.782756Z",
         []
        ],
        [
         "DR_Model_data",
         "schema",
         "`dr_eo_amperity`.`default`",
         "create",
         "success",
         "Schema `dr_eo_amperity`.`default` created or already exists.",
         "2025-08-29T18:34:21.270727Z",
         []
        ],
        [
         "DR_Model_data",
         "schema",
         "`dr_eo_amperity`.`bronze`",
         "create",
         "success",
         "Schema `dr_eo_amperity`.`bronze` created or already exists.",
         "2025-08-29T18:34:19.658793Z",
         []
        ],
        [
         "DR_Model_data",
         "catalog",
         "`dr_eo_amperity`",
         "create",
         "success",
         "Catalog `dr_eo_amperity` created or already exists.",
         "2025-08-29T18:33:57.274619Z",
         []
        ],
        [
         "DR_Model_data",
         "schema",
         "`satya_dr_catalog`.`satyendranath_sure`",
         "create",
         "success",
         "Schema `satya_dr_catalog`.`satyendranath_sure` created or already exists.",
         "2025-08-20T15:48:14.916583Z",
         []
        ],
        [
         "DR_Model_data",
         "catalog",
         "`satya_dr_catalog`",
         "create",
         "success",
         "Catalog `satya_dr_catalog` created or already exists.",
         "2025-08-20T15:48:12.815245Z",
         []
        ],
        [
         "DR_Model_data",
         "catalog_pair",
         "Processing of test_not:dr_test_not",
         "unhandled_error",
         "error",
         "An unhandled error occurred: [NO_SUCH_CATALOG_EXCEPTION] Catalog 'test_not' was not found. Please verify the catalog name and then retry the query or command again. SQLSTATE: 42704\n\nJVM stacktrace:\norg.apache.spark.sql.catalyst.analysis.NoSuchCatalogException\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$listSchemasProto$1(ManagedCatalogClientImpl.scala:1328)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$recordAndWrapExceptionBase$2(ManagedCatalogClientImpl.scala:7398)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$recordAndWrapExceptionBase$1(ManagedCatalogClientImpl.scala:7397)\n\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:66)\n\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:63)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:251)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapExceptionBase(ManagedCatalogClientImpl.scala:7378)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:7364)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.listSchemasProto(ManagedCatalogClientImpl.scala:1293)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.listSchemas(ManagedCatalogClientImpl.scala:1279)\n\tat com.databricks.sql.managedcatalog.ManagedCatalogCommon.listSchemas(ManagedCatalogCommon.scala:821)\n\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.$anonfun$listSchemas$1(ProfiledManagedCatalog.scala:168)\n\tat org.apache.spark.sql.catalyst.MetricKeyUtils$.measure(MetricKey.scala:1688)\n\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.$anonfun$profile$1(ProfiledManagedCatalog.scala:64)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.profile(ProfiledManagedCatalog.scala:63)\n\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.listSchemas(ProfiledManagedCatalog.scala:168)\n\tat com.databricks.sql.managedcatalog.ManagedCatalogSessionCatalog.listDatabasesWithCatalog(ManagedCatalogSessionCatalog.scala:908)\n\tat com.databricks.sql.managedcatalog.UnityCatalogV2Proxy.listNamespaces(UnityCatalogV2Proxy.scala:141)\n\tat org.apache.spark.sql.execution.datasources.v2.ShowNamespacesExec.run(ShowNamespacesExec.scala:42)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.$anonfun$result$2(V2CommandExec.scala:48)\n\tat org.apache.spark.sql.execution.SparkPlan.runCommandInAetherOrSpark(SparkPlan.scala:194)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.$anonfun$result$1(V2CommandExec.scala:48)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:47)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:45)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:56)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$5(QueryExecution.scala:450)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$4(QueryExecution.scala:450)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:219)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$3(QueryExecution.scala:449)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$11(SQLExecution.scala:478)\n\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$10(SQLExecution.scala:414)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:777)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:344)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:213)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:714)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:445)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1307)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:441)\n\tat org.apache.spark.sql.execution.QueryExecution.withMVTagsIfNecessary(QueryExecution.scala:370)\n\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:439)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:503)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:498)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:521)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:521)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:361)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:357)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:497)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$8(QueryExecution.scala:498)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:498)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:326)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:331)\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:406)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$3(Dataset.scala:150)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n\tat org.apache.spark.sql.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1469)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1469)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:141)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:1142)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:1094)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.executeSQL(SparkConnectPlanner.scala:3606)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleSqlCommand(SparkConnectPlanner.scala:3435)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3370)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:413)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:312)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:233)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:464)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:464)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:90)\n\tat org.apache.spark.util.Utils$.withContextClassLoader(Utils.scala:241)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:89)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:463)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:233)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:139)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:614)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n\tat scala.util.Using$.resource(Using.scala:269)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:614)\nTraceback (most recent call last):\n  File \"/home/spark-99074ce5-7219-4ba1-80ca-d5/.ipykernel/2410/command-8748648698932923-2826314312\", line 295, in main\n    source_schemas_df = spark.sql(f\"SHOW SCHEMAS IN `{source_catalog}`\")\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/databricks/spark/python/pyspark/sql/connect/session.py\", line 821, in sql\n    data, properties, ei = self.client.execute_command(cmd.command(self._client))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/databricks/spark/python/pyspark/sql/connect/client/core.py\", line 1481, in execute_command\n    data, _, metrics, observed_metrics, properties = self._execute_and_fetch(\n                                                     ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/databricks/spark/python/pyspark/sql/connect/client/core.py\", line 1970, in _execute_and_fetch\n    for response in self._execute_and_fetch_as_iterator(\n  File \"/databricks/spark/python/pyspark/sql/connect/client/core.py\", line 1946, in _execute_and_fetch_as_iterator\n    self._handle_error(error)\n  File \"/databricks/spark/python/pyspark/sql/connect/client/core.py\", line 2266, in _handle_error\n    self._handle_rpc_error(error)\n  File \"/databricks/spark/python/pyspark/sql/connect/client/core.py\", line 2377, in _handle_rpc_error\n    raise convert_exception(\npyspark.errors.exceptions.connect.AnalysisException: [NO_SUCH_CATALOG_EXCEPTION] Catalog 'test_not' was not found. Please verify the catalog name and then retry the query or command again. SQLSTATE: 42704\n\nJVM stacktrace:\norg.apache.spark.sql.catalyst.analysis.NoSuchCatalogException\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$listSchemasProto$1(ManagedCatalogClientImpl.scala:1328)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$recordAndWrapExceptionBase$2(ManagedCatalogClientImpl.scala:7398)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$recordAndWrapExceptionBase$1(ManagedCatalogClientImpl.scala:7397)\n\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:66)\n\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:63)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:251)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapExceptionBase(ManagedCatalogClientImpl.scala:7378)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:7364)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.listSchemasProto(ManagedCatalogClientImpl.scala:1293)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.listSchemas(ManagedCatalogClientImpl.scala:1279)\n\tat com.databricks.sql.managedcatalog.ManagedCatalogCommon.listSchemas(ManagedCatalogCommon.scala:821)\n\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.$anonfun$listSchemas$1(ProfiledManagedCatalog.scala:168)\n\tat org.apache.spark.sql.catalyst.MetricKeyUtils$.measure(MetricKey.scala:1688)\n\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.$anonfun$profile$1(ProfiledManagedCatalog.scala:64)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.profile(ProfiledManagedCatalog.scala:63)\n\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.listSchemas(ProfiledManagedCatalog.scala:168)\n\tat com.databricks.sql.managedcatalog.ManagedCatalogSessionCatalog.listDatabasesWithCatalog(ManagedCatalogSessionCatalog.scala:908)\n\tat com.databricks.sql.managedcatalog.UnityCatalogV2Proxy.listNamespaces(UnityCatalogV2Proxy.scala:141)\n\tat org.apache.spark.sql.execution.datasources.v2.ShowNamespacesExec.run(ShowNamespacesExec.scala:42)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.$anonfun$result$2(V2CommandExec.scala:48)\n\tat org.apache.spark.sql.execution.SparkPlan.runCommandInAetherOrSpark(SparkPlan.scala:194)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.$anonfun$result$1(V2CommandExec.scala:48)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:47)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:45)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:56)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$5(QueryExecution.scala:450)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$4(QueryExecution.scala:450)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:219)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$3(QueryExecution.scala:449)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$11(SQLExecution.scala:478)\n\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$10(SQLExecution.scala:414)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:777)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:344)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:213)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:714)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:445)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1307)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:441)\n\tat org.apache.spark.sql.execution.QueryExecution.withMVTagsIfNecessary(QueryExecution.scala:370)\n\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:439)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:503)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:498)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:521)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:521)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:361)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:357)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:497)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$8(QueryExecution.scala:498)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:498)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:326)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:331)\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:406)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$3(Dataset.scala:150)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n\tat org.apache.spark.sql.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1469)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1469)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:141)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:1142)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:1094)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.executeSQL(SparkConnectPlanner.scala:3606)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleSqlCommand(SparkConnectPlanner.scala:3435)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3370)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:413)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:312)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:233)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:464)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:464)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:90)\n\tat org.apache.spark.util.Utils$.withContextClassLoader(Utils.scala:241)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:89)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:463)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:233)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:139)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:614)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n\tat scala.util.Using$.resource(Using.scala:269)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:614)\n",
         "2025-08-20T15:48:11.398218Z",
         []
        ],
        [
         "DR_Model_data",
         "catalog",
         "`dr_test_not`",
         "create",
         "success",
         "Catalog `dr_test_not` created or already exists.",
         "2025-08-20T15:48:10.072018Z",
         []
        ],
        [
         "DR_Model_data",
         "schema",
         "`dr_accuweather`.`forecast`",
         "create",
         "success",
         "Schema `dr_accuweather`.`forecast` created or already exists.",
         "2025-08-20T15:48:08.592083Z",
         []
        ],
        [
         "DR_Model_data",
         "catalog",
         "`dr_accuweather`",
         "create",
         "success",
         "Catalog `dr_accuweather` created or already exists.",
         "2025-08-20T15:48:06.657797Z",
         []
        ],
        [
         "DR_Model_data",
         "schema",
         "`dr_eo_amperity`.`silver`",
         "create",
         "success",
         "Schema `dr_eo_amperity`.`silver` created or already exists.",
         "2025-08-20T15:48:05.019282Z",
         []
        ],
        [
         "DR_Model_data",
         "schema",
         "`dr_eo_amperity`.`default`",
         "create",
         "success",
         "Schema `dr_eo_amperity`.`default` created or already exists.",
         "2025-08-20T15:48:03.382008Z",
         []
        ],
        [
         "DR_Model_data",
         "schema",
         "`dr_eo_amperity`.`bronze`",
         "create",
         "success",
         "Schema `dr_eo_amperity`.`bronze` created or already exists.",
         "2025-08-20T15:48:01.785224Z",
         []
        ],
        [
         "DR_Model_data",
         "catalog",
         "`dr_eo_amperity`",
         "create",
         "success",
         "Catalog `dr_eo_amperity` created or already exists.",
         "2025-08-20T15:47:59.541252Z",
         []
        ],
        [
         "DR_Model_data",
         "model_alias",
         "satya_dr_catalog.satyendranath_sure.satya_model/2/ver2",
         "set",
         "success",
         "Successfully set alias 'ver2' on model version 2.",
         "2025-08-20T15:35:49.381423Z",
         [
          [
           "model_name",
           "satya_dr_catalog.satyendranath_sure.satya_model"
          ],
          [
           "version",
           "2"
          ],
          [
           "alias",
           "ver2"
          ]
         ]
        ],
        [
         "DR_Model_data",
         "model_version",
         "satya_dr_catalog.satyendranath_sure.satya_model/2",
         "copy",
         "success",
         "Successfully copied model version 2 from satya_share.satyendranath_sure.satya_model to satya_dr_catalog.satyendranath_sure.satya_model.",
         "2025-08-20T15:35:47.627569Z",
         [
          [
           "copy_status",
           "success"
          ],
          [
           "source_uri",
           "models:/satya_share.satyendranath_sure.satya_model/2"
          ],
          [
           "target_uri",
           "satya_dr_catalog.satyendranath_sure.satya_model"
          ],
          [
           "version_copied",
           "2"
          ],
          [
           "version_result",
           "<ModelVersion: aliases=[], creation_timestamp=1755704146673, current_stage=None, description='Comment2', last_updated_timestamp=1755704147398, name='satya_dr_catalog.satyendranath_sure.satya_model', run_id='', run_link=None, source='models:/satya_share.satyendranath_sure.satya_model/2', status='READY', status_message='', tags={}, user_id='satyendranath.sure@databricks.com', version='2'>"
          ]
         ]
        ],
        [
         "DR_Model_data",
         "model_alias",
         "satya_dr_catalog.satyendranath_sure.satya_model/1/ver1",
         "set",
         "success",
         "Successfully set alias 'ver1' on model version 1.",
         "2025-08-20T15:35:42.192633Z",
         [
          [
           "model_name",
           "satya_dr_catalog.satyendranath_sure.satya_model"
          ],
          [
           "version",
           "1"
          ],
          [
           "alias",
           "ver1"
          ]
         ]
        ],
        [
         "DR_Model_data",
         "model_version",
         "satya_dr_catalog.satyendranath_sure.satya_model/1",
         "copy",
         "success",
         "Successfully copied model version 1 from satya_share.satyendranath_sure.satya_model to satya_dr_catalog.satyendranath_sure.satya_model.",
         "2025-08-20T15:35:40.299971Z",
         [
          [
           "copy_status",
           "success"
          ],
          [
           "source_uri",
           "models:/satya_share.satyendranath_sure.satya_model/1"
          ],
          [
           "target_uri",
           "satya_dr_catalog.satyendranath_sure.satya_model"
          ],
          [
           "version_copied",
           "1"
          ],
          [
           "version_result",
           "<ModelVersion: aliases=[], creation_timestamp=1755704138700, current_stage=None, description='Comment1', last_updated_timestamp=1755704140051, name='satya_dr_catalog.satyendranath_sure.satya_model', run_id='', run_link=None, source='models:/satya_share.satyendranath_sure.satya_model/1', status='READY', status_message='', tags={}, user_id='satyendranath.sure@databricks.com', version='1'>"
          ]
         ]
        ],
        [
         "DR_Model_data",
         "schema",
         "`satya_dr_catalog`.`satyendranath_sure`",
         "create",
         "success",
         "Schema `satya_dr_catalog`.`satyendranath_sure` created or already exists.",
         "2025-08-20T15:35:32.68967Z",
         []
        ],
        [
         "DR_Model_data",
         "catalog",
         "`satya_dr_catalog`",
         "create",
         "success",
         "Catalog `satya_dr_catalog` created or already exists.",
         "2025-08-20T15:35:30.217706Z",
         []
        ],
        [
         "DR_Model_data",
         "catalog_pair",
         "Processing of test_not:dr_test_not",
         "unhandled_error",
         "error",
         "An unhandled error occurred: [NO_SUCH_CATALOG_EXCEPTION] Catalog 'test_not' was not found. Please verify the catalog name and then retry the query or command again. SQLSTATE: 42704\n\nJVM stacktrace:\norg.apache.spark.sql.catalyst.analysis.NoSuchCatalogException\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$listSchemasProto$1(ManagedCatalogClientImpl.scala:1328)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$recordAndWrapExceptionBase$2(ManagedCatalogClientImpl.scala:7398)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$recordAndWrapExceptionBase$1(ManagedCatalogClientImpl.scala:7397)\n\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:66)\n\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:63)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:251)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapExceptionBase(ManagedCatalogClientImpl.scala:7378)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:7364)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.listSchemasProto(ManagedCatalogClientImpl.scala:1293)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.listSchemas(ManagedCatalogClientImpl.scala:1279)\n\tat com.databricks.sql.managedcatalog.ManagedCatalogCommon.listSchemas(ManagedCatalogCommon.scala:821)\n\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.$anonfun$listSchemas$1(ProfiledManagedCatalog.scala:168)\n\tat org.apache.spark.sql.catalyst.MetricKeyUtils$.measure(MetricKey.scala:1688)\n\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.$anonfun$profile$1(ProfiledManagedCatalog.scala:64)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.profile(ProfiledManagedCatalog.scala:63)\n\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.listSchemas(ProfiledManagedCatalog.scala:168)\n\tat com.databricks.sql.managedcatalog.ManagedCatalogSessionCatalog.listDatabasesWithCatalog(ManagedCatalogSessionCatalog.scala:908)\n\tat com.databricks.sql.managedcatalog.UnityCatalogV2Proxy.listNamespaces(UnityCatalogV2Proxy.scala:141)\n\tat org.apache.spark.sql.execution.datasources.v2.ShowNamespacesExec.run(ShowNamespacesExec.scala:42)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.$anonfun$result$2(V2CommandExec.scala:48)\n\tat org.apache.spark.sql.execution.SparkPlan.runCommandInAetherOrSpark(SparkPlan.scala:194)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.$anonfun$result$1(V2CommandExec.scala:48)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:47)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:45)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:56)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$5(QueryExecution.scala:450)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$4(QueryExecution.scala:450)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:219)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$3(QueryExecution.scala:449)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$11(SQLExecution.scala:478)\n\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$10(SQLExecution.scala:414)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:777)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:344)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:213)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:714)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:445)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1307)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:441)\n\tat org.apache.spark.sql.execution.QueryExecution.withMVTagsIfNecessary(QueryExecution.scala:370)\n\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:439)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:503)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:498)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:521)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:521)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:361)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:357)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:497)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$8(QueryExecution.scala:498)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:498)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:326)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:331)\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:406)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$3(Dataset.scala:150)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n\tat org.apache.spark.sql.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1469)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1469)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:141)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:1142)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:1094)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.executeSQL(SparkConnectPlanner.scala:3606)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleSqlCommand(SparkConnectPlanner.scala:3435)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3370)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:413)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:312)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:233)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:464)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:464)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:90)\n\tat org.apache.spark.util.Utils$.withContextClassLoader(Utils.scala:241)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:89)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:463)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:233)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:139)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:614)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n\tat scala.util.Using$.resource(Using.scala:269)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:614)\nTraceback (most recent call last):\n  File \"/home/spark-99074ce5-7219-4ba1-80ca-d5/.ipykernel/2410/command-8748648698932923-2826314312\", line 295, in main\n    source_schemas_df = spark.sql(f\"SHOW SCHEMAS IN `{source_catalog}`\")\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/databricks/spark/python/pyspark/sql/connect/session.py\", line 821, in sql\n    data, properties, ei = self.client.execute_command(cmd.command(self._client))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/databricks/spark/python/pyspark/sql/connect/client/core.py\", line 1481, in execute_command\n    data, _, metrics, observed_metrics, properties = self._execute_and_fetch(\n                                                     ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/databricks/spark/python/pyspark/sql/connect/client/core.py\", line 1970, in _execute_and_fetch\n    for response in self._execute_and_fetch_as_iterator(\n  File \"/databricks/spark/python/pyspark/sql/connect/client/core.py\", line 1946, in _execute_and_fetch_as_iterator\n    self._handle_error(error)\n  File \"/databricks/spark/python/pyspark/sql/connect/client/core.py\", line 2266, in _handle_error\n    self._handle_rpc_error(error)\n  File \"/databricks/spark/python/pyspark/sql/connect/client/core.py\", line 2377, in _handle_rpc_error\n    raise convert_exception(\npyspark.errors.exceptions.connect.AnalysisException: [NO_SUCH_CATALOG_EXCEPTION] Catalog 'test_not' was not found. Please verify the catalog name and then retry the query or command again. SQLSTATE: 42704\n\nJVM stacktrace:\norg.apache.spark.sql.catalyst.analysis.NoSuchCatalogException\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$listSchemasProto$1(ManagedCatalogClientImpl.scala:1328)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$recordAndWrapExceptionBase$2(ManagedCatalogClientImpl.scala:7398)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$recordAndWrapExceptionBase$1(ManagedCatalogClientImpl.scala:7397)\n\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:66)\n\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:63)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:251)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapExceptionBase(ManagedCatalogClientImpl.scala:7378)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:7364)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.listSchemasProto(ManagedCatalogClientImpl.scala:1293)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.listSchemas(ManagedCatalogClientImpl.scala:1279)\n\tat com.databricks.sql.managedcatalog.ManagedCatalogCommon.listSchemas(ManagedCatalogCommon.scala:821)\n\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.$anonfun$listSchemas$1(ProfiledManagedCatalog.scala:168)\n\tat org.apache.spark.sql.catalyst.MetricKeyUtils$.measure(MetricKey.scala:1688)\n\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.$anonfun$profile$1(ProfiledManagedCatalog.scala:64)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.profile(ProfiledManagedCatalog.scala:63)\n\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.listSchemas(ProfiledManagedCatalog.scala:168)\n\tat com.databricks.sql.managedcatalog.ManagedCatalogSessionCatalog.listDatabasesWithCatalog(ManagedCatalogSessionCatalog.scala:908)\n\tat com.databricks.sql.managedcatalog.UnityCatalogV2Proxy.listNamespaces(UnityCatalogV2Proxy.scala:141)\n\tat org.apache.spark.sql.execution.datasources.v2.ShowNamespacesExec.run(ShowNamespacesExec.scala:42)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.$anonfun$result$2(V2CommandExec.scala:48)\n\tat org.apache.spark.sql.execution.SparkPlan.runCommandInAetherOrSpark(SparkPlan.scala:194)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.$anonfun$result$1(V2CommandExec.scala:48)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:47)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:45)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:56)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$5(QueryExecution.scala:450)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$4(QueryExecution.scala:450)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:219)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$3(QueryExecution.scala:449)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$11(SQLExecution.scala:478)\n\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$10(SQLExecution.scala:414)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:777)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:344)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:213)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:714)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:445)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1307)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:441)\n\tat org.apache.spark.sql.execution.QueryExecution.withMVTagsIfNecessary(QueryExecution.scala:370)\n\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:439)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:503)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:498)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:521)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:521)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:361)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:357)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:497)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$8(QueryExecution.scala:498)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:498)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:326)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:331)\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:406)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$3(Dataset.scala:150)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n\tat org.apache.spark.sql.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1469)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1469)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:141)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:1142)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:1094)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.executeSQL(SparkConnectPlanner.scala:3606)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleSqlCommand(SparkConnectPlanner.scala:3435)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3370)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:413)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:312)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:233)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:464)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:464)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:90)\n\tat org.apache.spark.util.Utils$.withContextClassLoader(Utils.scala:241)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:89)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:463)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:233)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:139)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:614)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n\tat scala.util.Using$.resource(Using.scala:269)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:614)\n",
         "2025-08-20T15:35:28.566707Z",
         []
        ],
        [
         "DR_Model_data",
         "catalog",
         "`dr_test_not`",
         "create",
         "success",
         "Catalog `dr_test_not` created or already exists.",
         "2025-08-20T15:35:26.898056Z",
         []
        ],
        [
         "DR_Model_data",
         "schema",
         "`dr_accuweather`.`forecast`",
         "create",
         "success",
         "Schema `dr_accuweather`.`forecast` created or already exists.",
         "2025-08-20T15:35:25.247892Z",
         []
        ],
        [
         "DR_Model_data",
         "catalog",
         "`dr_accuweather`",
         "create",
         "success",
         "Catalog `dr_accuweather` created or already exists.",
         "2025-08-20T15:35:22.810636Z",
         []
        ],
        [
         "DR_Model_data",
         "schema",
         "`dr_eo_amperity`.`silver`",
         "create",
         "success",
         "Schema `dr_eo_amperity`.`silver` created or already exists.",
         "2025-08-20T15:35:20.827643Z",
         []
        ],
        [
         "DR_Model_data",
         "schema",
         "`dr_eo_amperity`.`default`",
         "create",
         "success",
         "Schema `dr_eo_amperity`.`default` created or already exists.",
         "2025-08-20T15:35:19.043524Z",
         []
        ],
        [
         "DR_Model_data",
         "schema",
         "`dr_eo_amperity`.`bronze`",
         "create",
         "success",
         "Schema `dr_eo_amperity`.`bronze` created or already exists.",
         "2025-08-20T15:35:14.099212Z",
         []
        ],
        [
         "DR_Model_data",
         "catalog",
         "`dr_eo_amperity`",
         "create",
         "success",
         "Catalog `dr_eo_amperity` created or already exists.",
         "2025-08-20T15:34:39.443726Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "table",
         "`satya_dr_catalog`.`satyendranath_sure`.`satya_managed_table`",
         "clone",
         "success",
         "Table `satya_dr_catalog`.`satyendranath_sure`.`satya_managed_table` cloned successfully.",
         "2025-08-13T14:52:36.707876Z",
         [
          [
           "source_table_size",
           "827"
          ],
          [
           "source_num_of_files",
           "1"
          ],
          [
           "num_of_synced_transactions",
           "None"
          ],
          [
           "num_removed_files",
           "0"
          ],
          [
           "num_copied_files",
           "0"
          ],
          [
           "removed_files_size",
           "0"
          ],
          [
           "copied_files_size",
           "0"
          ]
         ]
        ],
        [
         "DR_Clone_data_v2",
         "table",
         "`satya_dr_catalog`.`satyendranath_sure`.`satya_external_table`",
         "clone",
         "success",
         "Table `satya_dr_catalog`.`satyendranath_sure`.`satya_external_table` cloned successfully.",
         "2025-08-13T14:52:35.441812Z",
         [
          [
           "source_table_size",
           "826"
          ],
          [
           "source_num_of_files",
           "1"
          ],
          [
           "num_of_synced_transactions",
           "None"
          ],
          [
           "num_removed_files",
           "0"
          ],
          [
           "num_copied_files",
           "0"
          ],
          [
           "removed_files_size",
           "0"
          ],
          [
           "copied_files_size",
           "0"
          ]
         ]
        ],
        [
         "DR_Clone_data_v2",
         "table/view",
         "`accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample`.`forecast`.`us_postal_hourly_metric`",
         "clone/create",
         "error",
         "Failed to process table/view `accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample`.`forecast`.`us_postal_hourly_metric`. Error: An error occurred while calling o427.sql.\n: com.databricks.sql.managedcatalog.acl.UnauthorizedAccessException: PERMISSION_DENIED: User does not have USE CATALOG on Catalog 'accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample'.\n\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:87)\n\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:63)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:251)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapExceptionBase(ManagedCatalogClientImpl.scala:7378)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:7364)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.getTableCredentials(ManagedCatalogClientImpl.scala:3951)\n\tat com.databricks.sql.managedcatalog.ManagedCatalogClient.getTemporaryCredentials(ManagedCatalogClient.scala:2620)\n\tat com.databricks.sql.managedcatalog.ManagedCatalogClient.getTemporaryCredentials$(ManagedCatalogClient.scala:2605)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.getTemporaryCredentials(ManagedCatalogClientImpl.scala:251)\n\tat com.databricks.unity.TempCredCache.$anonfun$getInternal$7(TemporaryCredentials.scala:448)\n\tat com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4724)\n\tat com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3522)\n\tat com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2315)\n\tat com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2278)\n\tat com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2193)\n\tat com.google.common.cache.LocalCache.get(LocalCache.java:3932)\n\tat com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4721)\n\tat com.databricks.unity.TempCredCache.liftedTree1$1(TemporaryCredentials.scala:447)\n\tat com.databricks.unity.TempCredCache.getInternal(TemporaryCredentials.scala:446)\n\tat com.databricks.unity.TempCredCache.get(TemporaryCredentials.scala:373)\n\tat com.databricks.unity.UnityCredentialManager.getTemporaryCredentials(CredentialManager.scala:488)\n\tat com.databricks.unity.CredentialManager$.getTemporaryCredentials(CredentialManager.scala:881)\n\tat com.databricks.unity.SAM.setConfFromCred$1(SAM.scala:338)\n\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$8(SAM.scala:353)\n\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$8$adapted(SAM.scala:352)\n\tat com.databricks.unity.TempCredRequest$.$anonfun$withFallbackOnOverlappingForParentPath$3(TemporaryCredentials.scala:147)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.databricks.unity.TempCredRequest$.withFallbackOnOverlappingForParentPath(TemporaryCredentials.scala:147)\n\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$7(SAM.scala:352)\n\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$7$adapted(SAM.scala:350)\n\tat scala.Option.map(Option.scala:230)\n\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$6(SAM.scala:350)\n\tat scala.Option.flatMap(Option.scala:271)\n\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$1(SAM.scala:349)\n\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$1$adapted(SAM.scala:336)\n\tat scala.Option.foreach(Option.scala:407)\n\tat com.databricks.unity.SAM.setFileSystemConfigs(SAM.scala:336)\n\tat com.databricks.unity.SAM.createDelegate(SAM.scala:388)\n\tat com.databricks.unity.SAM.createDelegate$(SAM.scala:372)\n\tat com.databricks.unity.TableSAM.createDelegate(SAM.scala:695)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.createDelegate(CredentialScopeFileSystem.scala:74)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.$anonfun$setDelegates$1(CredentialScopeFileSystem.scala:127)\n\tat com.databricks.sql.acl.fs.Lazy.apply(DelegatingFileSystem.scala:310)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.readDelegateMakeQualified(CredentialScopeFileSystem.scala:282)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.optimizedMakeQualifiedPath(CredentialScopeFileSystem.scala:277)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.makeQualified(CredentialScopeFileSystem.scala:294)\n\tat com.databricks.sql.transaction.tahoe.DeltaLog$.apply(DeltaLog.scala:1284)\n\tat com.databricks.sql.transaction.tahoe.DeltaLog$.forTable(DeltaLog.scala:1087)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$5(DeltaTableV2.scala:175)\n\tat com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$withAdditionalSnapshotInitializationUsageLogData$1(DeltaLog.scala:968)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n\tat com.databricks.sql.transaction.tahoe.DeltaLog$.withAdditionalSnapshotInitializationUsageLogData(DeltaLog.scala:969)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$2(DeltaTableV2.scala:169)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:700)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$1(DeltaTableV2.scala:150)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:83)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.deltaLog$lzycompute(DeltaTableV2.scala:150)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.deltaLog(DeltaTableV2.scala:148)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$8(DeltaTableV2.scala:264)\n\tat com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$withAdditionalSnapshotInitializationUsageLogData$1(DeltaLog.scala:968)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n\tat com.databricks.sql.transaction.tahoe.DeltaLog$.withAdditionalSnapshotInitializationUsageLogData(DeltaLog.scala:969)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$7(DeltaTableV2.scala:263)\n\tat scala.Option.orElse(Option.scala:447)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$5(DeltaTableV2.scala:263)\n\tat scala.Option.orElse(Option.scala:447)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$2(DeltaTableV2.scala:260)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:83)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$1(DeltaTableV2.scala:290)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:700)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot$lzycompute(DeltaTableV2.scala:240)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot(DeltaTableV2.scala:238)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$tableSchema$2(DeltaTableV2.scala:317)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.tableSchema$lzycompute(DeltaTableV2.scala:317)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.tableSchema(DeltaTableV2.scala:315)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.schema(DeltaTableV2.scala:322)\n\tat org.apache.spark.sql.connector.catalog.Table.columns(Table.java:68)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.columns(DeltaTableV2.scala:83)\n\tat org.apache.spark.sql.catalyst.analysis.ResolvedTable$.create(v2ResolutionPlans.scala:231)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$4(Analyzer.scala:1600)\n\tat scala.Option.map(Option.scala:230)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$3(Analyzer.scala:1592)\n\tat scala.Option.orElse(Option.scala:447)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableOrView(Analyzer.scala:1587)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1556)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1466)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:182)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:171)\n\tat scala.PartialFunction.applyOrElse(PartialFunction.scala:127)\n\tat scala.PartialFunction.applyOrElse$(PartialFunction.scala:126)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.applyOrElse(AnalysisHelper.scala:171)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:138)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1330)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1329)\n\tat org.apache.spark.sql.catalyst.plans.logical.DescribeRelation.mapChildren(v2Commands.scala:726)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning(AnalysisHelper.scala:186)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning$(AnalysisHelper.scala:167)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithSubqueriesAndPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.resolveOperatorsUpWithSubqueriesAndPruningByConf(Analyzer.scala:1674)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply0(Analyzer.scala:1466)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1419)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1365)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:480)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:629)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:613)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:131)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:480)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:479)\n\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:475)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:452)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22(RuleExecutor.scala:585)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22$adapted(RuleExecutor.scala:585)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:585)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:349)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:498)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:491)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:397)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:491)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:416)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:341)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:219)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:341)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:252)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:96)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:131)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:87)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:478)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:478)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:294)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:548)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:668)\n\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:151)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:668)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1307)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:661)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:658)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:658)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:288)\n\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:287)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:321)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:267)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$3(Dataset.scala:149)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n\tat org.apache.spark.sql.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1469)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1469)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:141)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:1048)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:1001)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:1071)\n\tat jdk.internal.reflect.GeneratedMethodAccessor593.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:197)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:117)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\n\t\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:87)\n\t\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:63)\n\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:251)\n\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapExceptionBase(ManagedCatalogClientImpl.scala:7378)\n\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:7364)\n\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.getTableCredentials(ManagedCatalogClientImpl.scala:3951)\n\t\tat com.databricks.sql.managedcatalog.ManagedCatalogClient.getTemporaryCredentials(ManagedCatalogClient.scala:2620)\n\t\tat com.databricks.sql.managedcatalog.ManagedCatalogClient.getTemporaryCredentials$(ManagedCatalogClient.scala:2605)\n\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.getTemporaryCredentials(ManagedCatalogClientImpl.scala:251)\n\t\tat com.databricks.unity.TempCredCache.$anonfun$getInternal$7(TemporaryCredentials.scala:448)\n\t\tat com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4724)\n\t\tat com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3522)\n\t\tat com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2315)\n\t\tat com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2278)\n\t\tat com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2193)\n\t\tat com.google.common.cache.LocalCache.get(LocalCache.java:3932)\n\t\tat com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4721)\n\t\tat com.databricks.unity.TempCredCache.liftedTree1$1(TemporaryCredentials.scala:447)\n\t\tat com.databricks.unity.TempCredCache.getInternal(TemporaryCredentials.scala:446)\n\t\tat com.databricks.unity.TempCredCache.get(TemporaryCredentials.scala:373)\n\t\tat com.databricks.unity.UnityCredentialManager.getTemporaryCredentials(CredentialManager.scala:488)\n\t\tat com.databricks.unity.CredentialManager$.getTemporaryCredentials(CredentialManager.scala:881)\n\t\tat com.databricks.unity.SAM.setConfFromCred$1(SAM.scala:338)\n\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$8(SAM.scala:353)\n\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$8$adapted(SAM.scala:352)\n\t\tat com.databricks.unity.TempCredRequest$.$anonfun$withFallbackOnOverlappingForParentPath$3(TemporaryCredentials.scala:147)\n\t\tat scala.util.Try$.apply(Try.scala:213)\n\t\tat com.databricks.unity.TempCredRequest$.withFallbackOnOverlappingForParentPath(TemporaryCredentials.scala:147)\n\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$7(SAM.scala:352)\n\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$7$adapted(SAM.scala:350)\n\t\tat scala.Option.map(Option.scala:230)\n\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$6(SAM.scala:350)\n\t\tat scala.Option.flatMap(Option.scala:271)\n\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$1(SAM.scala:349)\n\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$1$adapted(SAM.scala:336)\n\t\tat scala.Option.foreach(Option.scala:407)\n\t\tat com.databricks.unity.SAM.setFileSystemConfigs(SAM.scala:336)\n\t\tat com.databricks.unity.SAM.createDelegate(SAM.scala:388)\n\t\tat com.databricks.unity.SAM.createDelegate$(SAM.scala:372)\n\t\tat com.databricks.unity.TableSAM.createDelegate(SAM.scala:695)\n\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.createDelegate(CredentialScopeFileSystem.scala:74)\n\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.$anonfun$setDelegates$1(CredentialScopeFileSystem.scala:127)\n\t\tat com.databricks.sql.acl.fs.Lazy.apply(DelegatingFileSystem.scala:310)\n\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.readDelegateMakeQualified(CredentialScopeFileSystem.scala:282)\n\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.optimizedMakeQualifiedPath(CredentialScopeFileSystem.scala:277)\n\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.makeQualified(CredentialScopeFileSystem.scala:294)\n\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.apply(DeltaLog.scala:1284)\n\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.forTable(DeltaLog.scala:1087)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$5(DeltaTableV2.scala:175)\n\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$withAdditionalSnapshotInitializationUsageLogData$1(DeltaLog.scala:968)\n\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.withAdditionalSnapshotInitializationUsageLogData(DeltaLog.scala:969)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$2(DeltaTableV2.scala:169)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:700)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$1(DeltaTableV2.scala:150)\n\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\t\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n\t\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:83)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.deltaLog$lzycompute(DeltaTableV2.scala:150)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.deltaLog(DeltaTableV2.scala:148)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$8(DeltaTableV2.scala:264)\n\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$withAdditionalSnapshotInitializationUsageLogData$1(DeltaLog.scala:968)\n\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.withAdditionalSnapshotInitializationUsageLogData(DeltaLog.scala:969)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$7(DeltaTableV2.scala:263)\n\t\tat scala.Option.orElse(Option.scala:447)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$5(DeltaTableV2.scala:263)\n\t\tat scala.Option.orElse(Option.scala:447)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$2(DeltaTableV2.scala:260)\n\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\t\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n\t\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:83)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$1(DeltaTableV2.scala:290)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:700)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot$lzycompute(DeltaTableV2.scala:240)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot(DeltaTableV2.scala:238)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$tableSchema$2(DeltaTableV2.scala:317)\n\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.tableSchema$lzycompute(DeltaTableV2.scala:317)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.tableSchema(DeltaTableV2.scala:315)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.schema(DeltaTableV2.scala:322)\n\t\tat org.apache.spark.sql.connector.catalog.Table.columns(Table.java:68)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.columns(DeltaTableV2.scala:83)\n\t\tat org.apache.spark.sql.catalyst.analysis.ResolvedTable$.create(v2ResolutionPlans.scala:231)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$4(Analyzer.scala:1600)\n\t\tat scala.Option.map(Option.scala:230)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$3(Analyzer.scala:1592)\n\t\tat scala.Option.orElse(Option.scala:447)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableOrView(Analyzer.scala:1587)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1556)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1466)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:182)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:171)\n\t\tat scala.PartialFunction.applyOrElse(PartialFunction.scala:127)\n\t\tat scala.PartialFunction.applyOrElse$(PartialFunction.scala:126)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.applyOrElse(AnalysisHelper.scala:171)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)\n\t\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:138)\n\t\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1330)\n\t\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1329)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.DescribeRelation.mapChildren(v2Commands.scala:726)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning(AnalysisHelper.scala:186)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning$(AnalysisHelper.scala:167)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithSubqueriesAndPruning(LogicalPlan.scala:42)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.resolveOperatorsUpWithSubqueriesAndPruningByConf(Analyzer.scala:1674)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply0(Analyzer.scala:1466)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1419)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1365)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:480)\n\t\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:629)\n\t\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:613)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:131)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:480)\n\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:479)\n\t\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n\t\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n\t\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:475)\n\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:452)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22(RuleExecutor.scala:585)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22$adapted(RuleExecutor.scala:585)\n\t\tat scala.collection.immutable.List.foreach(List.scala:431)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:585)\n\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:349)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:498)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:491)\n\t\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:397)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:491)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:416)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:341)\n\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:219)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:341)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:252)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:96)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:131)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:87)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:478)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:478)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:294)\n\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:548)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:668)\n\t\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:151)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:668)\n\t\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1307)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:661)\n\t\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:658)\n\t\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n\t\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:658)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:288)\n\t\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:287)\n\t\tat scala.util.Try$.apply(Try.scala:213)\n\t\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)\n\t\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n\t\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n\t\t... 24 more\n",
         "2025-08-13T14:52:13.322232Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "table/view",
         "`accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample`.`forecast`.`us_postal_hourly_imperial`",
         "clone/create",
         "error",
         "Failed to process table/view `accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample`.`forecast`.`us_postal_hourly_imperial`. Error: An error occurred while calling o427.sql.\n: com.databricks.sql.managedcatalog.acl.UnauthorizedAccessException: PERMISSION_DENIED: User does not have USE CATALOG on Catalog 'accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample'.\n\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:87)\n\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:63)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:251)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapExceptionBase(ManagedCatalogClientImpl.scala:7378)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:7364)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.getTableCredentials(ManagedCatalogClientImpl.scala:3951)\n\tat com.databricks.sql.managedcatalog.ManagedCatalogClient.getTemporaryCredentials(ManagedCatalogClient.scala:2620)\n\tat com.databricks.sql.managedcatalog.ManagedCatalogClient.getTemporaryCredentials$(ManagedCatalogClient.scala:2605)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.getTemporaryCredentials(ManagedCatalogClientImpl.scala:251)\n\tat com.databricks.unity.TempCredCache.$anonfun$getInternal$7(TemporaryCredentials.scala:448)\n\tat com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4724)\n\tat com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3522)\n\tat com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2315)\n\tat com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2278)\n\tat com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2193)\n\tat com.google.common.cache.LocalCache.get(LocalCache.java:3932)\n\tat com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4721)\n\tat com.databricks.unity.TempCredCache.liftedTree1$1(TemporaryCredentials.scala:447)\n\tat com.databricks.unity.TempCredCache.getInternal(TemporaryCredentials.scala:446)\n\tat com.databricks.unity.TempCredCache.get(TemporaryCredentials.scala:373)\n\tat com.databricks.unity.UnityCredentialManager.getTemporaryCredentials(CredentialManager.scala:488)\n\tat com.databricks.unity.CredentialManager$.getTemporaryCredentials(CredentialManager.scala:881)\n\tat com.databricks.unity.SAM.setConfFromCred$1(SAM.scala:338)\n\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$8(SAM.scala:353)\n\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$8$adapted(SAM.scala:352)\n\tat com.databricks.unity.TempCredRequest$.$anonfun$withFallbackOnOverlappingForParentPath$3(TemporaryCredentials.scala:147)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.databricks.unity.TempCredRequest$.withFallbackOnOverlappingForParentPath(TemporaryCredentials.scala:147)\n\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$7(SAM.scala:352)\n\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$7$adapted(SAM.scala:350)\n\tat scala.Option.map(Option.scala:230)\n\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$6(SAM.scala:350)\n\tat scala.Option.flatMap(Option.scala:271)\n\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$1(SAM.scala:349)\n\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$1$adapted(SAM.scala:336)\n\tat scala.Option.foreach(Option.scala:407)\n\tat com.databricks.unity.SAM.setFileSystemConfigs(SAM.scala:336)\n\tat com.databricks.unity.SAM.createDelegate(SAM.scala:388)\n\tat com.databricks.unity.SAM.createDelegate$(SAM.scala:372)\n\tat com.databricks.unity.TableSAM.createDelegate(SAM.scala:695)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.createDelegate(CredentialScopeFileSystem.scala:74)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.$anonfun$setDelegates$1(CredentialScopeFileSystem.scala:127)\n\tat com.databricks.sql.acl.fs.Lazy.apply(DelegatingFileSystem.scala:310)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.readDelegateMakeQualified(CredentialScopeFileSystem.scala:282)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.optimizedMakeQualifiedPath(CredentialScopeFileSystem.scala:277)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.makeQualified(CredentialScopeFileSystem.scala:294)\n\tat com.databricks.sql.transaction.tahoe.DeltaLog$.apply(DeltaLog.scala:1284)\n\tat com.databricks.sql.transaction.tahoe.DeltaLog$.forTable(DeltaLog.scala:1087)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$5(DeltaTableV2.scala:175)\n\tat com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$withAdditionalSnapshotInitializationUsageLogData$1(DeltaLog.scala:968)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n\tat com.databricks.sql.transaction.tahoe.DeltaLog$.withAdditionalSnapshotInitializationUsageLogData(DeltaLog.scala:969)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$2(DeltaTableV2.scala:169)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:700)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$1(DeltaTableV2.scala:150)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:83)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.deltaLog$lzycompute(DeltaTableV2.scala:150)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.deltaLog(DeltaTableV2.scala:148)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$8(DeltaTableV2.scala:264)\n\tat com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$withAdditionalSnapshotInitializationUsageLogData$1(DeltaLog.scala:968)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n\tat com.databricks.sql.transaction.tahoe.DeltaLog$.withAdditionalSnapshotInitializationUsageLogData(DeltaLog.scala:969)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$7(DeltaTableV2.scala:263)\n\tat scala.Option.orElse(Option.scala:447)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$5(DeltaTableV2.scala:263)\n\tat scala.Option.orElse(Option.scala:447)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$2(DeltaTableV2.scala:260)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:83)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$1(DeltaTableV2.scala:290)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:700)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot$lzycompute(DeltaTableV2.scala:240)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot(DeltaTableV2.scala:238)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$tableSchema$2(DeltaTableV2.scala:317)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.tableSchema$lzycompute(DeltaTableV2.scala:317)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.tableSchema(DeltaTableV2.scala:315)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.schema(DeltaTableV2.scala:322)\n\tat org.apache.spark.sql.connector.catalog.Table.columns(Table.java:68)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.columns(DeltaTableV2.scala:83)\n\tat org.apache.spark.sql.catalyst.analysis.ResolvedTable$.create(v2ResolutionPlans.scala:231)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$4(Analyzer.scala:1600)\n\tat scala.Option.map(Option.scala:230)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$3(Analyzer.scala:1592)\n\tat scala.Option.orElse(Option.scala:447)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableOrView(Analyzer.scala:1587)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1556)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1466)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:182)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:171)\n\tat scala.PartialFunction.applyOrElse(PartialFunction.scala:127)\n\tat scala.PartialFunction.applyOrElse$(PartialFunction.scala:126)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.applyOrElse(AnalysisHelper.scala:171)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:138)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1330)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1329)\n\tat org.apache.spark.sql.catalyst.plans.logical.DescribeRelation.mapChildren(v2Commands.scala:726)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning(AnalysisHelper.scala:186)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning$(AnalysisHelper.scala:167)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithSubqueriesAndPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.resolveOperatorsUpWithSubqueriesAndPruningByConf(Analyzer.scala:1674)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply0(Analyzer.scala:1466)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1419)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1365)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:480)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:629)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:613)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:131)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:480)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:479)\n\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:475)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:452)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22(RuleExecutor.scala:585)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22$adapted(RuleExecutor.scala:585)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:585)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:349)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:498)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:491)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:397)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:491)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:416)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:341)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:219)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:341)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:252)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:96)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:131)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:87)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:478)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:478)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:294)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:548)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:668)\n\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:151)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:668)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1307)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:661)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:658)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:658)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:288)\n\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:287)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:321)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:267)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$3(Dataset.scala:149)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n\tat org.apache.spark.sql.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1469)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1469)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:141)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:1048)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:1001)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:1071)\n\tat jdk.internal.reflect.GeneratedMethodAccessor593.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:197)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:117)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\n\t\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:87)\n\t\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:63)\n\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:251)\n\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapExceptionBase(ManagedCatalogClientImpl.scala:7378)\n\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:7364)\n\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.getTableCredentials(ManagedCatalogClientImpl.scala:3951)\n\t\tat com.databricks.sql.managedcatalog.ManagedCatalogClient.getTemporaryCredentials(ManagedCatalogClient.scala:2620)\n\t\tat com.databricks.sql.managedcatalog.ManagedCatalogClient.getTemporaryCredentials$(ManagedCatalogClient.scala:2605)\n\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.getTemporaryCredentials(ManagedCatalogClientImpl.scala:251)\n\t\tat com.databricks.unity.TempCredCache.$anonfun$getInternal$7(TemporaryCredentials.scala:448)\n\t\tat com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4724)\n\t\tat com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3522)\n\t\tat com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2315)\n\t\tat com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2278)\n\t\tat com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2193)\n\t\tat com.google.common.cache.LocalCache.get(LocalCache.java:3932)\n\t\tat com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4721)\n\t\tat com.databricks.unity.TempCredCache.liftedTree1$1(TemporaryCredentials.scala:447)\n\t\tat com.databricks.unity.TempCredCache.getInternal(TemporaryCredentials.scala:446)\n\t\tat com.databricks.unity.TempCredCache.get(TemporaryCredentials.scala:373)\n\t\tat com.databricks.unity.UnityCredentialManager.getTemporaryCredentials(CredentialManager.scala:488)\n\t\tat com.databricks.unity.CredentialManager$.getTemporaryCredentials(CredentialManager.scala:881)\n\t\tat com.databricks.unity.SAM.setConfFromCred$1(SAM.scala:338)\n\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$8(SAM.scala:353)\n\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$8$adapted(SAM.scala:352)\n\t\tat com.databricks.unity.TempCredRequest$.$anonfun$withFallbackOnOverlappingForParentPath$3(TemporaryCredentials.scala:147)\n\t\tat scala.util.Try$.apply(Try.scala:213)\n\t\tat com.databricks.unity.TempCredRequest$.withFallbackOnOverlappingForParentPath(TemporaryCredentials.scala:147)\n\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$7(SAM.scala:352)\n\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$7$adapted(SAM.scala:350)\n\t\tat scala.Option.map(Option.scala:230)\n\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$6(SAM.scala:350)\n\t\tat scala.Option.flatMap(Option.scala:271)\n\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$1(SAM.scala:349)\n\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$1$adapted(SAM.scala:336)\n\t\tat scala.Option.foreach(Option.scala:407)\n\t\tat com.databricks.unity.SAM.setFileSystemConfigs(SAM.scala:336)\n\t\tat com.databricks.unity.SAM.createDelegate(SAM.scala:388)\n\t\tat com.databricks.unity.SAM.createDelegate$(SAM.scala:372)\n\t\tat com.databricks.unity.TableSAM.createDelegate(SAM.scala:695)\n\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.createDelegate(CredentialScopeFileSystem.scala:74)\n\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.$anonfun$setDelegates$1(CredentialScopeFileSystem.scala:127)\n\t\tat com.databricks.sql.acl.fs.Lazy.apply(DelegatingFileSystem.scala:310)\n\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.readDelegateMakeQualified(CredentialScopeFileSystem.scala:282)\n\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.optimizedMakeQualifiedPath(CredentialScopeFileSystem.scala:277)\n\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.makeQualified(CredentialScopeFileSystem.scala:294)\n\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.apply(DeltaLog.scala:1284)\n\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.forTable(DeltaLog.scala:1087)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$5(DeltaTableV2.scala:175)\n\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$withAdditionalSnapshotInitializationUsageLogData$1(DeltaLog.scala:968)\n\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.withAdditionalSnapshotInitializationUsageLogData(DeltaLog.scala:969)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$2(DeltaTableV2.scala:169)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:700)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$1(DeltaTableV2.scala:150)\n\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\t\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n\t\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:83)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.deltaLog$lzycompute(DeltaTableV2.scala:150)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.deltaLog(DeltaTableV2.scala:148)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$8(DeltaTableV2.scala:264)\n\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$withAdditionalSnapshotInitializationUsageLogData$1(DeltaLog.scala:968)\n\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.withAdditionalSnapshotInitializationUsageLogData(DeltaLog.scala:969)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$7(DeltaTableV2.scala:263)\n\t\tat scala.Option.orElse(Option.scala:447)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$5(DeltaTableV2.scala:263)\n\t\tat scala.Option.orElse(Option.scala:447)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$2(DeltaTableV2.scala:260)\n\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\t\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n\t\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:83)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$1(DeltaTableV2.scala:290)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:700)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot$lzycompute(DeltaTableV2.scala:240)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot(DeltaTableV2.scala:238)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$tableSchema$2(DeltaTableV2.scala:317)\n\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.tableSchema$lzycompute(DeltaTableV2.scala:317)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.tableSchema(DeltaTableV2.scala:315)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.schema(DeltaTableV2.scala:322)\n\t\tat org.apache.spark.sql.connector.catalog.Table.columns(Table.java:68)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.columns(DeltaTableV2.scala:83)\n\t\tat org.apache.spark.sql.catalyst.analysis.ResolvedTable$.create(v2ResolutionPlans.scala:231)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$4(Analyzer.scala:1600)\n\t\tat scala.Option.map(Option.scala:230)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$3(Analyzer.scala:1592)\n\t\tat scala.Option.orElse(Option.scala:447)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableOrView(Analyzer.scala:1587)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1556)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1466)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:182)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:171)\n\t\tat scala.PartialFunction.applyOrElse(PartialFunction.scala:127)\n\t\tat scala.PartialFunction.applyOrElse$(PartialFunction.scala:126)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.applyOrElse(AnalysisHelper.scala:171)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)\n\t\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:138)\n\t\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1330)\n\t\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1329)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.DescribeRelation.mapChildren(v2Commands.scala:726)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning(AnalysisHelper.scala:186)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning$(AnalysisHelper.scala:167)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithSubqueriesAndPruning(LogicalPlan.scala:42)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.resolveOperatorsUpWithSubqueriesAndPruningByConf(Analyzer.scala:1674)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply0(Analyzer.scala:1466)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1419)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1365)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:480)\n\t\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:629)\n\t\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:613)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:131)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:480)\n\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:479)\n\t\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n\t\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n\t\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:475)\n\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:452)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22(RuleExecutor.scala:585)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22$adapted(RuleExecutor.scala:585)\n\t\tat scala.collection.immutable.List.foreach(List.scala:431)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:585)\n\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:349)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:498)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:491)\n\t\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:397)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:491)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:416)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:341)\n\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:219)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:341)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:252)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:96)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:131)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:87)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:478)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:478)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:294)\n\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:548)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:668)\n\t\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:151)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:668)\n\t\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1307)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:661)\n\t\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:658)\n\t\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n\t\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:658)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:288)\n\t\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:287)\n\t\tat scala.util.Try$.apply(Try.scala:213)\n\t\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)\n\t\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n\t\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n\t\t... 24 more\n",
         "2025-08-13T14:52:13.308379Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "table/view",
         "`accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample`.`forecast`.`us_postal_daynight_metric`",
         "clone/create",
         "error",
         "Failed to process table/view `accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample`.`forecast`.`us_postal_daynight_metric`. Error: An error occurred while calling o427.sql.\n: com.databricks.sql.managedcatalog.acl.UnauthorizedAccessException: PERMISSION_DENIED: User does not have USE CATALOG on Catalog 'accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample'.\n\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:87)\n\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:63)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:251)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapExceptionBase(ManagedCatalogClientImpl.scala:7378)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:7364)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.getTableCredentials(ManagedCatalogClientImpl.scala:3951)\n\tat com.databricks.sql.managedcatalog.ManagedCatalogClient.getTemporaryCredentials(ManagedCatalogClient.scala:2620)\n\tat com.databricks.sql.managedcatalog.ManagedCatalogClient.getTemporaryCredentials$(ManagedCatalogClient.scala:2605)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.getTemporaryCredentials(ManagedCatalogClientImpl.scala:251)\n\tat com.databricks.unity.TempCredCache.$anonfun$getInternal$7(TemporaryCredentials.scala:448)\n\tat com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4724)\n\tat com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3522)\n\tat com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2315)\n\tat com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2278)\n\tat com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2193)\n\tat com.google.common.cache.LocalCache.get(LocalCache.java:3932)\n\tat com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4721)\n\tat com.databricks.unity.TempCredCache.liftedTree1$1(TemporaryCredentials.scala:447)\n\tat com.databricks.unity.TempCredCache.getInternal(TemporaryCredentials.scala:446)\n\tat com.databricks.unity.TempCredCache.get(TemporaryCredentials.scala:373)\n\tat com.databricks.unity.UnityCredentialManager.getTemporaryCredentials(CredentialManager.scala:488)\n\tat com.databricks.unity.CredentialManager$.getTemporaryCredentials(CredentialManager.scala:881)\n\tat com.databricks.unity.SAM.setConfFromCred$1(SAM.scala:338)\n\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$8(SAM.scala:353)\n\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$8$adapted(SAM.scala:352)\n\tat com.databricks.unity.TempCredRequest$.$anonfun$withFallbackOnOverlappingForParentPath$3(TemporaryCredentials.scala:147)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.databricks.unity.TempCredRequest$.withFallbackOnOverlappingForParentPath(TemporaryCredentials.scala:147)\n\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$7(SAM.scala:352)\n\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$7$adapted(SAM.scala:350)\n\tat scala.Option.map(Option.scala:230)\n\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$6(SAM.scala:350)\n\tat scala.Option.flatMap(Option.scala:271)\n\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$1(SAM.scala:349)\n\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$1$adapted(SAM.scala:336)\n\tat scala.Option.foreach(Option.scala:407)\n\tat com.databricks.unity.SAM.setFileSystemConfigs(SAM.scala:336)\n\tat com.databricks.unity.SAM.createDelegate(SAM.scala:388)\n\tat com.databricks.unity.SAM.createDelegate$(SAM.scala:372)\n\tat com.databricks.unity.TableSAM.createDelegate(SAM.scala:695)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.createDelegate(CredentialScopeFileSystem.scala:74)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.$anonfun$setDelegates$1(CredentialScopeFileSystem.scala:127)\n\tat com.databricks.sql.acl.fs.Lazy.apply(DelegatingFileSystem.scala:310)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.readDelegateMakeQualified(CredentialScopeFileSystem.scala:282)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.optimizedMakeQualifiedPath(CredentialScopeFileSystem.scala:277)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.makeQualified(CredentialScopeFileSystem.scala:294)\n\tat com.databricks.sql.transaction.tahoe.DeltaLog$.apply(DeltaLog.scala:1284)\n\tat com.databricks.sql.transaction.tahoe.DeltaLog$.forTable(DeltaLog.scala:1087)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$5(DeltaTableV2.scala:175)\n\tat com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$withAdditionalSnapshotInitializationUsageLogData$1(DeltaLog.scala:968)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n\tat com.databricks.sql.transaction.tahoe.DeltaLog$.withAdditionalSnapshotInitializationUsageLogData(DeltaLog.scala:969)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$2(DeltaTableV2.scala:169)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:700)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$1(DeltaTableV2.scala:150)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:83)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.deltaLog$lzycompute(DeltaTableV2.scala:150)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.deltaLog(DeltaTableV2.scala:148)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$8(DeltaTableV2.scala:264)\n\tat com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$withAdditionalSnapshotInitializationUsageLogData$1(DeltaLog.scala:968)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n\tat com.databricks.sql.transaction.tahoe.DeltaLog$.withAdditionalSnapshotInitializationUsageLogData(DeltaLog.scala:969)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$7(DeltaTableV2.scala:263)\n\tat scala.Option.orElse(Option.scala:447)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$5(DeltaTableV2.scala:263)\n\tat scala.Option.orElse(Option.scala:447)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$2(DeltaTableV2.scala:260)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:83)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$1(DeltaTableV2.scala:290)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:700)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot$lzycompute(DeltaTableV2.scala:240)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot(DeltaTableV2.scala:238)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$tableSchema$2(DeltaTableV2.scala:317)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.tableSchema$lzycompute(DeltaTableV2.scala:317)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.tableSchema(DeltaTableV2.scala:315)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.schema(DeltaTableV2.scala:322)\n\tat org.apache.spark.sql.connector.catalog.Table.columns(Table.java:68)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.columns(DeltaTableV2.scala:83)\n\tat org.apache.spark.sql.catalyst.analysis.ResolvedTable$.create(v2ResolutionPlans.scala:231)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$4(Analyzer.scala:1600)\n\tat scala.Option.map(Option.scala:230)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$3(Analyzer.scala:1592)\n\tat scala.Option.orElse(Option.scala:447)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableOrView(Analyzer.scala:1587)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1556)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1466)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:182)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:171)\n\tat scala.PartialFunction.applyOrElse(PartialFunction.scala:127)\n\tat scala.PartialFunction.applyOrElse$(PartialFunction.scala:126)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.applyOrElse(AnalysisHelper.scala:171)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:138)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1330)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1329)\n\tat org.apache.spark.sql.catalyst.plans.logical.DescribeRelation.mapChildren(v2Commands.scala:726)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning(AnalysisHelper.scala:186)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning$(AnalysisHelper.scala:167)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithSubqueriesAndPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.resolveOperatorsUpWithSubqueriesAndPruningByConf(Analyzer.scala:1674)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply0(Analyzer.scala:1466)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1419)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1365)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:480)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:629)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:613)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:131)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:480)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:479)\n\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:475)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:452)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22(RuleExecutor.scala:585)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22$adapted(RuleExecutor.scala:585)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:585)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:349)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:498)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:491)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:397)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:491)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:416)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:341)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:219)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:341)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:252)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:96)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:131)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:87)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:478)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:478)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:294)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:548)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:668)\n\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:151)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:668)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1307)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:661)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:658)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:658)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:288)\n\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:287)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:321)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:267)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$3(Dataset.scala:149)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n\tat org.apache.spark.sql.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1469)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1469)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:141)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:1048)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:1001)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:1071)\n\tat jdk.internal.reflect.GeneratedMethodAccessor593.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:197)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:117)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\n\t\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:87)\n\t\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:63)\n\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:251)\n\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapExceptionBase(ManagedCatalogClientImpl.scala:7378)\n\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:7364)\n\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.getTableCredentials(ManagedCatalogClientImpl.scala:3951)\n\t\tat com.databricks.sql.managedcatalog.ManagedCatalogClient.getTemporaryCredentials(ManagedCatalogClient.scala:2620)\n\t\tat com.databricks.sql.managedcatalog.ManagedCatalogClient.getTemporaryCredentials$(ManagedCatalogClient.scala:2605)\n\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.getTemporaryCredentials(ManagedCatalogClientImpl.scala:251)\n\t\tat com.databricks.unity.TempCredCache.$anonfun$getInternal$7(TemporaryCredentials.scala:448)\n\t\tat com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4724)\n\t\tat com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3522)\n\t\tat com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2315)\n\t\tat com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2278)\n\t\tat com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2193)\n\t\tat com.google.common.cache.LocalCache.get(LocalCache.java:3932)\n\t\tat com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4721)\n\t\tat com.databricks.unity.TempCredCache.liftedTree1$1(TemporaryCredentials.scala:447)\n\t\tat com.databricks.unity.TempCredCache.getInternal(TemporaryCredentials.scala:446)\n\t\tat com.databricks.unity.TempCredCache.get(TemporaryCredentials.scala:373)\n\t\tat com.databricks.unity.UnityCredentialManager.getTemporaryCredentials(CredentialManager.scala:488)\n\t\tat com.databricks.unity.CredentialManager$.getTemporaryCredentials(CredentialManager.scala:881)\n\t\tat com.databricks.unity.SAM.setConfFromCred$1(SAM.scala:338)\n\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$8(SAM.scala:353)\n\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$8$adapted(SAM.scala:352)\n\t\tat com.databricks.unity.TempCredRequest$.$anonfun$withFallbackOnOverlappingForParentPath$3(TemporaryCredentials.scala:147)\n\t\tat scala.util.Try$.apply(Try.scala:213)\n\t\tat com.databricks.unity.TempCredRequest$.withFallbackOnOverlappingForParentPath(TemporaryCredentials.scala:147)\n\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$7(SAM.scala:352)\n\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$7$adapted(SAM.scala:350)\n\t\tat scala.Option.map(Option.scala:230)\n\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$6(SAM.scala:350)\n\t\tat scala.Option.flatMap(Option.scala:271)\n\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$1(SAM.scala:349)\n\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$1$adapted(SAM.scala:336)\n\t\tat scala.Option.foreach(Option.scala:407)\n\t\tat com.databricks.unity.SAM.setFileSystemConfigs(SAM.scala:336)\n\t\tat com.databricks.unity.SAM.createDelegate(SAM.scala:388)\n\t\tat com.databricks.unity.SAM.createDelegate$(SAM.scala:372)\n\t\tat com.databricks.unity.TableSAM.createDelegate(SAM.scala:695)\n\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.createDelegate(CredentialScopeFileSystem.scala:74)\n\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.$anonfun$setDelegates$1(CredentialScopeFileSystem.scala:127)\n\t\tat com.databricks.sql.acl.fs.Lazy.apply(DelegatingFileSystem.scala:310)\n\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.readDelegateMakeQualified(CredentialScopeFileSystem.scala:282)\n\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.optimizedMakeQualifiedPath(CredentialScopeFileSystem.scala:277)\n\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.makeQualified(CredentialScopeFileSystem.scala:294)\n\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.apply(DeltaLog.scala:1284)\n\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.forTable(DeltaLog.scala:1087)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$5(DeltaTableV2.scala:175)\n\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$withAdditionalSnapshotInitializationUsageLogData$1(DeltaLog.scala:968)\n\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.withAdditionalSnapshotInitializationUsageLogData(DeltaLog.scala:969)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$2(DeltaTableV2.scala:169)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:700)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$1(DeltaTableV2.scala:150)\n\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\t\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n\t\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:83)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.deltaLog$lzycompute(DeltaTableV2.scala:150)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.deltaLog(DeltaTableV2.scala:148)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$8(DeltaTableV2.scala:264)\n\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$withAdditionalSnapshotInitializationUsageLogData$1(DeltaLog.scala:968)\n\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.withAdditionalSnapshotInitializationUsageLogData(DeltaLog.scala:969)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$7(DeltaTableV2.scala:263)\n\t\tat scala.Option.orElse(Option.scala:447)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$5(DeltaTableV2.scala:263)\n\t\tat scala.Option.orElse(Option.scala:447)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$2(DeltaTableV2.scala:260)\n\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\t\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n\t\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:83)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$1(DeltaTableV2.scala:290)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:700)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot$lzycompute(DeltaTableV2.scala:240)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot(DeltaTableV2.scala:238)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$tableSchema$2(DeltaTableV2.scala:317)\n\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.tableSchema$lzycompute(DeltaTableV2.scala:317)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.tableSchema(DeltaTableV2.scala:315)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.schema(DeltaTableV2.scala:322)\n\t\tat org.apache.spark.sql.connector.catalog.Table.columns(Table.java:68)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.columns(DeltaTableV2.scala:83)\n\t\tat org.apache.spark.sql.catalyst.analysis.ResolvedTable$.create(v2ResolutionPlans.scala:231)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$4(Analyzer.scala:1600)\n\t\tat scala.Option.map(Option.scala:230)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$3(Analyzer.scala:1592)\n\t\tat scala.Option.orElse(Option.scala:447)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableOrView(Analyzer.scala:1587)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1556)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1466)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:182)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:171)\n\t\tat scala.PartialFunction.applyOrElse(PartialFunction.scala:127)\n\t\tat scala.PartialFunction.applyOrElse$(PartialFunction.scala:126)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.applyOrElse(AnalysisHelper.scala:171)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)\n\t\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:138)\n\t\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1330)\n\t\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1329)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.DescribeRelation.mapChildren(v2Commands.scala:726)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning(AnalysisHelper.scala:186)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning$(AnalysisHelper.scala:167)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithSubqueriesAndPruning(LogicalPlan.scala:42)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.resolveOperatorsUpWithSubqueriesAndPruningByConf(Analyzer.scala:1674)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply0(Analyzer.scala:1466)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1419)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1365)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:480)\n\t\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:629)\n\t\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:613)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:131)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:480)\n\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:479)\n\t\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n\t\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n\t\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:475)\n\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:452)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22(RuleExecutor.scala:585)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22$adapted(RuleExecutor.scala:585)\n\t\tat scala.collection.immutable.List.foreach(List.scala:431)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:585)\n\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:349)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:498)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:491)\n\t\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:397)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:491)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:416)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:341)\n\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:219)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:341)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:252)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:96)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:131)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:87)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:478)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:478)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:294)\n\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:548)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:668)\n\t\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:151)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:668)\n\t\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1307)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:661)\n\t\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:658)\n\t\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n\t\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:658)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:288)\n\t\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:287)\n\t\tat scala.util.Try$.apply(Try.scala:213)\n\t\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)\n\t\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n\t\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n\t\t... 24 more\n",
         "2025-08-13T14:52:13.280398Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "table/view",
         "`accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample`.`forecast`.`us_postal_daynight_imperial`",
         "clone/create",
         "error",
         "Failed to process table/view `accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample`.`forecast`.`us_postal_daynight_imperial`. Error: An error occurred while calling o427.sql.\n: com.databricks.sql.managedcatalog.acl.UnauthorizedAccessException: PERMISSION_DENIED: User does not have USE CATALOG on Catalog 'accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample'.\n\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:87)\n\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:63)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:251)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapExceptionBase(ManagedCatalogClientImpl.scala:7378)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:7364)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.getTableCredentials(ManagedCatalogClientImpl.scala:3951)\n\tat com.databricks.sql.managedcatalog.ManagedCatalogClient.getTemporaryCredentials(ManagedCatalogClient.scala:2620)\n\tat com.databricks.sql.managedcatalog.ManagedCatalogClient.getTemporaryCredentials$(ManagedCatalogClient.scala:2605)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.getTemporaryCredentials(ManagedCatalogClientImpl.scala:251)\n\tat com.databricks.unity.TempCredCache.$anonfun$getInternal$7(TemporaryCredentials.scala:448)\n\tat com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4724)\n\tat com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3522)\n\tat com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2315)\n\tat com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2278)\n\tat com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2193)\n\tat com.google.common.cache.LocalCache.get(LocalCache.java:3932)\n\tat com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4721)\n\tat com.databricks.unity.TempCredCache.liftedTree1$1(TemporaryCredentials.scala:447)\n\tat com.databricks.unity.TempCredCache.getInternal(TemporaryCredentials.scala:446)\n\tat com.databricks.unity.TempCredCache.get(TemporaryCredentials.scala:373)\n\tat com.databricks.unity.UnityCredentialManager.getTemporaryCredentials(CredentialManager.scala:488)\n\tat com.databricks.unity.CredentialManager$.getTemporaryCredentials(CredentialManager.scala:881)\n\tat com.databricks.unity.SAM.setConfFromCred$1(SAM.scala:338)\n\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$8(SAM.scala:353)\n\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$8$adapted(SAM.scala:352)\n\tat com.databricks.unity.TempCredRequest$.$anonfun$withFallbackOnOverlappingForParentPath$3(TemporaryCredentials.scala:147)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.databricks.unity.TempCredRequest$.withFallbackOnOverlappingForParentPath(TemporaryCredentials.scala:147)\n\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$7(SAM.scala:352)\n\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$7$adapted(SAM.scala:350)\n\tat scala.Option.map(Option.scala:230)\n\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$6(SAM.scala:350)\n\tat scala.Option.flatMap(Option.scala:271)\n\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$1(SAM.scala:349)\n\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$1$adapted(SAM.scala:336)\n\tat scala.Option.foreach(Option.scala:407)\n\tat com.databricks.unity.SAM.setFileSystemConfigs(SAM.scala:336)\n\tat com.databricks.unity.SAM.createDelegate(SAM.scala:388)\n\tat com.databricks.unity.SAM.createDelegate$(SAM.scala:372)\n\tat com.databricks.unity.TableSAM.createDelegate(SAM.scala:695)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.createDelegate(CredentialScopeFileSystem.scala:74)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.$anonfun$setDelegates$1(CredentialScopeFileSystem.scala:127)\n\tat com.databricks.sql.acl.fs.Lazy.apply(DelegatingFileSystem.scala:310)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.readDelegateMakeQualified(CredentialScopeFileSystem.scala:282)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.optimizedMakeQualifiedPath(CredentialScopeFileSystem.scala:277)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.makeQualified(CredentialScopeFileSystem.scala:294)\n\tat com.databricks.sql.transaction.tahoe.DeltaLog$.apply(DeltaLog.scala:1284)\n\tat com.databricks.sql.transaction.tahoe.DeltaLog$.forTable(DeltaLog.scala:1087)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$5(DeltaTableV2.scala:175)\n\tat com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$withAdditionalSnapshotInitializationUsageLogData$1(DeltaLog.scala:968)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n\tat com.databricks.sql.transaction.tahoe.DeltaLog$.withAdditionalSnapshotInitializationUsageLogData(DeltaLog.scala:969)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$2(DeltaTableV2.scala:169)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:700)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$1(DeltaTableV2.scala:150)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:83)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.deltaLog$lzycompute(DeltaTableV2.scala:150)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.deltaLog(DeltaTableV2.scala:148)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$8(DeltaTableV2.scala:264)\n\tat com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$withAdditionalSnapshotInitializationUsageLogData$1(DeltaLog.scala:968)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n\tat com.databricks.sql.transaction.tahoe.DeltaLog$.withAdditionalSnapshotInitializationUsageLogData(DeltaLog.scala:969)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$7(DeltaTableV2.scala:263)\n\tat scala.Option.orElse(Option.scala:447)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$5(DeltaTableV2.scala:263)\n\tat scala.Option.orElse(Option.scala:447)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$2(DeltaTableV2.scala:260)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:83)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$1(DeltaTableV2.scala:290)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:700)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot$lzycompute(DeltaTableV2.scala:240)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot(DeltaTableV2.scala:238)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$tableSchema$2(DeltaTableV2.scala:317)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.tableSchema$lzycompute(DeltaTableV2.scala:317)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.tableSchema(DeltaTableV2.scala:315)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.schema(DeltaTableV2.scala:322)\n\tat org.apache.spark.sql.connector.catalog.Table.columns(Table.java:68)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.columns(DeltaTableV2.scala:83)\n\tat org.apache.spark.sql.catalyst.analysis.ResolvedTable$.create(v2ResolutionPlans.scala:231)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$4(Analyzer.scala:1600)\n\tat scala.Option.map(Option.scala:230)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$3(Analyzer.scala:1592)\n\tat scala.Option.orElse(Option.scala:447)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableOrView(Analyzer.scala:1587)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1556)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1466)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:182)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:171)\n\tat scala.PartialFunction.applyOrElse(PartialFunction.scala:127)\n\tat scala.PartialFunction.applyOrElse$(PartialFunction.scala:126)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.applyOrElse(AnalysisHelper.scala:171)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:138)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1330)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1329)\n\tat org.apache.spark.sql.catalyst.plans.logical.DescribeRelation.mapChildren(v2Commands.scala:726)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning(AnalysisHelper.scala:186)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning$(AnalysisHelper.scala:167)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithSubqueriesAndPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.resolveOperatorsUpWithSubqueriesAndPruningByConf(Analyzer.scala:1674)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply0(Analyzer.scala:1466)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1419)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1365)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:480)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:629)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:613)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:131)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:480)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:479)\n\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:475)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:452)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22(RuleExecutor.scala:585)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22$adapted(RuleExecutor.scala:585)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:585)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:349)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:498)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:491)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:397)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:491)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:416)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:341)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:219)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:341)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:252)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:96)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:131)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:87)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:478)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:478)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:294)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:548)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:668)\n\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:151)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:668)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1307)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:661)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:658)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:658)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:288)\n\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:287)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:321)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:267)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$3(Dataset.scala:149)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n\tat org.apache.spark.sql.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1469)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1469)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:141)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:1048)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:1001)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:1071)\n\tat jdk.internal.reflect.GeneratedMethodAccessor593.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:197)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:117)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\n\t\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:87)\n\t\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:63)\n\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:251)\n\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapExceptionBase(ManagedCatalogClientImpl.scala:7378)\n\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:7364)\n\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.getTableCredentials(ManagedCatalogClientImpl.scala:3951)\n\t\tat com.databricks.sql.managedcatalog.ManagedCatalogClient.getTemporaryCredentials(ManagedCatalogClient.scala:2620)\n\t\tat com.databricks.sql.managedcatalog.ManagedCatalogClient.getTemporaryCredentials$(ManagedCatalogClient.scala:2605)\n\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.getTemporaryCredentials(ManagedCatalogClientImpl.scala:251)\n\t\tat com.databricks.unity.TempCredCache.$anonfun$getInternal$7(TemporaryCredentials.scala:448)\n\t\tat com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4724)\n\t\tat com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3522)\n\t\tat com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2315)\n\t\tat com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2278)\n\t\tat com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2193)\n\t\tat com.google.common.cache.LocalCache.get(LocalCache.java:3932)\n\t\tat com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4721)\n\t\tat com.databricks.unity.TempCredCache.liftedTree1$1(TemporaryCredentials.scala:447)\n\t\tat com.databricks.unity.TempCredCache.getInternal(TemporaryCredentials.scala:446)\n\t\tat com.databricks.unity.TempCredCache.get(TemporaryCredentials.scala:373)\n\t\tat com.databricks.unity.UnityCredentialManager.getTemporaryCredentials(CredentialManager.scala:488)\n\t\tat com.databricks.unity.CredentialManager$.getTemporaryCredentials(CredentialManager.scala:881)\n\t\tat com.databricks.unity.SAM.setConfFromCred$1(SAM.scala:338)\n\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$8(SAM.scala:353)\n\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$8$adapted(SAM.scala:352)\n\t\tat com.databricks.unity.TempCredRequest$.$anonfun$withFallbackOnOverlappingForParentPath$3(TemporaryCredentials.scala:147)\n\t\tat scala.util.Try$.apply(Try.scala:213)\n\t\tat com.databricks.unity.TempCredRequest$.withFallbackOnOverlappingForParentPath(TemporaryCredentials.scala:147)\n\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$7(SAM.scala:352)\n\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$7$adapted(SAM.scala:350)\n\t\tat scala.Option.map(Option.scala:230)\n\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$6(SAM.scala:350)\n\t\tat scala.Option.flatMap(Option.scala:271)\n\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$1(SAM.scala:349)\n\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$1$adapted(SAM.scala:336)\n\t\tat scala.Option.foreach(Option.scala:407)\n\t\tat com.databricks.unity.SAM.setFileSystemConfigs(SAM.scala:336)\n\t\tat com.databricks.unity.SAM.createDelegate(SAM.scala:388)\n\t\tat com.databricks.unity.SAM.createDelegate$(SAM.scala:372)\n\t\tat com.databricks.unity.TableSAM.createDelegate(SAM.scala:695)\n\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.createDelegate(CredentialScopeFileSystem.scala:74)\n\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.$anonfun$setDelegates$1(CredentialScopeFileSystem.scala:127)\n\t\tat com.databricks.sql.acl.fs.Lazy.apply(DelegatingFileSystem.scala:310)\n\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.readDelegateMakeQualified(CredentialScopeFileSystem.scala:282)\n\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.optimizedMakeQualifiedPath(CredentialScopeFileSystem.scala:277)\n\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.makeQualified(CredentialScopeFileSystem.scala:294)\n\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.apply(DeltaLog.scala:1284)\n\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.forTable(DeltaLog.scala:1087)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$5(DeltaTableV2.scala:175)\n\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$withAdditionalSnapshotInitializationUsageLogData$1(DeltaLog.scala:968)\n\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.withAdditionalSnapshotInitializationUsageLogData(DeltaLog.scala:969)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$2(DeltaTableV2.scala:169)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:700)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$1(DeltaTableV2.scala:150)\n\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\t\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n\t\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:83)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.deltaLog$lzycompute(DeltaTableV2.scala:150)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.deltaLog(DeltaTableV2.scala:148)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$8(DeltaTableV2.scala:264)\n\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$withAdditionalSnapshotInitializationUsageLogData$1(DeltaLog.scala:968)\n\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.withAdditionalSnapshotInitializationUsageLogData(DeltaLog.scala:969)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$7(DeltaTableV2.scala:263)\n\t\tat scala.Option.orElse(Option.scala:447)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$5(DeltaTableV2.scala:263)\n\t\tat scala.Option.orElse(Option.scala:447)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$2(DeltaTableV2.scala:260)\n\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\t\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n\t\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:83)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$1(DeltaTableV2.scala:290)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:700)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot$lzycompute(DeltaTableV2.scala:240)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot(DeltaTableV2.scala:238)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$tableSchema$2(DeltaTableV2.scala:317)\n\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.tableSchema$lzycompute(DeltaTableV2.scala:317)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.tableSchema(DeltaTableV2.scala:315)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.schema(DeltaTableV2.scala:322)\n\t\tat org.apache.spark.sql.connector.catalog.Table.columns(Table.java:68)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.columns(DeltaTableV2.scala:83)\n\t\tat org.apache.spark.sql.catalyst.analysis.ResolvedTable$.create(v2ResolutionPlans.scala:231)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$4(Analyzer.scala:1600)\n\t\tat scala.Option.map(Option.scala:230)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$3(Analyzer.scala:1592)\n\t\tat scala.Option.orElse(Option.scala:447)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableOrView(Analyzer.scala:1587)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1556)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1466)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:182)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:171)\n\t\tat scala.PartialFunction.applyOrElse(PartialFunction.scala:127)\n\t\tat scala.PartialFunction.applyOrElse$(PartialFunction.scala:126)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.applyOrElse(AnalysisHelper.scala:171)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)\n\t\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:138)\n\t\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1330)\n\t\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1329)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.DescribeRelation.mapChildren(v2Commands.scala:726)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning(AnalysisHelper.scala:186)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning$(AnalysisHelper.scala:167)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithSubqueriesAndPruning(LogicalPlan.scala:42)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.resolveOperatorsUpWithSubqueriesAndPruningByConf(Analyzer.scala:1674)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply0(Analyzer.scala:1466)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1419)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1365)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:480)\n\t\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:629)\n\t\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:613)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:131)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:480)\n\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:479)\n\t\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n\t\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n\t\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:475)\n\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:452)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22(RuleExecutor.scala:585)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22$adapted(RuleExecutor.scala:585)\n\t\tat scala.collection.immutable.List.foreach(List.scala:431)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:585)\n\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:349)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:498)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:491)\n\t\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:397)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:491)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:416)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:341)\n\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:219)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:341)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:252)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:96)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:131)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:87)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:478)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:478)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:294)\n\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:548)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:668)\n\t\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:151)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:668)\n\t\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1307)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:661)\n\t\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:658)\n\t\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n\t\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:658)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:288)\n\t\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:287)\n\t\tat scala.util.Try$.apply(Try.scala:213)\n\t\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)\n\t\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n\t\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n\t\t... 24 more\n",
         "2025-08-13T14:52:12.808418Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "table",
         "`dr_eo_amperity`.`silver`.`unified_coalesced`",
         "clone",
         "success",
         "Table `dr_eo_amperity`.`silver`.`unified_coalesced` cloned successfully.",
         "2025-08-13T14:52:09.960439Z",
         [
          [
           "source_table_size",
           "25008603680"
          ],
          [
           "source_num_of_files",
           "234"
          ],
          [
           "num_of_synced_transactions",
           "None"
          ],
          [
           "num_removed_files",
           "0"
          ],
          [
           "num_copied_files",
           "0"
          ],
          [
           "removed_files_size",
           "0"
          ],
          [
           "copied_files_size",
           "0"
          ]
         ]
        ],
        [
         "DR_Clone_data_v2",
         "table",
         "`dr_eo_amperity`.`silver`.`unified_transactions`",
         "clone",
         "error",
         "Failed to clone table `dr_eo_amperity`.`silver`.`unified_transactions`.. Error: [DELTA_CLONE_UNSUPPORTED_SOURCE] Unsupported DEEP clone source '_eo_amperity.silver.unified_transactions', whose format is deltasharing.\nThe supported formats are 'delta', 'iceberg' and 'parquet'.",
         "2025-08-13T14:52:08.040833Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "table",
         "`dr_eo_amperity`.`silver`.`unified_scores`",
         "clone",
         "error",
         "Failed to clone table `dr_eo_amperity`.`silver`.`unified_scores`.. Error: [DELTA_CLONE_UNSUPPORTED_SOURCE] Unsupported DEEP clone source '_eo_amperity.silver.unified_scores', whose format is deltasharing.\nThe supported formats are 'delta', 'iceberg' and 'parquet'.",
         "2025-08-13T14:52:07.910126Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "table/view",
         "`accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample`.`forecast`.`us_postal_daily_metric`",
         "clone/create",
         "error",
         "Failed to process table/view `accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample`.`forecast`.`us_postal_daily_metric`. Error: An error occurred while calling o427.sql.\n: com.databricks.sql.managedcatalog.acl.UnauthorizedAccessException: PERMISSION_DENIED: User does not have USE CATALOG on Catalog 'accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample'.\n\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:87)\n\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:63)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:251)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapExceptionBase(ManagedCatalogClientImpl.scala:7378)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:7364)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.getTableCredentials(ManagedCatalogClientImpl.scala:3951)\n\tat com.databricks.sql.managedcatalog.ManagedCatalogClient.getTemporaryCredentials(ManagedCatalogClient.scala:2620)\n\tat com.databricks.sql.managedcatalog.ManagedCatalogClient.getTemporaryCredentials$(ManagedCatalogClient.scala:2605)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.getTemporaryCredentials(ManagedCatalogClientImpl.scala:251)\n\tat com.databricks.unity.TempCredCache.$anonfun$getInternal$7(TemporaryCredentials.scala:448)\n\tat com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4724)\n\tat com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3522)\n\tat com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2315)\n\tat com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2278)\n\tat com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2193)\n\tat com.google.common.cache.LocalCache.get(LocalCache.java:3932)\n\tat com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4721)\n\tat com.databricks.unity.TempCredCache.liftedTree1$1(TemporaryCredentials.scala:447)\n\tat com.databricks.unity.TempCredCache.getInternal(TemporaryCredentials.scala:446)\n\tat com.databricks.unity.TempCredCache.get(TemporaryCredentials.scala:373)\n\tat com.databricks.unity.UnityCredentialManager.getTemporaryCredentials(CredentialManager.scala:488)\n\tat com.databricks.unity.CredentialManager$.getTemporaryCredentials(CredentialManager.scala:881)\n\tat com.databricks.unity.SAM.setConfFromCred$1(SAM.scala:338)\n\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$8(SAM.scala:353)\n\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$8$adapted(SAM.scala:352)\n\tat com.databricks.unity.TempCredRequest$.$anonfun$withFallbackOnOverlappingForParentPath$3(TemporaryCredentials.scala:147)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.databricks.unity.TempCredRequest$.withFallbackOnOverlappingForParentPath(TemporaryCredentials.scala:147)\n\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$7(SAM.scala:352)\n\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$7$adapted(SAM.scala:350)\n\tat scala.Option.map(Option.scala:230)\n\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$6(SAM.scala:350)\n\tat scala.Option.flatMap(Option.scala:271)\n\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$1(SAM.scala:349)\n\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$1$adapted(SAM.scala:336)\n\tat scala.Option.foreach(Option.scala:407)\n\tat com.databricks.unity.SAM.setFileSystemConfigs(SAM.scala:336)\n\tat com.databricks.unity.SAM.createDelegate(SAM.scala:388)\n\tat com.databricks.unity.SAM.createDelegate$(SAM.scala:372)\n\tat com.databricks.unity.TableSAM.createDelegate(SAM.scala:695)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.createDelegate(CredentialScopeFileSystem.scala:74)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.$anonfun$setDelegates$1(CredentialScopeFileSystem.scala:127)\n\tat com.databricks.sql.acl.fs.Lazy.apply(DelegatingFileSystem.scala:310)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.readDelegateMakeQualified(CredentialScopeFileSystem.scala:282)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.optimizedMakeQualifiedPath(CredentialScopeFileSystem.scala:277)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.makeQualified(CredentialScopeFileSystem.scala:294)\n\tat com.databricks.sql.transaction.tahoe.DeltaLog$.apply(DeltaLog.scala:1284)\n\tat com.databricks.sql.transaction.tahoe.DeltaLog$.forTable(DeltaLog.scala:1087)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$5(DeltaTableV2.scala:175)\n\tat com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$withAdditionalSnapshotInitializationUsageLogData$1(DeltaLog.scala:968)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n\tat com.databricks.sql.transaction.tahoe.DeltaLog$.withAdditionalSnapshotInitializationUsageLogData(DeltaLog.scala:969)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$2(DeltaTableV2.scala:169)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:700)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$1(DeltaTableV2.scala:150)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:83)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.deltaLog$lzycompute(DeltaTableV2.scala:150)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.deltaLog(DeltaTableV2.scala:148)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$8(DeltaTableV2.scala:264)\n\tat com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$withAdditionalSnapshotInitializationUsageLogData$1(DeltaLog.scala:968)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n\tat com.databricks.sql.transaction.tahoe.DeltaLog$.withAdditionalSnapshotInitializationUsageLogData(DeltaLog.scala:969)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$7(DeltaTableV2.scala:263)\n\tat scala.Option.orElse(Option.scala:447)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$5(DeltaTableV2.scala:263)\n\tat scala.Option.orElse(Option.scala:447)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$2(DeltaTableV2.scala:260)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:83)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$1(DeltaTableV2.scala:290)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:700)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot$lzycompute(DeltaTableV2.scala:240)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot(DeltaTableV2.scala:238)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$tableSchema$2(DeltaTableV2.scala:317)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.tableSchema$lzycompute(DeltaTableV2.scala:317)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.tableSchema(DeltaTableV2.scala:315)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.schema(DeltaTableV2.scala:322)\n\tat org.apache.spark.sql.connector.catalog.Table.columns(Table.java:68)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.columns(DeltaTableV2.scala:83)\n\tat org.apache.spark.sql.catalyst.analysis.ResolvedTable$.create(v2ResolutionPlans.scala:231)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$4(Analyzer.scala:1600)\n\tat scala.Option.map(Option.scala:230)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$3(Analyzer.scala:1592)\n\tat scala.Option.orElse(Option.scala:447)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableOrView(Analyzer.scala:1587)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1556)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1466)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:182)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:171)\n\tat scala.PartialFunction.applyOrElse(PartialFunction.scala:127)\n\tat scala.PartialFunction.applyOrElse$(PartialFunction.scala:126)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.applyOrElse(AnalysisHelper.scala:171)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:138)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1330)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1329)\n\tat org.apache.spark.sql.catalyst.plans.logical.DescribeRelation.mapChildren(v2Commands.scala:726)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning(AnalysisHelper.scala:186)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning$(AnalysisHelper.scala:167)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithSubqueriesAndPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.resolveOperatorsUpWithSubqueriesAndPruningByConf(Analyzer.scala:1674)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply0(Analyzer.scala:1466)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1419)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1365)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:480)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:629)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:613)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:131)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:480)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:479)\n\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:475)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:452)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22(RuleExecutor.scala:585)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22$adapted(RuleExecutor.scala:585)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:585)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:349)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:498)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:491)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:397)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:491)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:416)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:341)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:219)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:341)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:252)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:96)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:131)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:87)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:478)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:478)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:294)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:548)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:668)\n\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:151)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:668)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1307)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:661)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:658)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:658)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:288)\n\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:287)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:321)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:267)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$3(Dataset.scala:149)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n\tat org.apache.spark.sql.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1469)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1469)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:141)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:1048)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:1001)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:1071)\n\tat jdk.internal.reflect.GeneratedMethodAccessor593.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:197)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:117)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\n\t\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:87)\n\t\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:63)\n\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:251)\n\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapExceptionBase(ManagedCatalogClientImpl.scala:7378)\n\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:7364)\n\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.getTableCredentials(ManagedCatalogClientImpl.scala:3951)\n\t\tat com.databricks.sql.managedcatalog.ManagedCatalogClient.getTemporaryCredentials(ManagedCatalogClient.scala:2620)\n\t\tat com.databricks.sql.managedcatalog.ManagedCatalogClient.getTemporaryCredentials$(ManagedCatalogClient.scala:2605)\n\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.getTemporaryCredentials(ManagedCatalogClientImpl.scala:251)\n\t\tat com.databricks.unity.TempCredCache.$anonfun$getInternal$7(TemporaryCredentials.scala:448)\n\t\tat com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4724)\n\t\tat com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3522)\n\t\tat com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2315)\n\t\tat com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2278)\n\t\tat com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2193)\n\t\tat com.google.common.cache.LocalCache.get(LocalCache.java:3932)\n\t\tat com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4721)\n\t\tat com.databricks.unity.TempCredCache.liftedTree1$1(TemporaryCredentials.scala:447)\n\t\tat com.databricks.unity.TempCredCache.getInternal(TemporaryCredentials.scala:446)\n\t\tat com.databricks.unity.TempCredCache.get(TemporaryCredentials.scala:373)\n\t\tat com.databricks.unity.UnityCredentialManager.getTemporaryCredentials(CredentialManager.scala:488)\n\t\tat com.databricks.unity.CredentialManager$.getTemporaryCredentials(CredentialManager.scala:881)\n\t\tat com.databricks.unity.SAM.setConfFromCred$1(SAM.scala:338)\n\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$8(SAM.scala:353)\n\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$8$adapted(SAM.scala:352)\n\t\tat com.databricks.unity.TempCredRequest$.$anonfun$withFallbackOnOverlappingForParentPath$3(TemporaryCredentials.scala:147)\n\t\tat scala.util.Try$.apply(Try.scala:213)\n\t\tat com.databricks.unity.TempCredRequest$.withFallbackOnOverlappingForParentPath(TemporaryCredentials.scala:147)\n\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$7(SAM.scala:352)\n\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$7$adapted(SAM.scala:350)\n\t\tat scala.Option.map(Option.scala:230)\n\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$6(SAM.scala:350)\n\t\tat scala.Option.flatMap(Option.scala:271)\n\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$1(SAM.scala:349)\n\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$1$adapted(SAM.scala:336)\n\t\tat scala.Option.foreach(Option.scala:407)\n\t\tat com.databricks.unity.SAM.setFileSystemConfigs(SAM.scala:336)\n\t\tat com.databricks.unity.SAM.createDelegate(SAM.scala:388)\n\t\tat com.databricks.unity.SAM.createDelegate$(SAM.scala:372)\n\t\tat com.databricks.unity.TableSAM.createDelegate(SAM.scala:695)\n\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.createDelegate(CredentialScopeFileSystem.scala:74)\n\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.$anonfun$setDelegates$1(CredentialScopeFileSystem.scala:127)\n\t\tat com.databricks.sql.acl.fs.Lazy.apply(DelegatingFileSystem.scala:310)\n\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.readDelegateMakeQualified(CredentialScopeFileSystem.scala:282)\n\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.optimizedMakeQualifiedPath(CredentialScopeFileSystem.scala:277)\n\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.makeQualified(CredentialScopeFileSystem.scala:294)\n\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.apply(DeltaLog.scala:1284)\n\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.forTable(DeltaLog.scala:1087)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$5(DeltaTableV2.scala:175)\n\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$withAdditionalSnapshotInitializationUsageLogData$1(DeltaLog.scala:968)\n\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.withAdditionalSnapshotInitializationUsageLogData(DeltaLog.scala:969)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$2(DeltaTableV2.scala:169)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:700)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$1(DeltaTableV2.scala:150)\n\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\t\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n\t\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:83)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.deltaLog$lzycompute(DeltaTableV2.scala:150)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.deltaLog(DeltaTableV2.scala:148)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$8(DeltaTableV2.scala:264)\n\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$withAdditionalSnapshotInitializationUsageLogData$1(DeltaLog.scala:968)\n\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.withAdditionalSnapshotInitializationUsageLogData(DeltaLog.scala:969)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$7(DeltaTableV2.scala:263)\n\t\tat scala.Option.orElse(Option.scala:447)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$5(DeltaTableV2.scala:263)\n\t\tat scala.Option.orElse(Option.scala:447)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$2(DeltaTableV2.scala:260)\n\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\t\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n\t\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:83)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$1(DeltaTableV2.scala:290)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:700)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot$lzycompute(DeltaTableV2.scala:240)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot(DeltaTableV2.scala:238)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$tableSchema$2(DeltaTableV2.scala:317)\n\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.tableSchema$lzycompute(DeltaTableV2.scala:317)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.tableSchema(DeltaTableV2.scala:315)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.schema(DeltaTableV2.scala:322)\n\t\tat org.apache.spark.sql.connector.catalog.Table.columns(Table.java:68)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.columns(DeltaTableV2.scala:83)\n\t\tat org.apache.spark.sql.catalyst.analysis.ResolvedTable$.create(v2ResolutionPlans.scala:231)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$4(Analyzer.scala:1600)\n\t\tat scala.Option.map(Option.scala:230)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$3(Analyzer.scala:1592)\n\t\tat scala.Option.orElse(Option.scala:447)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableOrView(Analyzer.scala:1587)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1556)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1466)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:182)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:171)\n\t\tat scala.PartialFunction.applyOrElse(PartialFunction.scala:127)\n\t\tat scala.PartialFunction.applyOrElse$(PartialFunction.scala:126)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.applyOrElse(AnalysisHelper.scala:171)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)\n\t\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:138)\n\t\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1330)\n\t\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1329)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.DescribeRelation.mapChildren(v2Commands.scala:726)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning(AnalysisHelper.scala:186)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning$(AnalysisHelper.scala:167)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithSubqueriesAndPruning(LogicalPlan.scala:42)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.resolveOperatorsUpWithSubqueriesAndPruningByConf(Analyzer.scala:1674)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply0(Analyzer.scala:1466)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1419)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1365)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:480)\n\t\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:629)\n\t\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:613)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:131)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:480)\n\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:479)\n\t\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n\t\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n\t\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:475)\n\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:452)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22(RuleExecutor.scala:585)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22$adapted(RuleExecutor.scala:585)\n\t\tat scala.collection.immutable.List.foreach(List.scala:431)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:585)\n\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:349)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:498)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:491)\n\t\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:397)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:491)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:416)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:341)\n\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:219)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:341)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:252)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:96)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:131)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:87)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:478)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:478)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:294)\n\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:548)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:668)\n\t\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:151)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:668)\n\t\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1307)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:661)\n\t\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:658)\n\t\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n\t\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:658)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:288)\n\t\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:287)\n\t\tat scala.util.Try$.apply(Try.scala:213)\n\t\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)\n\t\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n\t\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n\t\t... 24 more\n",
         "2025-08-13T14:52:07.592439Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "table/view",
         "`accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample`.`forecast`.`us_postal_daily_imperial`",
         "clone/create",
         "error",
         "Failed to process table/view `accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample`.`forecast`.`us_postal_daily_imperial`. Error: An error occurred while calling o427.sql.\n: com.databricks.sql.managedcatalog.acl.UnauthorizedAccessException: PERMISSION_DENIED: User does not have USE CATALOG on Catalog 'accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample'.\n\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:87)\n\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:63)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:251)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapExceptionBase(ManagedCatalogClientImpl.scala:7378)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:7364)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.getTableCredentials(ManagedCatalogClientImpl.scala:3951)\n\tat com.databricks.sql.managedcatalog.ManagedCatalogClient.getTemporaryCredentials(ManagedCatalogClient.scala:2620)\n\tat com.databricks.sql.managedcatalog.ManagedCatalogClient.getTemporaryCredentials$(ManagedCatalogClient.scala:2605)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.getTemporaryCredentials(ManagedCatalogClientImpl.scala:251)\n\tat com.databricks.unity.TempCredCache.$anonfun$getInternal$7(TemporaryCredentials.scala:448)\n\tat com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4724)\n\tat com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3522)\n\tat com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2315)\n\tat com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2278)\n\tat com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2193)\n\tat com.google.common.cache.LocalCache.get(LocalCache.java:3932)\n\tat com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4721)\n\tat com.databricks.unity.TempCredCache.liftedTree1$1(TemporaryCredentials.scala:447)\n\tat com.databricks.unity.TempCredCache.getInternal(TemporaryCredentials.scala:446)\n\tat com.databricks.unity.TempCredCache.get(TemporaryCredentials.scala:373)\n\tat com.databricks.unity.UnityCredentialManager.getTemporaryCredentials(CredentialManager.scala:488)\n\tat com.databricks.unity.CredentialManager$.getTemporaryCredentials(CredentialManager.scala:881)\n\tat com.databricks.unity.SAM.setConfFromCred$1(SAM.scala:338)\n\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$8(SAM.scala:353)\n\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$8$adapted(SAM.scala:352)\n\tat com.databricks.unity.TempCredRequest$.$anonfun$withFallbackOnOverlappingForParentPath$3(TemporaryCredentials.scala:147)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.databricks.unity.TempCredRequest$.withFallbackOnOverlappingForParentPath(TemporaryCredentials.scala:147)\n\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$7(SAM.scala:352)\n\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$7$adapted(SAM.scala:350)\n\tat scala.Option.map(Option.scala:230)\n\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$6(SAM.scala:350)\n\tat scala.Option.flatMap(Option.scala:271)\n\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$1(SAM.scala:349)\n\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$1$adapted(SAM.scala:336)\n\tat scala.Option.foreach(Option.scala:407)\n\tat com.databricks.unity.SAM.setFileSystemConfigs(SAM.scala:336)\n\tat com.databricks.unity.SAM.createDelegate(SAM.scala:388)\n\tat com.databricks.unity.SAM.createDelegate$(SAM.scala:372)\n\tat com.databricks.unity.TableSAM.createDelegate(SAM.scala:695)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.createDelegate(CredentialScopeFileSystem.scala:74)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.$anonfun$setDelegates$1(CredentialScopeFileSystem.scala:127)\n\tat com.databricks.sql.acl.fs.Lazy.apply(DelegatingFileSystem.scala:310)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.readDelegateMakeQualified(CredentialScopeFileSystem.scala:282)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.optimizedMakeQualifiedPath(CredentialScopeFileSystem.scala:277)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.makeQualified(CredentialScopeFileSystem.scala:294)\n\tat com.databricks.sql.transaction.tahoe.DeltaLog$.apply(DeltaLog.scala:1284)\n\tat com.databricks.sql.transaction.tahoe.DeltaLog$.forTable(DeltaLog.scala:1087)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$5(DeltaTableV2.scala:175)\n\tat com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$withAdditionalSnapshotInitializationUsageLogData$1(DeltaLog.scala:968)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n\tat com.databricks.sql.transaction.tahoe.DeltaLog$.withAdditionalSnapshotInitializationUsageLogData(DeltaLog.scala:969)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$2(DeltaTableV2.scala:169)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:700)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$1(DeltaTableV2.scala:150)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:83)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.deltaLog$lzycompute(DeltaTableV2.scala:150)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.deltaLog(DeltaTableV2.scala:148)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$8(DeltaTableV2.scala:264)\n\tat com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$withAdditionalSnapshotInitializationUsageLogData$1(DeltaLog.scala:968)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n\tat com.databricks.sql.transaction.tahoe.DeltaLog$.withAdditionalSnapshotInitializationUsageLogData(DeltaLog.scala:969)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$7(DeltaTableV2.scala:263)\n\tat scala.Option.orElse(Option.scala:447)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$5(DeltaTableV2.scala:263)\n\tat scala.Option.orElse(Option.scala:447)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$2(DeltaTableV2.scala:260)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:83)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$1(DeltaTableV2.scala:290)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:700)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot$lzycompute(DeltaTableV2.scala:240)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot(DeltaTableV2.scala:238)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$tableSchema$2(DeltaTableV2.scala:317)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.tableSchema$lzycompute(DeltaTableV2.scala:317)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.tableSchema(DeltaTableV2.scala:315)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.schema(DeltaTableV2.scala:322)\n\tat org.apache.spark.sql.connector.catalog.Table.columns(Table.java:68)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.columns(DeltaTableV2.scala:83)\n\tat org.apache.spark.sql.catalyst.analysis.ResolvedTable$.create(v2ResolutionPlans.scala:231)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$4(Analyzer.scala:1600)\n\tat scala.Option.map(Option.scala:230)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$3(Analyzer.scala:1592)\n\tat scala.Option.orElse(Option.scala:447)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableOrView(Analyzer.scala:1587)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1556)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1466)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:182)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:171)\n\tat scala.PartialFunction.applyOrElse(PartialFunction.scala:127)\n\tat scala.PartialFunction.applyOrElse$(PartialFunction.scala:126)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.applyOrElse(AnalysisHelper.scala:171)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:138)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1330)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1329)\n\tat org.apache.spark.sql.catalyst.plans.logical.DescribeRelation.mapChildren(v2Commands.scala:726)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning(AnalysisHelper.scala:186)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning$(AnalysisHelper.scala:167)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithSubqueriesAndPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.resolveOperatorsUpWithSubqueriesAndPruningByConf(Analyzer.scala:1674)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply0(Analyzer.scala:1466)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1419)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1365)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:480)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:629)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:613)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:131)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:480)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:479)\n\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:475)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:452)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22(RuleExecutor.scala:585)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22$adapted(RuleExecutor.scala:585)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:585)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:349)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:498)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:491)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:397)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:491)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:416)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:341)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:219)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:341)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:252)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:96)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:131)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:87)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:478)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:478)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:294)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:548)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:668)\n\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:151)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:668)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1307)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:661)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:658)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:658)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:288)\n\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:287)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:321)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:267)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$3(Dataset.scala:149)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n\tat org.apache.spark.sql.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1469)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1469)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:141)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:1048)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:1001)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:1071)\n\tat jdk.internal.reflect.GeneratedMethodAccessor593.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:197)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:117)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\n\t\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:87)\n\t\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:63)\n\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:251)\n\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapExceptionBase(ManagedCatalogClientImpl.scala:7378)\n\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:7364)\n\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.getTableCredentials(ManagedCatalogClientImpl.scala:3951)\n\t\tat com.databricks.sql.managedcatalog.ManagedCatalogClient.getTemporaryCredentials(ManagedCatalogClient.scala:2620)\n\t\tat com.databricks.sql.managedcatalog.ManagedCatalogClient.getTemporaryCredentials$(ManagedCatalogClient.scala:2605)\n\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.getTemporaryCredentials(ManagedCatalogClientImpl.scala:251)\n\t\tat com.databricks.unity.TempCredCache.$anonfun$getInternal$7(TemporaryCredentials.scala:448)\n\t\tat com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4724)\n\t\tat com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3522)\n\t\tat com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2315)\n\t\tat com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2278)\n\t\tat com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2193)\n\t\tat com.google.common.cache.LocalCache.get(LocalCache.java:3932)\n\t\tat com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4721)\n\t\tat com.databricks.unity.TempCredCache.liftedTree1$1(TemporaryCredentials.scala:447)\n\t\tat com.databricks.unity.TempCredCache.getInternal(TemporaryCredentials.scala:446)\n\t\tat com.databricks.unity.TempCredCache.get(TemporaryCredentials.scala:373)\n\t\tat com.databricks.unity.UnityCredentialManager.getTemporaryCredentials(CredentialManager.scala:488)\n\t\tat com.databricks.unity.CredentialManager$.getTemporaryCredentials(CredentialManager.scala:881)\n\t\tat com.databricks.unity.SAM.setConfFromCred$1(SAM.scala:338)\n\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$8(SAM.scala:353)\n\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$8$adapted(SAM.scala:352)\n\t\tat com.databricks.unity.TempCredRequest$.$anonfun$withFallbackOnOverlappingForParentPath$3(TemporaryCredentials.scala:147)\n\t\tat scala.util.Try$.apply(Try.scala:213)\n\t\tat com.databricks.unity.TempCredRequest$.withFallbackOnOverlappingForParentPath(TemporaryCredentials.scala:147)\n\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$7(SAM.scala:352)\n\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$7$adapted(SAM.scala:350)\n\t\tat scala.Option.map(Option.scala:230)\n\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$6(SAM.scala:350)\n\t\tat scala.Option.flatMap(Option.scala:271)\n\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$1(SAM.scala:349)\n\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$1$adapted(SAM.scala:336)\n\t\tat scala.Option.foreach(Option.scala:407)\n\t\tat com.databricks.unity.SAM.setFileSystemConfigs(SAM.scala:336)\n\t\tat com.databricks.unity.SAM.createDelegate(SAM.scala:388)\n\t\tat com.databricks.unity.SAM.createDelegate$(SAM.scala:372)\n\t\tat com.databricks.unity.TableSAM.createDelegate(SAM.scala:695)\n\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.createDelegate(CredentialScopeFileSystem.scala:74)\n\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.$anonfun$setDelegates$1(CredentialScopeFileSystem.scala:127)\n\t\tat com.databricks.sql.acl.fs.Lazy.apply(DelegatingFileSystem.scala:310)\n\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.readDelegateMakeQualified(CredentialScopeFileSystem.scala:282)\n\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.optimizedMakeQualifiedPath(CredentialScopeFileSystem.scala:277)\n\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.makeQualified(CredentialScopeFileSystem.scala:294)\n\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.apply(DeltaLog.scala:1284)\n\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.forTable(DeltaLog.scala:1087)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$5(DeltaTableV2.scala:175)\n\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$withAdditionalSnapshotInitializationUsageLogData$1(DeltaLog.scala:968)\n\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.withAdditionalSnapshotInitializationUsageLogData(DeltaLog.scala:969)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$2(DeltaTableV2.scala:169)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:700)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$1(DeltaTableV2.scala:150)\n\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\t\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n\t\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:83)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.deltaLog$lzycompute(DeltaTableV2.scala:150)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.deltaLog(DeltaTableV2.scala:148)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$8(DeltaTableV2.scala:264)\n\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$withAdditionalSnapshotInitializationUsageLogData$1(DeltaLog.scala:968)\n\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.withAdditionalSnapshotInitializationUsageLogData(DeltaLog.scala:969)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$7(DeltaTableV2.scala:263)\n\t\tat scala.Option.orElse(Option.scala:447)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$5(DeltaTableV2.scala:263)\n\t\tat scala.Option.orElse(Option.scala:447)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$2(DeltaTableV2.scala:260)\n\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\t\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n\t\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:83)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$1(DeltaTableV2.scala:290)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:700)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot$lzycompute(DeltaTableV2.scala:240)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot(DeltaTableV2.scala:238)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$tableSchema$2(DeltaTableV2.scala:317)\n\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.tableSchema$lzycompute(DeltaTableV2.scala:317)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.tableSchema(DeltaTableV2.scala:315)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.schema(DeltaTableV2.scala:322)\n\t\tat org.apache.spark.sql.connector.catalog.Table.columns(Table.java:68)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.columns(DeltaTableV2.scala:83)\n\t\tat org.apache.spark.sql.catalyst.analysis.ResolvedTable$.create(v2ResolutionPlans.scala:231)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$4(Analyzer.scala:1600)\n\t\tat scala.Option.map(Option.scala:230)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$3(Analyzer.scala:1592)\n\t\tat scala.Option.orElse(Option.scala:447)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableOrView(Analyzer.scala:1587)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1556)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1466)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:182)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:171)\n\t\tat scala.PartialFunction.applyOrElse(PartialFunction.scala:127)\n\t\tat scala.PartialFunction.applyOrElse$(PartialFunction.scala:126)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.applyOrElse(AnalysisHelper.scala:171)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)\n\t\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:138)\n\t\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1330)\n\t\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1329)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.DescribeRelation.mapChildren(v2Commands.scala:726)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning(AnalysisHelper.scala:186)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning$(AnalysisHelper.scala:167)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithSubqueriesAndPruning(LogicalPlan.scala:42)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.resolveOperatorsUpWithSubqueriesAndPruningByConf(Analyzer.scala:1674)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply0(Analyzer.scala:1466)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1419)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1365)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:480)\n\t\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:629)\n\t\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:613)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:131)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:480)\n\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:479)\n\t\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n\t\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n\t\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:475)\n\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:452)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22(RuleExecutor.scala:585)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22$adapted(RuleExecutor.scala:585)\n\t\tat scala.collection.immutable.List.foreach(List.scala:431)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:585)\n\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:349)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:498)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:491)\n\t\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:397)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:491)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:416)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:341)\n\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:219)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:341)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:252)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:96)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:131)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:87)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:478)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:478)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:294)\n\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:548)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:668)\n\t\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:151)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:668)\n\t\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1307)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:661)\n\t\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:658)\n\t\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n\t\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:658)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:288)\n\t\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:287)\n\t\tat scala.util.Try$.apply(Try.scala:213)\n\t\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)\n\t\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n\t\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n\t\t... 24 more\n",
         "2025-08-13T14:52:07.57649Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "table",
         "`dr_eo_amperity`.`silver`.`unified_merged`",
         "clone",
         "error",
         "Failed to clone table `dr_eo_amperity`.`silver`.`unified_merged`.. Error: [DELTA_CLONE_UNSUPPORTED_SOURCE] Unsupported DEEP clone source '_eo_amperity.silver.unified_merged', whose format is deltasharing.\nThe supported formats are 'delta', 'iceberg' and 'parquet'.",
         "2025-08-13T14:51:51.756756Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "table",
         "`dr_eo_amperity`.`silver`.`unified_loyalty_events`",
         "clone",
         "error",
         "Failed to clone table `dr_eo_amperity`.`silver`.`unified_loyalty_events`.. Error: [DELTA_CLONE_UNSUPPORTED_SOURCE] Unsupported DEEP clone source '_eo_amperity.silver.unified_loyalty_events', whose format is deltasharing.\nThe supported formats are 'delta', 'iceberg' and 'parquet'.",
         "2025-08-13T14:51:51.697674Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "table",
         "`dr_eo_amperity`.`silver`.`unified_paid_media`",
         "clone",
         "error",
         "Failed to clone table `dr_eo_amperity`.`silver`.`unified_paid_media`.. Error: [DELTA_CLONE_UNSUPPORTED_SOURCE] Unsupported DEEP clone source '_eo_amperity.silver.unified_paid_media', whose format is deltasharing.\nThe supported formats are 'delta', 'iceberg' and 'parquet'.",
         "2025-08-13T14:51:51.683968Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "table",
         "`dr_eo_amperity`.`silver`.`unified_loyalty`",
         "clone",
         "error",
         "Failed to clone table `dr_eo_amperity`.`silver`.`unified_loyalty`.. Error: [DELTA_CLONE_UNSUPPORTED_SOURCE] Unsupported DEEP clone source '_eo_amperity.silver.unified_loyalty', whose format is deltasharing.\nThe supported formats are 'delta', 'iceberg' and 'parquet'.",
         "2025-08-13T14:51:51.367173Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "table",
         "`dr_eo_amperity`.`silver`.`unified_itemized_transactions`",
         "clone",
         "error",
         "Failed to clone table `dr_eo_amperity`.`silver`.`unified_itemized_transactions`.. Error: [DELTA_CLONE_UNSUPPORTED_SOURCE] Unsupported DEEP clone source '_eo_amperity.silver.unified_itemized_transactions', whose format is deltasharing.\nThe supported formats are 'delta', 'iceberg' and 'parquet'.",
         "2025-08-13T14:51:47.305991Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "table",
         "`dr_eo_amperity`.`silver`.`unified_household`",
         "clone",
         "error",
         "Failed to clone table `dr_eo_amperity`.`silver`.`unified_household`.. Error: [DELTA_CLONE_UNSUPPORTED_SOURCE] Unsupported DEEP clone source '_eo_amperity.silver.unified_household', whose format is deltasharing.\nThe supported formats are 'delta', 'iceberg' and 'parquet'.",
         "2025-08-13T14:51:47.113467Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "table",
         "`dr_eo_amperity`.`silver`.`unified_customer`",
         "clone",
         "error",
         "Failed to clone table `dr_eo_amperity`.`silver`.`unified_customer`.. Error: [DELTA_CLONE_UNSUPPORTED_SOURCE] Unsupported DEEP clone source '_eo_amperity.silver.unified_customer', whose format is deltasharing.\nThe supported formats are 'delta', 'iceberg' and 'parquet'.",
         "2025-08-13T14:51:46.924336Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "table",
         "`dr_eo_amperity`.`silver`.`unified_compliance`",
         "clone",
         "error",
         "Failed to clone table `dr_eo_amperity`.`silver`.`unified_compliance`.. Error: [DELTA_CLONE_UNSUPPORTED_SOURCE] Unsupported DEEP clone source '_eo_amperity.silver.unified_compliance', whose format is deltasharing.\nThe supported formats are 'delta', 'iceberg' and 'parquet'.",
         "2025-08-13T14:51:46.836848Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "table",
         "`dr_eo_amperity`.`silver`.`fiscal_calendar`",
         "clone",
         "error",
         "Failed to clone table `dr_eo_amperity`.`silver`.`fiscal_calendar`.. Error: [DELTA_CLONE_UNSUPPORTED_SOURCE] Unsupported DEEP clone source '_eo_amperity.silver.fiscal_calendar', whose format is deltasharing.\nThe supported formats are 'delta', 'iceberg' and 'parquet'.",
         "2025-08-13T14:51:42.681901Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "table",
         "`dr_eo_amperity`.`default`.`transaction_attributes_extended`",
         "clone",
         "error",
         "Failed to clone table `dr_eo_amperity`.`default`.`transaction_attributes_extended`.. Error: [DELTA_CLONE_UNSUPPORTED_SOURCE] Unsupported DEEP clone source '_eo_amperity.default.transaction_attributes_extended', whose format is deltasharing.\nThe supported formats are 'delta', 'iceberg' and 'parquet'.",
         "2025-08-13T14:51:42.606026Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "table",
         "`dr_eo_amperity`.`default`.`unified_transactions_reporting`",
         "clone",
         "error",
         "Failed to clone table `dr_eo_amperity`.`default`.`unified_transactions_reporting`.. Error: [DELTA_CLONE_UNSUPPORTED_SOURCE] Unsupported DEEP clone source '_eo_amperity.default.unified_transactions_reporting', whose format is deltasharing.\nThe supported formats are 'delta', 'iceberg' and 'parquet'.",
         "2025-08-13T14:51:42.545915Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "table",
         "`dr_eo_amperity`.`default`.`transaction_attributes`",
         "clone",
         "error",
         "Failed to clone table `dr_eo_amperity`.`default`.`transaction_attributes`.. Error: [DELTA_CLONE_UNSUPPORTED_SOURCE] Unsupported DEEP clone source '_eo_amperity.default.transaction_attributes', whose format is deltasharing.\nThe supported formats are 'delta', 'iceberg' and 'parquet'.",
         "2025-08-13T14:51:42.521191Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "table",
         "`dr_eo_amperity`.`default`.`predicted_clv_attributes`",
         "clone",
         "error",
         "Failed to clone table `dr_eo_amperity`.`default`.`predicted_clv_attributes`.. Error: [DELTA_CLONE_UNSUPPORTED_SOURCE] Unsupported DEEP clone source '_eo_amperity.default.predicted_clv_attributes', whose format is deltasharing.\nThe supported formats are 'delta', 'iceberg' and 'parquet'.",
         "2025-08-13T14:51:42.271169Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "table",
         "`dr_eo_amperity`.`bronze`.`wifi_connections`",
         "clone",
         "success",
         "Table `dr_eo_amperity`.`bronze`.`wifi_connections` cloned successfully.",
         "2025-08-13T14:51:39.568689Z",
         [
          [
           "source_table_size",
           "7882742359"
          ],
          [
           "source_num_of_files",
           "52"
          ],
          [
           "num_of_synced_transactions",
           "None"
          ],
          [
           "num_removed_files",
           "0"
          ],
          [
           "num_copied_files",
           "0"
          ],
          [
           "removed_files_size",
           "0"
          ],
          [
           "copied_files_size",
           "0"
          ]
         ]
        ],
        [
         "DR_Clone_data_v2",
         "table",
         "`dr_eo_amperity`.`bronze`.`web_visitors`",
         "clone",
         "success",
         "Table `dr_eo_amperity`.`bronze`.`web_visitors` cloned successfully.",
         "2025-08-13T14:51:39.481785Z",
         [
          [
           "source_table_size",
           "334349516"
          ],
          [
           "source_num_of_files",
           "7"
          ],
          [
           "num_of_synced_transactions",
           "None"
          ],
          [
           "num_removed_files",
           "0"
          ],
          [
           "num_copied_files",
           "0"
          ],
          [
           "removed_files_size",
           "0"
          ],
          [
           "copied_files_size",
           "0"
          ]
         ]
        ],
        [
         "DR_Clone_data_v2",
         "table",
         "`dr_eo_amperity`.`default`.`email_engagement_summary`",
         "clone",
         "error",
         "Failed to clone table `dr_eo_amperity`.`default`.`email_engagement_summary`.. Error: [DELTA_CLONE_UNSUPPORTED_SOURCE] Unsupported DEEP clone source '_eo_amperity.default.email_engagement_summary', whose format is deltasharing.\nThe supported formats are 'delta', 'iceberg' and 'parquet'.",
         "2025-08-13T14:51:35.794341Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "table",
         "`dr_eo_amperity`.`default`.`predicted_affinity`",
         "clone",
         "error",
         "Failed to clone table `dr_eo_amperity`.`default`.`predicted_affinity`.. Error: [DELTA_CLONE_UNSUPPORTED_SOURCE] Unsupported DEEP clone source '_eo_amperity.default.predicted_affinity', whose format is deltasharing.\nThe supported formats are 'delta', 'iceberg' and 'parquet'.",
         "2025-08-13T14:51:35.79349Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "table",
         "`dr_eo_amperity`.`default`.`email_engagement_attributes`",
         "clone",
         "error",
         "Failed to clone table `dr_eo_amperity`.`default`.`email_engagement_attributes`.. Error: [DELTA_CLONE_UNSUPPORTED_SOURCE] Unsupported DEEP clone source '_eo_amperity.default.email_engagement_attributes', whose format is deltasharing.\nThe supported formats are 'delta', 'iceberg' and 'parquet'.",
         "2025-08-13T14:51:35.52174Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "table",
         "`dr_eo_amperity`.`default`.`customer_attributes`",
         "clone",
         "error",
         "Failed to clone table `dr_eo_amperity`.`default`.`customer_attributes`.. Error: [DELTA_CLONE_UNSUPPORTED_SOURCE] Unsupported DEEP clone source '_eo_amperity.default.customer_attributes', whose format is deltasharing.\nThe supported formats are 'delta', 'iceberg' and 'parquet'.",
         "2025-08-13T14:51:00.347108Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "table",
         "`dr_eo_amperity`.`default`.`customer_360`",
         "clone",
         "error",
         "Failed to clone table `dr_eo_amperity`.`default`.`customer_360`.. Error: [DELTA_CLONE_UNSUPPORTED_SOURCE] Unsupported DEEP clone source '_eo_amperity.default.customer_360', whose format is deltasharing.\nThe supported formats are 'delta', 'iceberg' and 'parquet'.",
         "2025-08-13T14:51:00.340051Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "table",
         "`dr_eo_amperity`.`default`.`analytics_tae`",
         "clone",
         "error",
         "Failed to clone table `dr_eo_amperity`.`default`.`analytics_tae`.. Error: [DELTA_CLONE_UNSUPPORTED_SOURCE] Unsupported DEEP clone source '_eo_amperity.default.analytics_tae', whose format is deltasharing.\nThe supported formats are 'delta', 'iceberg' and 'parquet'.",
         "2025-08-13T14:51:00.195134Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "table",
         "`dr_eo_amperity`.`bronze`.`loyalty_members`",
         "clone",
         "error",
         "Failed to clone table `dr_eo_amperity`.`bronze`.`loyalty_members`.. Error: [DELTA_CLONE_UNSUPPORTED_SOURCE] Unsupported DEEP clone source '_eo_amperity.bronze.loyalty_members', whose format is deltasharing.\nThe supported formats are 'delta', 'iceberg' and 'parquet'.",
         "2025-08-13T14:50:56.183075Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "table",
         "`dr_eo_amperity`.`bronze`.`pos_customers`",
         "clone",
         "error",
         "Failed to clone table `dr_eo_amperity`.`bronze`.`pos_customers`.. Error: [DELTA_CLONE_UNSUPPORTED_SOURCE] Unsupported DEEP clone source '_eo_amperity.bronze.pos_customers', whose format is deltasharing.\nThe supported formats are 'delta', 'iceberg' and 'parquet'.",
         "2025-08-13T14:50:56.0049Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "table",
         "`dr_eo_amperity`.`bronze`.`e_commerce_customers`",
         "clone",
         "error",
         "Failed to clone table `dr_eo_amperity`.`bronze`.`e_commerce_customers`.. Error: [DELTA_CLONE_UNSUPPORTED_SOURCE] Unsupported DEEP clone source '_eo_amperity.bronze.e_commerce_customers', whose format is deltasharing.\nThe supported formats are 'delta', 'iceberg' and 'parquet'.",
         "2025-08-13T14:50:55.962848Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "catalog_pair",
         "satya_share -> satya_dr_catalog",
         "process",
         "sequential_complete",
         "Finished sequential processing for catalog pair: satya_share -> satya_dr_catalog",
         "2025-08-13T14:50:53.422799Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "schema",
         "`satya_dr_catalog`.`satyendranath_sure`",
         "change_ownership",
         "skipped",
         "No ownership mapping found for this schema. Skipping ownership change.",
         "2025-08-13T14:50:51.832389Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "schema",
         "`satya_dr_catalog`.`satyendranath_sure`",
         "create",
         "success",
         "Schema `satya_dr_catalog`.`satyendranath_sure` created or already exists.",
         "2025-08-13T14:50:50.430502Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "schema",
         "`satya_share`.`information_schema`",
         "skip",
         "success",
         "Skipping `information_schema`.",
         "2025-08-13T14:50:48.987385Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "catalog",
         "`satya_dr_catalog`",
         "create",
         "success",
         "Catalog `satya_dr_catalog` created or already exists.",
         "2025-08-13T14:50:46.482613Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "catalog_pair",
         "satya_share -> satya_dr_catalog",
         "process",
         "start",
         "Starting processing for catalog pair: satya_share -> satya_dr_catalog",
         "2025-08-13T14:50:44.845345Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "catalog",
         "`test_not`",
         "get_schemas",
         "error",
         "Failed to get schemas from source catalog `test_not`. Error: [NO_SUCH_CATALOG_EXCEPTION] Catalog 'test_not' was not found. Please verify the catalog name and then retry the query or command again. SQLSTATE: 42704",
         "2025-08-13T14:50:42.940357Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "catalog",
         "`dr_test_not`",
         "create",
         "success",
         "Catalog `dr_test_not` created or already exists.",
         "2025-08-13T14:50:41.400151Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "catalog_pair",
         "test_not -> dr_test_not",
         "process",
         "start",
         "Starting processing for catalog pair: test_not -> dr_test_not",
         "2025-08-13T14:50:39.749677Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "catalog_pair",
         "accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample -> dr_accuweather",
         "process",
         "sequential_complete",
         "Finished sequential processing for catalog pair: accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample -> dr_accuweather",
         "2025-08-13T14:50:38.223289Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "schema",
         "`accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample`.`information_schema`",
         "skip",
         "success",
         "Skipping `information_schema`.",
         "2025-08-13T14:50:36.711376Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "schema",
         "`dr_accuweather`.`forecast`",
         "change_ownership",
         "skipped",
         "No ownership mapping found for this schema. Skipping ownership change.",
         "2025-08-13T14:50:34.051606Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "schema",
         "`dr_accuweather`.`forecast`",
         "create",
         "success",
         "Schema `dr_accuweather`.`forecast` created or already exists.",
         "2025-08-13T14:50:32.192943Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "catalog",
         "`dr_accuweather`",
         "create",
         "success",
         "Catalog `dr_accuweather` created or already exists.",
         "2025-08-13T14:50:29.233097Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "catalog_pair",
         "accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample -> dr_accuweather",
         "process",
         "start",
         "Starting processing for catalog pair: accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample -> dr_accuweather",
         "2025-08-13T14:50:27.452586Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "catalog_pair",
         "_eo_amperity -> dr_eo_amperity",
         "process",
         "sequential_complete",
         "Finished sequential processing for catalog pair: _eo_amperity -> dr_eo_amperity",
         "2025-08-13T14:50:26.026816Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "schema",
         "`dr_eo_amperity`.`silver`",
         "change_ownership",
         "success",
         "Ownership of schema `dr_eo_amperity`.`silver` changed to `grp_test_clt_usa`.",
         "2025-08-13T14:50:24.429794Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "schema",
         "`dr_eo_amperity`.`silver`",
         "create",
         "success",
         "Schema `dr_eo_amperity`.`silver` created or already exists.",
         "2025-08-13T14:50:22.368961Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "schema",
         "`_eo_amperity`.`information_schema`",
         "skip",
         "success",
         "Skipping `information_schema`.",
         "2025-08-13T14:50:20.451951Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "schema",
         "`dr_eo_amperity`.`default`",
         "change_ownership",
         "success",
         "Ownership of schema `dr_eo_amperity`.`default` changed to `grp_test_clt_usa`.",
         "2025-08-13T14:50:18.733137Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "schema",
         "`dr_eo_amperity`.`default`",
         "create",
         "success",
         "Schema `dr_eo_amperity`.`default` created or already exists.",
         "2025-08-13T14:50:16.860986Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "schema",
         "`dr_eo_amperity`.`bronze`",
         "change_ownership",
         "success",
         "Ownership of schema `dr_eo_amperity`.`bronze` changed to `grp_test_clt_usa`.",
         "2025-08-13T14:50:14.866714Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "schema",
         "`dr_eo_amperity`.`bronze`",
         "create",
         "success",
         "Schema `dr_eo_amperity`.`bronze` created or already exists.",
         "2025-08-13T14:50:12.872357Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "catalog",
         "`dr_eo_amperity`",
         "create",
         "success",
         "Catalog `dr_eo_amperity` created or already exists.",
         "2025-08-13T14:50:10.517933Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "catalog_pair",
         "_eo_amperity -> dr_eo_amperity",
         "process",
         "start",
         "Starting processing for catalog pair: _eo_amperity -> dr_eo_amperity",
         "2025-08-13T14:49:55.847726Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "table",
         "`satya_dr_catalog`.`satyendranath_sure`.`satya_managed_table`",
         "clone",
         "success",
         "Table `satya_dr_catalog`.`satyendranath_sure`.`satya_managed_table` cloned successfully.",
         "2025-08-13T14:38:23.717714Z",
         [
          [
           "source_table_size",
           "827"
          ],
          [
           "source_num_of_files",
           "1"
          ],
          [
           "num_of_synced_transactions",
           "None"
          ],
          [
           "num_removed_files",
           "0"
          ],
          [
           "num_copied_files",
           "0"
          ],
          [
           "removed_files_size",
           "0"
          ],
          [
           "copied_files_size",
           "0"
          ]
         ]
        ],
        [
         "DR_Clone_data_v2",
         "table",
         "`satya_dr_catalog`.`satyendranath_sure`.`satya_external_table`",
         "clone",
         "success",
         "Table `satya_dr_catalog`.`satyendranath_sure`.`satya_external_table` cloned successfully.",
         "2025-08-13T14:38:22.387917Z",
         [
          [
           "source_table_size",
           "826"
          ],
          [
           "source_num_of_files",
           "1"
          ],
          [
           "num_of_synced_transactions",
           "None"
          ],
          [
           "num_removed_files",
           "0"
          ],
          [
           "num_copied_files",
           "0"
          ],
          [
           "removed_files_size",
           "0"
          ],
          [
           "copied_files_size",
           "0"
          ]
         ]
        ],
        [
         "DR_Clone_data_v2",
         "table/view",
         "`accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample`.`forecast`.`us_postal_daynight_imperial`",
         "clone/create",
         "error",
         "Failed to process table/view `accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample`.`forecast`.`us_postal_daynight_imperial`. Error: An error occurred while calling o427.sql.\n: com.databricks.sql.managedcatalog.acl.UnauthorizedAccessException: PERMISSION_DENIED: User does not have USE CATALOG on Catalog 'accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample'.\n\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:87)\n\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:63)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:251)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapExceptionBase(ManagedCatalogClientImpl.scala:7378)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:7364)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.getTableCredentials(ManagedCatalogClientImpl.scala:3951)\n\tat com.databricks.sql.managedcatalog.ManagedCatalogClient.getTemporaryCredentials(ManagedCatalogClient.scala:2620)\n\tat com.databricks.sql.managedcatalog.ManagedCatalogClient.getTemporaryCredentials$(ManagedCatalogClient.scala:2605)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.getTemporaryCredentials(ManagedCatalogClientImpl.scala:251)\n\tat com.databricks.unity.TempCredCache.$anonfun$getInternal$7(TemporaryCredentials.scala:448)\n\tat com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4724)\n\tat com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3522)\n\tat com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2315)\n\tat com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2278)\n\tat com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2193)\n\tat com.google.common.cache.LocalCache.get(LocalCache.java:3932)\n\tat com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4721)\n\tat com.databricks.unity.TempCredCache.liftedTree1$1(TemporaryCredentials.scala:447)\n\tat com.databricks.unity.TempCredCache.getInternal(TemporaryCredentials.scala:446)\n\tat com.databricks.unity.TempCredCache.get(TemporaryCredentials.scala:373)\n\tat com.databricks.unity.UnityCredentialManager.getTemporaryCredentials(CredentialManager.scala:488)\n\tat com.databricks.unity.CredentialManager$.getTemporaryCredentials(CredentialManager.scala:881)\n\tat com.databricks.unity.SAM.setConfFromCred$1(SAM.scala:338)\n\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$8(SAM.scala:353)\n\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$8$adapted(SAM.scala:352)\n\tat com.databricks.unity.TempCredRequest$.$anonfun$withFallbackOnOverlappingForParentPath$3(TemporaryCredentials.scala:147)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.databricks.unity.TempCredRequest$.withFallbackOnOverlappingForParentPath(TemporaryCredentials.scala:147)\n\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$7(SAM.scala:352)\n\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$7$adapted(SAM.scala:350)\n\tat scala.Option.map(Option.scala:230)\n\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$6(SAM.scala:350)\n\tat scala.Option.flatMap(Option.scala:271)\n\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$1(SAM.scala:349)\n\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$1$adapted(SAM.scala:336)\n\tat scala.Option.foreach(Option.scala:407)\n\tat com.databricks.unity.SAM.setFileSystemConfigs(SAM.scala:336)\n\tat com.databricks.unity.SAM.createDelegate(SAM.scala:388)\n\tat com.databricks.unity.SAM.createDelegate$(SAM.scala:372)\n\tat com.databricks.unity.TableSAM.createDelegate(SAM.scala:695)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.createDelegate(CredentialScopeFileSystem.scala:74)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.$anonfun$setDelegates$1(CredentialScopeFileSystem.scala:127)\n\tat com.databricks.sql.acl.fs.Lazy.apply(DelegatingFileSystem.scala:310)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.readDelegateMakeQualified(CredentialScopeFileSystem.scala:282)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.optimizedMakeQualifiedPath(CredentialScopeFileSystem.scala:277)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.makeQualified(CredentialScopeFileSystem.scala:294)\n\tat com.databricks.sql.transaction.tahoe.DeltaLog$.apply(DeltaLog.scala:1284)\n\tat com.databricks.sql.transaction.tahoe.DeltaLog$.forTable(DeltaLog.scala:1087)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$5(DeltaTableV2.scala:175)\n\tat com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$withAdditionalSnapshotInitializationUsageLogData$1(DeltaLog.scala:968)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n\tat com.databricks.sql.transaction.tahoe.DeltaLog$.withAdditionalSnapshotInitializationUsageLogData(DeltaLog.scala:969)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$2(DeltaTableV2.scala:169)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:700)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$1(DeltaTableV2.scala:150)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:83)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.deltaLog$lzycompute(DeltaTableV2.scala:150)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.deltaLog(DeltaTableV2.scala:148)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$8(DeltaTableV2.scala:264)\n\tat com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$withAdditionalSnapshotInitializationUsageLogData$1(DeltaLog.scala:968)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n\tat com.databricks.sql.transaction.tahoe.DeltaLog$.withAdditionalSnapshotInitializationUsageLogData(DeltaLog.scala:969)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$7(DeltaTableV2.scala:263)\n\tat scala.Option.orElse(Option.scala:447)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$5(DeltaTableV2.scala:263)\n\tat scala.Option.orElse(Option.scala:447)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$2(DeltaTableV2.scala:260)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:83)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$1(DeltaTableV2.scala:290)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:700)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot$lzycompute(DeltaTableV2.scala:240)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot(DeltaTableV2.scala:238)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$tableSchema$2(DeltaTableV2.scala:317)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.tableSchema$lzycompute(DeltaTableV2.scala:317)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.tableSchema(DeltaTableV2.scala:315)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.schema(DeltaTableV2.scala:322)\n\tat org.apache.spark.sql.connector.catalog.Table.columns(Table.java:68)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.columns(DeltaTableV2.scala:83)\n\tat org.apache.spark.sql.catalyst.analysis.ResolvedTable$.create(v2ResolutionPlans.scala:231)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$4(Analyzer.scala:1600)\n\tat scala.Option.map(Option.scala:230)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$3(Analyzer.scala:1592)\n\tat scala.Option.orElse(Option.scala:447)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableOrView(Analyzer.scala:1587)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1556)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1466)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:182)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:171)\n\tat scala.PartialFunction.applyOrElse(PartialFunction.scala:127)\n\tat scala.PartialFunction.applyOrElse$(PartialFunction.scala:126)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.applyOrElse(AnalysisHelper.scala:171)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:138)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1330)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1329)\n\tat org.apache.spark.sql.catalyst.plans.logical.DescribeRelation.mapChildren(v2Commands.scala:726)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning(AnalysisHelper.scala:186)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning$(AnalysisHelper.scala:167)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithSubqueriesAndPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.resolveOperatorsUpWithSubqueriesAndPruningByConf(Analyzer.scala:1674)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply0(Analyzer.scala:1466)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1419)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1365)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:480)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:629)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:613)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:131)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:480)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:479)\n\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:475)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:452)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22(RuleExecutor.scala:585)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22$adapted(RuleExecutor.scala:585)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:585)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:349)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:498)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:491)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:397)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:491)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:416)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:341)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:219)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:341)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:252)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:96)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:131)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:87)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:478)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:478)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:294)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:548)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:668)\n\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:151)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:668)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1307)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:661)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:658)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:658)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:288)\n\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:287)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:321)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:267)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$3(Dataset.scala:149)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n\tat org.apache.spark.sql.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1469)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1469)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:141)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:1048)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:1001)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:1071)\n\tat jdk.internal.reflect.GeneratedMethodAccessor593.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:197)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:117)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\n\t\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:87)\n\t\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:63)\n\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:251)\n\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapExceptionBase(ManagedCatalogClientImpl.scala:7378)\n\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:7364)\n\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.getTableCredentials(ManagedCatalogClientImpl.scala:3951)\n\t\tat com.databricks.sql.managedcatalog.ManagedCatalogClient.getTemporaryCredentials(ManagedCatalogClient.scala:2620)\n\t\tat com.databricks.sql.managedcatalog.ManagedCatalogClient.getTemporaryCredentials$(ManagedCatalogClient.scala:2605)\n\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.getTemporaryCredentials(ManagedCatalogClientImpl.scala:251)\n\t\tat com.databricks.unity.TempCredCache.$anonfun$getInternal$7(TemporaryCredentials.scala:448)\n\t\tat com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4724)\n\t\tat com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3522)\n\t\tat com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2315)\n\t\tat com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2278)\n\t\tat com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2193)\n\t\tat com.google.common.cache.LocalCache.get(LocalCache.java:3932)\n\t\tat com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4721)\n\t\tat com.databricks.unity.TempCredCache.liftedTree1$1(TemporaryCredentials.scala:447)\n\t\tat com.databricks.unity.TempCredCache.getInternal(TemporaryCredentials.scala:446)\n\t\tat com.databricks.unity.TempCredCache.get(TemporaryCredentials.scala:373)\n\t\tat com.databricks.unity.UnityCredentialManager.getTemporaryCredentials(CredentialManager.scala:488)\n\t\tat com.databricks.unity.CredentialManager$.getTemporaryCredentials(CredentialManager.scala:881)\n\t\tat com.databricks.unity.SAM.setConfFromCred$1(SAM.scala:338)\n\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$8(SAM.scala:353)\n\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$8$adapted(SAM.scala:352)\n\t\tat com.databricks.unity.TempCredRequest$.$anonfun$withFallbackOnOverlappingForParentPath$3(TemporaryCredentials.scala:147)\n\t\tat scala.util.Try$.apply(Try.scala:213)\n\t\tat com.databricks.unity.TempCredRequest$.withFallbackOnOverlappingForParentPath(TemporaryCredentials.scala:147)\n\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$7(SAM.scala:352)\n\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$7$adapted(SAM.scala:350)\n\t\tat scala.Option.map(Option.scala:230)\n\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$6(SAM.scala:350)\n\t\tat scala.Option.flatMap(Option.scala:271)\n\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$1(SAM.scala:349)\n\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$1$adapted(SAM.scala:336)\n\t\tat scala.Option.foreach(Option.scala:407)\n\t\tat com.databricks.unity.SAM.setFileSystemConfigs(SAM.scala:336)\n\t\tat com.databricks.unity.SAM.createDelegate(SAM.scala:388)\n\t\tat com.databricks.unity.SAM.createDelegate$(SAM.scala:372)\n\t\tat com.databricks.unity.TableSAM.createDelegate(SAM.scala:695)\n\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.createDelegate(CredentialScopeFileSystem.scala:74)\n\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.$anonfun$setDelegates$1(CredentialScopeFileSystem.scala:127)\n\t\tat com.databricks.sql.acl.fs.Lazy.apply(DelegatingFileSystem.scala:310)\n\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.readDelegateMakeQualified(CredentialScopeFileSystem.scala:282)\n\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.optimizedMakeQualifiedPath(CredentialScopeFileSystem.scala:277)\n\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.makeQualified(CredentialScopeFileSystem.scala:294)\n\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.apply(DeltaLog.scala:1284)\n\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.forTable(DeltaLog.scala:1087)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$5(DeltaTableV2.scala:175)\n\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$withAdditionalSnapshotInitializationUsageLogData$1(DeltaLog.scala:968)\n\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.withAdditionalSnapshotInitializationUsageLogData(DeltaLog.scala:969)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$2(DeltaTableV2.scala:169)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:700)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$1(DeltaTableV2.scala:150)\n\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\t\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n\t\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:83)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.deltaLog$lzycompute(DeltaTableV2.scala:150)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.deltaLog(DeltaTableV2.scala:148)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$8(DeltaTableV2.scala:264)\n\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$withAdditionalSnapshotInitializationUsageLogData$1(DeltaLog.scala:968)\n\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.withAdditionalSnapshotInitializationUsageLogData(DeltaLog.scala:969)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$7(DeltaTableV2.scala:263)\n\t\tat scala.Option.orElse(Option.scala:447)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$5(DeltaTableV2.scala:263)\n\t\tat scala.Option.orElse(Option.scala:447)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$2(DeltaTableV2.scala:260)\n\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\t\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n\t\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:83)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$1(DeltaTableV2.scala:290)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:700)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot$lzycompute(DeltaTableV2.scala:240)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot(DeltaTableV2.scala:238)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$tableSchema$2(DeltaTableV2.scala:317)\n\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.tableSchema$lzycompute(DeltaTableV2.scala:317)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.tableSchema(DeltaTableV2.scala:315)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.schema(DeltaTableV2.scala:322)\n\t\tat org.apache.spark.sql.connector.catalog.Table.columns(Table.java:68)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.columns(DeltaTableV2.scala:83)\n\t\tat org.apache.spark.sql.catalyst.analysis.ResolvedTable$.create(v2ResolutionPlans.scala:231)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$4(Analyzer.scala:1600)\n\t\tat scala.Option.map(Option.scala:230)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$3(Analyzer.scala:1592)\n\t\tat scala.Option.orElse(Option.scala:447)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableOrView(Analyzer.scala:1587)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1556)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1466)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:182)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:171)\n\t\tat scala.PartialFunction.applyOrElse(PartialFunction.scala:127)\n\t\tat scala.PartialFunction.applyOrElse$(PartialFunction.scala:126)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.applyOrElse(AnalysisHelper.scala:171)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)\n\t\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:138)\n\t\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1330)\n\t\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1329)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.DescribeRelation.mapChildren(v2Commands.scala:726)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning(AnalysisHelper.scala:186)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning$(AnalysisHelper.scala:167)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithSubqueriesAndPruning(LogicalPlan.scala:42)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.resolveOperatorsUpWithSubqueriesAndPruningByConf(Analyzer.scala:1674)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply0(Analyzer.scala:1466)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1419)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1365)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:480)\n\t\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:629)\n\t\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:613)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:131)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:480)\n\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:479)\n\t\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n\t\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n\t\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:475)\n\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:452)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22(RuleExecutor.scala:585)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22$adapted(RuleExecutor.scala:585)\n\t\tat scala.collection.immutable.List.foreach(List.scala:431)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:585)\n\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:349)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:498)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:491)\n\t\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:397)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:491)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:416)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:341)\n\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:219)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:341)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:252)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:96)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:131)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:87)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:478)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:478)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:294)\n\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:548)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:668)\n\t\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:151)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:668)\n\t\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1307)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:661)\n\t\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:658)\n\t\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n\t\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:658)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:288)\n\t\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:287)\n\t\tat scala.util.Try$.apply(Try.scala:213)\n\t\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)\n\t\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n\t\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n\t\t... 24 more\n",
         "2025-08-13T14:38:00.764356Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "table/view",
         "`accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample`.`forecast`.`us_postal_hourly_imperial`",
         "clone/create",
         "error",
         "Failed to process table/view `accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample`.`forecast`.`us_postal_hourly_imperial`. Error: An error occurred while calling o427.sql.\n: com.databricks.sql.managedcatalog.acl.UnauthorizedAccessException: PERMISSION_DENIED: User does not have USE CATALOG on Catalog 'accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample'.\n\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:87)\n\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:63)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:251)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapExceptionBase(ManagedCatalogClientImpl.scala:7378)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:7364)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.getTableCredentials(ManagedCatalogClientImpl.scala:3951)\n\tat com.databricks.sql.managedcatalog.ManagedCatalogClient.getTemporaryCredentials(ManagedCatalogClient.scala:2620)\n\tat com.databricks.sql.managedcatalog.ManagedCatalogClient.getTemporaryCredentials$(ManagedCatalogClient.scala:2605)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.getTemporaryCredentials(ManagedCatalogClientImpl.scala:251)\n\tat com.databricks.unity.TempCredCache.$anonfun$getInternal$7(TemporaryCredentials.scala:448)\n\tat com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4724)\n\tat com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3522)\n\tat com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2315)\n\tat com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2278)\n\tat com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2193)\n\tat com.google.common.cache.LocalCache.get(LocalCache.java:3932)\n\tat com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4721)\n\tat com.databricks.unity.TempCredCache.liftedTree1$1(TemporaryCredentials.scala:447)\n\tat com.databricks.unity.TempCredCache.getInternal(TemporaryCredentials.scala:446)\n\tat com.databricks.unity.TempCredCache.get(TemporaryCredentials.scala:373)\n\tat com.databricks.unity.UnityCredentialManager.getTemporaryCredentials(CredentialManager.scala:488)\n\tat com.databricks.unity.CredentialManager$.getTemporaryCredentials(CredentialManager.scala:881)\n\tat com.databricks.unity.SAM.setConfFromCred$1(SAM.scala:338)\n\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$8(SAM.scala:353)\n\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$8$adapted(SAM.scala:352)\n\tat com.databricks.unity.TempCredRequest$.$anonfun$withFallbackOnOverlappingForParentPath$3(TemporaryCredentials.scala:147)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.databricks.unity.TempCredRequest$.withFallbackOnOverlappingForParentPath(TemporaryCredentials.scala:147)\n\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$7(SAM.scala:352)\n\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$7$adapted(SAM.scala:350)\n\tat scala.Option.map(Option.scala:230)\n\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$6(SAM.scala:350)\n\tat scala.Option.flatMap(Option.scala:271)\n\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$1(SAM.scala:349)\n\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$1$adapted(SAM.scala:336)\n\tat scala.Option.foreach(Option.scala:407)\n\tat com.databricks.unity.SAM.setFileSystemConfigs(SAM.scala:336)\n\tat com.databricks.unity.SAM.createDelegate(SAM.scala:388)\n\tat com.databricks.unity.SAM.createDelegate$(SAM.scala:372)\n\tat com.databricks.unity.TableSAM.createDelegate(SAM.scala:695)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.createDelegate(CredentialScopeFileSystem.scala:74)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.$anonfun$setDelegates$1(CredentialScopeFileSystem.scala:127)\n\tat com.databricks.sql.acl.fs.Lazy.apply(DelegatingFileSystem.scala:310)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.readDelegateMakeQualified(CredentialScopeFileSystem.scala:282)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.optimizedMakeQualifiedPath(CredentialScopeFileSystem.scala:277)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.makeQualified(CredentialScopeFileSystem.scala:294)\n\tat com.databricks.sql.transaction.tahoe.DeltaLog$.apply(DeltaLog.scala:1284)\n\tat com.databricks.sql.transaction.tahoe.DeltaLog$.forTable(DeltaLog.scala:1087)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$5(DeltaTableV2.scala:175)\n\tat com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$withAdditionalSnapshotInitializationUsageLogData$1(DeltaLog.scala:968)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n\tat com.databricks.sql.transaction.tahoe.DeltaLog$.withAdditionalSnapshotInitializationUsageLogData(DeltaLog.scala:969)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$2(DeltaTableV2.scala:169)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:700)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$1(DeltaTableV2.scala:150)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:83)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.deltaLog$lzycompute(DeltaTableV2.scala:150)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.deltaLog(DeltaTableV2.scala:148)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$8(DeltaTableV2.scala:264)\n\tat com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$withAdditionalSnapshotInitializationUsageLogData$1(DeltaLog.scala:968)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n\tat com.databricks.sql.transaction.tahoe.DeltaLog$.withAdditionalSnapshotInitializationUsageLogData(DeltaLog.scala:969)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$7(DeltaTableV2.scala:263)\n\tat scala.Option.orElse(Option.scala:447)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$5(DeltaTableV2.scala:263)\n\tat scala.Option.orElse(Option.scala:447)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$2(DeltaTableV2.scala:260)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:83)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$1(DeltaTableV2.scala:290)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:700)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot$lzycompute(DeltaTableV2.scala:240)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot(DeltaTableV2.scala:238)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$tableSchema$2(DeltaTableV2.scala:317)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.tableSchema$lzycompute(DeltaTableV2.scala:317)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.tableSchema(DeltaTableV2.scala:315)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.schema(DeltaTableV2.scala:322)\n\tat org.apache.spark.sql.connector.catalog.Table.columns(Table.java:68)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.columns(DeltaTableV2.scala:83)\n\tat org.apache.spark.sql.catalyst.analysis.ResolvedTable$.create(v2ResolutionPlans.scala:231)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$4(Analyzer.scala:1600)\n\tat scala.Option.map(Option.scala:230)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$3(Analyzer.scala:1592)\n\tat scala.Option.orElse(Option.scala:447)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableOrView(Analyzer.scala:1587)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1556)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1466)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:182)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:171)\n\tat scala.PartialFunction.applyOrElse(PartialFunction.scala:127)\n\tat scala.PartialFunction.applyOrElse$(PartialFunction.scala:126)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.applyOrElse(AnalysisHelper.scala:171)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:138)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1330)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1329)\n\tat org.apache.spark.sql.catalyst.plans.logical.DescribeRelation.mapChildren(v2Commands.scala:726)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning(AnalysisHelper.scala:186)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning$(AnalysisHelper.scala:167)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithSubqueriesAndPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.resolveOperatorsUpWithSubqueriesAndPruningByConf(Analyzer.scala:1674)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply0(Analyzer.scala:1466)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1419)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1365)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:480)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:629)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:613)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:131)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:480)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:479)\n\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:475)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:452)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22(RuleExecutor.scala:585)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22$adapted(RuleExecutor.scala:585)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:585)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:349)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:498)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:491)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:397)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:491)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:416)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:341)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:219)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:341)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:252)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:96)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:131)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:87)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:478)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:478)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:294)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:548)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:668)\n\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:151)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:668)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1307)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:661)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:658)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:658)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:288)\n\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:287)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:321)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:267)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$3(Dataset.scala:149)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n\tat org.apache.spark.sql.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1469)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1469)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:141)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:1048)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:1001)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:1071)\n\tat jdk.internal.reflect.GeneratedMethodAccessor593.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:197)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:117)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\n\t\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:87)\n\t\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:63)\n\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:251)\n\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapExceptionBase(ManagedCatalogClientImpl.scala:7378)\n\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:7364)\n\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.getTableCredentials(ManagedCatalogClientImpl.scala:3951)\n\t\tat com.databricks.sql.managedcatalog.ManagedCatalogClient.getTemporaryCredentials(ManagedCatalogClient.scala:2620)\n\t\tat com.databricks.sql.managedcatalog.ManagedCatalogClient.getTemporaryCredentials$(ManagedCatalogClient.scala:2605)\n\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.getTemporaryCredentials(ManagedCatalogClientImpl.scala:251)\n\t\tat com.databricks.unity.TempCredCache.$anonfun$getInternal$7(TemporaryCredentials.scala:448)\n\t\tat com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4724)\n\t\tat com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3522)\n\t\tat com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2315)\n\t\tat com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2278)\n\t\tat com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2193)\n\t\tat com.google.common.cache.LocalCache.get(LocalCache.java:3932)\n\t\tat com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4721)\n\t\tat com.databricks.unity.TempCredCache.liftedTree1$1(TemporaryCredentials.scala:447)\n\t\tat com.databricks.unity.TempCredCache.getInternal(TemporaryCredentials.scala:446)\n\t\tat com.databricks.unity.TempCredCache.get(TemporaryCredentials.scala:373)\n\t\tat com.databricks.unity.UnityCredentialManager.getTemporaryCredentials(CredentialManager.scala:488)\n\t\tat com.databricks.unity.CredentialManager$.getTemporaryCredentials(CredentialManager.scala:881)\n\t\tat com.databricks.unity.SAM.setConfFromCred$1(SAM.scala:338)\n\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$8(SAM.scala:353)\n\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$8$adapted(SAM.scala:352)\n\t\tat com.databricks.unity.TempCredRequest$.$anonfun$withFallbackOnOverlappingForParentPath$3(TemporaryCredentials.scala:147)\n\t\tat scala.util.Try$.apply(Try.scala:213)\n\t\tat com.databricks.unity.TempCredRequest$.withFallbackOnOverlappingForParentPath(TemporaryCredentials.scala:147)\n\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$7(SAM.scala:352)\n\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$7$adapted(SAM.scala:350)\n\t\tat scala.Option.map(Option.scala:230)\n\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$6(SAM.scala:350)\n\t\tat scala.Option.flatMap(Option.scala:271)\n\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$1(SAM.scala:349)\n\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$1$adapted(SAM.scala:336)\n\t\tat scala.Option.foreach(Option.scala:407)\n\t\tat com.databricks.unity.SAM.setFileSystemConfigs(SAM.scala:336)\n\t\tat com.databricks.unity.SAM.createDelegate(SAM.scala:388)\n\t\tat com.databricks.unity.SAM.createDelegate$(SAM.scala:372)\n\t\tat com.databricks.unity.TableSAM.createDelegate(SAM.scala:695)\n\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.createDelegate(CredentialScopeFileSystem.scala:74)\n\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.$anonfun$setDelegates$1(CredentialScopeFileSystem.scala:127)\n\t\tat com.databricks.sql.acl.fs.Lazy.apply(DelegatingFileSystem.scala:310)\n\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.readDelegateMakeQualified(CredentialScopeFileSystem.scala:282)\n\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.optimizedMakeQualifiedPath(CredentialScopeFileSystem.scala:277)\n\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.makeQualified(CredentialScopeFileSystem.scala:294)\n\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.apply(DeltaLog.scala:1284)\n\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.forTable(DeltaLog.scala:1087)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$5(DeltaTableV2.scala:175)\n\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$withAdditionalSnapshotInitializationUsageLogData$1(DeltaLog.scala:968)\n\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.withAdditionalSnapshotInitializationUsageLogData(DeltaLog.scala:969)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$2(DeltaTableV2.scala:169)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:700)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$1(DeltaTableV2.scala:150)\n\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\t\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n\t\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:83)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.deltaLog$lzycompute(DeltaTableV2.scala:150)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.deltaLog(DeltaTableV2.scala:148)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$8(DeltaTableV2.scala:264)\n\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$withAdditionalSnapshotInitializationUsageLogData$1(DeltaLog.scala:968)\n\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.withAdditionalSnapshotInitializationUsageLogData(DeltaLog.scala:969)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$7(DeltaTableV2.scala:263)\n\t\tat scala.Option.orElse(Option.scala:447)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$5(DeltaTableV2.scala:263)\n\t\tat scala.Option.orElse(Option.scala:447)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$2(DeltaTableV2.scala:260)\n\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\t\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n\t\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:83)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$1(DeltaTableV2.scala:290)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:700)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot$lzycompute(DeltaTableV2.scala:240)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot(DeltaTableV2.scala:238)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$tableSchema$2(DeltaTableV2.scala:317)\n\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.tableSchema$lzycompute(DeltaTableV2.scala:317)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.tableSchema(DeltaTableV2.scala:315)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.schema(DeltaTableV2.scala:322)\n\t\tat org.apache.spark.sql.connector.catalog.Table.columns(Table.java:68)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.columns(DeltaTableV2.scala:83)\n\t\tat org.apache.spark.sql.catalyst.analysis.ResolvedTable$.create(v2ResolutionPlans.scala:231)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$4(Analyzer.scala:1600)\n\t\tat scala.Option.map(Option.scala:230)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$3(Analyzer.scala:1592)\n\t\tat scala.Option.orElse(Option.scala:447)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableOrView(Analyzer.scala:1587)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1556)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1466)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:182)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:171)\n\t\tat scala.PartialFunction.applyOrElse(PartialFunction.scala:127)\n\t\tat scala.PartialFunction.applyOrElse$(PartialFunction.scala:126)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.applyOrElse(AnalysisHelper.scala:171)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)\n\t\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:138)\n\t\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1330)\n\t\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1329)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.DescribeRelation.mapChildren(v2Commands.scala:726)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning(AnalysisHelper.scala:186)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning$(AnalysisHelper.scala:167)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithSubqueriesAndPruning(LogicalPlan.scala:42)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.resolveOperatorsUpWithSubqueriesAndPruningByConf(Analyzer.scala:1674)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply0(Analyzer.scala:1466)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1419)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1365)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:480)\n\t\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:629)\n\t\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:613)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:131)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:480)\n\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:479)\n\t\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n\t\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n\t\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:475)\n\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:452)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22(RuleExecutor.scala:585)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22$adapted(RuleExecutor.scala:585)\n\t\tat scala.collection.immutable.List.foreach(List.scala:431)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:585)\n\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:349)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:498)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:491)\n\t\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:397)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:491)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:416)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:341)\n\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:219)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:341)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:252)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:96)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:131)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:87)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:478)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:478)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:294)\n\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:548)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:668)\n\t\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:151)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:668)\n\t\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1307)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:661)\n\t\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:658)\n\t\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n\t\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:658)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:288)\n\t\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:287)\n\t\tat scala.util.Try$.apply(Try.scala:213)\n\t\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)\n\t\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n\t\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n\t\t... 24 more\n",
         "2025-08-13T14:38:00.744419Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "table/view",
         "`accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample`.`forecast`.`us_postal_daynight_metric`",
         "clone/create",
         "error",
         "Failed to process table/view `accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample`.`forecast`.`us_postal_daynight_metric`. Error: An error occurred while calling o427.sql.\n: com.databricks.sql.managedcatalog.acl.UnauthorizedAccessException: PERMISSION_DENIED: User does not have USE CATALOG on Catalog 'accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample'.\n\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:87)\n\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:63)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:251)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapExceptionBase(ManagedCatalogClientImpl.scala:7378)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:7364)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.getTableCredentials(ManagedCatalogClientImpl.scala:3951)\n\tat com.databricks.sql.managedcatalog.ManagedCatalogClient.getTemporaryCredentials(ManagedCatalogClient.scala:2620)\n\tat com.databricks.sql.managedcatalog.ManagedCatalogClient.getTemporaryCredentials$(ManagedCatalogClient.scala:2605)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.getTemporaryCredentials(ManagedCatalogClientImpl.scala:251)\n\tat com.databricks.unity.TempCredCache.$anonfun$getInternal$7(TemporaryCredentials.scala:448)\n\tat com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4724)\n\tat com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3522)\n\tat com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2315)\n\tat com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2278)\n\tat com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2193)\n\tat com.google.common.cache.LocalCache.get(LocalCache.java:3932)\n\tat com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4721)\n\tat com.databricks.unity.TempCredCache.liftedTree1$1(TemporaryCredentials.scala:447)\n\tat com.databricks.unity.TempCredCache.getInternal(TemporaryCredentials.scala:446)\n\tat com.databricks.unity.TempCredCache.get(TemporaryCredentials.scala:373)\n\tat com.databricks.unity.UnityCredentialManager.getTemporaryCredentials(CredentialManager.scala:488)\n\tat com.databricks.unity.CredentialManager$.getTemporaryCredentials(CredentialManager.scala:881)\n\tat com.databricks.unity.SAM.setConfFromCred$1(SAM.scala:338)\n\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$8(SAM.scala:353)\n\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$8$adapted(SAM.scala:352)\n\tat com.databricks.unity.TempCredRequest$.$anonfun$withFallbackOnOverlappingForParentPath$3(TemporaryCredentials.scala:147)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.databricks.unity.TempCredRequest$.withFallbackOnOverlappingForParentPath(TemporaryCredentials.scala:147)\n\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$7(SAM.scala:352)\n\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$7$adapted(SAM.scala:350)\n\tat scala.Option.map(Option.scala:230)\n\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$6(SAM.scala:350)\n\tat scala.Option.flatMap(Option.scala:271)\n\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$1(SAM.scala:349)\n\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$1$adapted(SAM.scala:336)\n\tat scala.Option.foreach(Option.scala:407)\n\tat com.databricks.unity.SAM.setFileSystemConfigs(SAM.scala:336)\n\tat com.databricks.unity.SAM.createDelegate(SAM.scala:388)\n\tat com.databricks.unity.SAM.createDelegate$(SAM.scala:372)\n\tat com.databricks.unity.TableSAM.createDelegate(SAM.scala:695)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.createDelegate(CredentialScopeFileSystem.scala:74)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.$anonfun$setDelegates$1(CredentialScopeFileSystem.scala:127)\n\tat com.databricks.sql.acl.fs.Lazy.apply(DelegatingFileSystem.scala:310)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.readDelegateMakeQualified(CredentialScopeFileSystem.scala:282)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.optimizedMakeQualifiedPath(CredentialScopeFileSystem.scala:277)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.makeQualified(CredentialScopeFileSystem.scala:294)\n\tat com.databricks.sql.transaction.tahoe.DeltaLog$.apply(DeltaLog.scala:1284)\n\tat com.databricks.sql.transaction.tahoe.DeltaLog$.forTable(DeltaLog.scala:1087)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$5(DeltaTableV2.scala:175)\n\tat com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$withAdditionalSnapshotInitializationUsageLogData$1(DeltaLog.scala:968)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n\tat com.databricks.sql.transaction.tahoe.DeltaLog$.withAdditionalSnapshotInitializationUsageLogData(DeltaLog.scala:969)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$2(DeltaTableV2.scala:169)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:700)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$1(DeltaTableV2.scala:150)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:83)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.deltaLog$lzycompute(DeltaTableV2.scala:150)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.deltaLog(DeltaTableV2.scala:148)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$8(DeltaTableV2.scala:264)\n\tat com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$withAdditionalSnapshotInitializationUsageLogData$1(DeltaLog.scala:968)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n\tat com.databricks.sql.transaction.tahoe.DeltaLog$.withAdditionalSnapshotInitializationUsageLogData(DeltaLog.scala:969)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$7(DeltaTableV2.scala:263)\n\tat scala.Option.orElse(Option.scala:447)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$5(DeltaTableV2.scala:263)\n\tat scala.Option.orElse(Option.scala:447)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$2(DeltaTableV2.scala:260)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:83)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$1(DeltaTableV2.scala:290)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:700)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot$lzycompute(DeltaTableV2.scala:240)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot(DeltaTableV2.scala:238)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$tableSchema$2(DeltaTableV2.scala:317)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.tableSchema$lzycompute(DeltaTableV2.scala:317)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.tableSchema(DeltaTableV2.scala:315)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.schema(DeltaTableV2.scala:322)\n\tat org.apache.spark.sql.connector.catalog.Table.columns(Table.java:68)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.columns(DeltaTableV2.scala:83)\n\tat org.apache.spark.sql.catalyst.analysis.ResolvedTable$.create(v2ResolutionPlans.scala:231)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$4(Analyzer.scala:1600)\n\tat scala.Option.map(Option.scala:230)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$3(Analyzer.scala:1592)\n\tat scala.Option.orElse(Option.scala:447)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableOrView(Analyzer.scala:1587)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1556)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1466)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:182)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:171)\n\tat scala.PartialFunction.applyOrElse(PartialFunction.scala:127)\n\tat scala.PartialFunction.applyOrElse$(PartialFunction.scala:126)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.applyOrElse(AnalysisHelper.scala:171)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:138)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1330)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1329)\n\tat org.apache.spark.sql.catalyst.plans.logical.DescribeRelation.mapChildren(v2Commands.scala:726)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning(AnalysisHelper.scala:186)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning$(AnalysisHelper.scala:167)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithSubqueriesAndPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.resolveOperatorsUpWithSubqueriesAndPruningByConf(Analyzer.scala:1674)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply0(Analyzer.scala:1466)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1419)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1365)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:480)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:629)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:613)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:131)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:480)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:479)\n\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:475)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:452)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22(RuleExecutor.scala:585)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22$adapted(RuleExecutor.scala:585)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:585)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:349)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:498)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:491)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:397)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:491)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:416)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:341)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:219)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:341)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:252)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:96)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:131)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:87)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:478)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:478)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:294)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:548)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:668)\n\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:151)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:668)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1307)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:661)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:658)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:658)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:288)\n\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:287)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:321)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:267)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$3(Dataset.scala:149)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n\tat org.apache.spark.sql.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1469)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1469)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:141)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:1048)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:1001)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:1071)\n\tat jdk.internal.reflect.GeneratedMethodAccessor593.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:197)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:117)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\n\t\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:87)\n\t\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:63)\n\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:251)\n\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapExceptionBase(ManagedCatalogClientImpl.scala:7378)\n\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:7364)\n\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.getTableCredentials(ManagedCatalogClientImpl.scala:3951)\n\t\tat com.databricks.sql.managedcatalog.ManagedCatalogClient.getTemporaryCredentials(ManagedCatalogClient.scala:2620)\n\t\tat com.databricks.sql.managedcatalog.ManagedCatalogClient.getTemporaryCredentials$(ManagedCatalogClient.scala:2605)\n\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.getTemporaryCredentials(ManagedCatalogClientImpl.scala:251)\n\t\tat com.databricks.unity.TempCredCache.$anonfun$getInternal$7(TemporaryCredentials.scala:448)\n\t\tat com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4724)\n\t\tat com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3522)\n\t\tat com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2315)\n\t\tat com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2278)\n\t\tat com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2193)\n\t\tat com.google.common.cache.LocalCache.get(LocalCache.java:3932)\n\t\tat com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4721)\n\t\tat com.databricks.unity.TempCredCache.liftedTree1$1(TemporaryCredentials.scala:447)\n\t\tat com.databricks.unity.TempCredCache.getInternal(TemporaryCredentials.scala:446)\n\t\tat com.databricks.unity.TempCredCache.get(TemporaryCredentials.scala:373)\n\t\tat com.databricks.unity.UnityCredentialManager.getTemporaryCredentials(CredentialManager.scala:488)\n\t\tat com.databricks.unity.CredentialManager$.getTemporaryCredentials(CredentialManager.scala:881)\n\t\tat com.databricks.unity.SAM.setConfFromCred$1(SAM.scala:338)\n\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$8(SAM.scala:353)\n\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$8$adapted(SAM.scala:352)\n\t\tat com.databricks.unity.TempCredRequest$.$anonfun$withFallbackOnOverlappingForParentPath$3(TemporaryCredentials.scala:147)\n\t\tat scala.util.Try$.apply(Try.scala:213)\n\t\tat com.databricks.unity.TempCredRequest$.withFallbackOnOverlappingForParentPath(TemporaryCredentials.scala:147)\n\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$7(SAM.scala:352)\n\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$7$adapted(SAM.scala:350)\n\t\tat scala.Option.map(Option.scala:230)\n\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$6(SAM.scala:350)\n\t\tat scala.Option.flatMap(Option.scala:271)\n\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$1(SAM.scala:349)\n\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$1$adapted(SAM.scala:336)\n\t\tat scala.Option.foreach(Option.scala:407)\n\t\tat com.databricks.unity.SAM.setFileSystemConfigs(SAM.scala:336)\n\t\tat com.databricks.unity.SAM.createDelegate(SAM.scala:388)\n\t\tat com.databricks.unity.SAM.createDelegate$(SAM.scala:372)\n\t\tat com.databricks.unity.TableSAM.createDelegate(SAM.scala:695)\n\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.createDelegate(CredentialScopeFileSystem.scala:74)\n\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.$anonfun$setDelegates$1(CredentialScopeFileSystem.scala:127)\n\t\tat com.databricks.sql.acl.fs.Lazy.apply(DelegatingFileSystem.scala:310)\n\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.readDelegateMakeQualified(CredentialScopeFileSystem.scala:282)\n\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.optimizedMakeQualifiedPath(CredentialScopeFileSystem.scala:277)\n\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.makeQualified(CredentialScopeFileSystem.scala:294)\n\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.apply(DeltaLog.scala:1284)\n\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.forTable(DeltaLog.scala:1087)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$5(DeltaTableV2.scala:175)\n\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$withAdditionalSnapshotInitializationUsageLogData$1(DeltaLog.scala:968)\n\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.withAdditionalSnapshotInitializationUsageLogData(DeltaLog.scala:969)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$2(DeltaTableV2.scala:169)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:700)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$1(DeltaTableV2.scala:150)\n\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\t\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n\t\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:83)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.deltaLog$lzycompute(DeltaTableV2.scala:150)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.deltaLog(DeltaTableV2.scala:148)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$8(DeltaTableV2.scala:264)\n\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$withAdditionalSnapshotInitializationUsageLogData$1(DeltaLog.scala:968)\n\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.withAdditionalSnapshotInitializationUsageLogData(DeltaLog.scala:969)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$7(DeltaTableV2.scala:263)\n\t\tat scala.Option.orElse(Option.scala:447)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$5(DeltaTableV2.scala:263)\n\t\tat scala.Option.orElse(Option.scala:447)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$2(DeltaTableV2.scala:260)\n\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\t\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n\t\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:83)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$1(DeltaTableV2.scala:290)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:700)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot$lzycompute(DeltaTableV2.scala:240)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot(DeltaTableV2.scala:238)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$tableSchema$2(DeltaTableV2.scala:317)\n\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.tableSchema$lzycompute(DeltaTableV2.scala:317)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.tableSchema(DeltaTableV2.scala:315)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.schema(DeltaTableV2.scala:322)\n\t\tat org.apache.spark.sql.connector.catalog.Table.columns(Table.java:68)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.columns(DeltaTableV2.scala:83)\n\t\tat org.apache.spark.sql.catalyst.analysis.ResolvedTable$.create(v2ResolutionPlans.scala:231)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$4(Analyzer.scala:1600)\n\t\tat scala.Option.map(Option.scala:230)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$3(Analyzer.scala:1592)\n\t\tat scala.Option.orElse(Option.scala:447)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableOrView(Analyzer.scala:1587)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1556)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1466)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:182)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:171)\n\t\tat scala.PartialFunction.applyOrElse(PartialFunction.scala:127)\n\t\tat scala.PartialFunction.applyOrElse$(PartialFunction.scala:126)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.applyOrElse(AnalysisHelper.scala:171)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)\n\t\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:138)\n\t\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1330)\n\t\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1329)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.DescribeRelation.mapChildren(v2Commands.scala:726)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning(AnalysisHelper.scala:186)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning$(AnalysisHelper.scala:167)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithSubqueriesAndPruning(LogicalPlan.scala:42)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.resolveOperatorsUpWithSubqueriesAndPruningByConf(Analyzer.scala:1674)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply0(Analyzer.scala:1466)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1419)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1365)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:480)\n\t\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:629)\n\t\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:613)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:131)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:480)\n\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:479)\n\t\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n\t\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n\t\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:475)\n\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:452)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22(RuleExecutor.scala:585)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22$adapted(RuleExecutor.scala:585)\n\t\tat scala.collection.immutable.List.foreach(List.scala:431)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:585)\n\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:349)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:498)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:491)\n\t\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:397)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:491)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:416)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:341)\n\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:219)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:341)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:252)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:96)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:131)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:87)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:478)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:478)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:294)\n\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:548)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:668)\n\t\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:151)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:668)\n\t\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1307)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:661)\n\t\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:658)\n\t\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n\t\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:658)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:288)\n\t\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:287)\n\t\tat scala.util.Try$.apply(Try.scala:213)\n\t\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)\n\t\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n\t\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n\t\t... 24 more\n",
         "2025-08-13T14:38:00.684353Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "table/view",
         "`accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample`.`forecast`.`us_postal_hourly_metric`",
         "clone/create",
         "error",
         "Failed to process table/view `accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample`.`forecast`.`us_postal_hourly_metric`. Error: An error occurred while calling o427.sql.\n: com.databricks.sql.managedcatalog.acl.UnauthorizedAccessException: PERMISSION_DENIED: User does not have USE CATALOG on Catalog 'accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample'.\n\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:87)\n\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:63)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:251)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapExceptionBase(ManagedCatalogClientImpl.scala:7378)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:7364)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.getTableCredentials(ManagedCatalogClientImpl.scala:3951)\n\tat com.databricks.sql.managedcatalog.ManagedCatalogClient.getTemporaryCredentials(ManagedCatalogClient.scala:2620)\n\tat com.databricks.sql.managedcatalog.ManagedCatalogClient.getTemporaryCredentials$(ManagedCatalogClient.scala:2605)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.getTemporaryCredentials(ManagedCatalogClientImpl.scala:251)\n\tat com.databricks.unity.TempCredCache.$anonfun$getInternal$7(TemporaryCredentials.scala:448)\n\tat com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4724)\n\tat com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3522)\n\tat com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2315)\n\tat com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2278)\n\tat com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2193)\n\tat com.google.common.cache.LocalCache.get(LocalCache.java:3932)\n\tat com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4721)\n\tat com.databricks.unity.TempCredCache.liftedTree1$1(TemporaryCredentials.scala:447)\n\tat com.databricks.unity.TempCredCache.getInternal(TemporaryCredentials.scala:446)\n\tat com.databricks.unity.TempCredCache.get(TemporaryCredentials.scala:373)\n\tat com.databricks.unity.UnityCredentialManager.getTemporaryCredentials(CredentialManager.scala:488)\n\tat com.databricks.unity.CredentialManager$.getTemporaryCredentials(CredentialManager.scala:881)\n\tat com.databricks.unity.SAM.setConfFromCred$1(SAM.scala:338)\n\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$8(SAM.scala:353)\n\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$8$adapted(SAM.scala:352)\n\tat com.databricks.unity.TempCredRequest$.$anonfun$withFallbackOnOverlappingForParentPath$3(TemporaryCredentials.scala:147)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.databricks.unity.TempCredRequest$.withFallbackOnOverlappingForParentPath(TemporaryCredentials.scala:147)\n\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$7(SAM.scala:352)\n\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$7$adapted(SAM.scala:350)\n\tat scala.Option.map(Option.scala:230)\n\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$6(SAM.scala:350)\n\tat scala.Option.flatMap(Option.scala:271)\n\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$1(SAM.scala:349)\n\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$1$adapted(SAM.scala:336)\n\tat scala.Option.foreach(Option.scala:407)\n\tat com.databricks.unity.SAM.setFileSystemConfigs(SAM.scala:336)\n\tat com.databricks.unity.SAM.createDelegate(SAM.scala:388)\n\tat com.databricks.unity.SAM.createDelegate$(SAM.scala:372)\n\tat com.databricks.unity.TableSAM.createDelegate(SAM.scala:695)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.createDelegate(CredentialScopeFileSystem.scala:74)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.$anonfun$setDelegates$1(CredentialScopeFileSystem.scala:127)\n\tat com.databricks.sql.acl.fs.Lazy.apply(DelegatingFileSystem.scala:310)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.readDelegateMakeQualified(CredentialScopeFileSystem.scala:282)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.optimizedMakeQualifiedPath(CredentialScopeFileSystem.scala:277)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.makeQualified(CredentialScopeFileSystem.scala:294)\n\tat com.databricks.sql.transaction.tahoe.DeltaLog$.apply(DeltaLog.scala:1284)\n\tat com.databricks.sql.transaction.tahoe.DeltaLog$.forTable(DeltaLog.scala:1087)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$5(DeltaTableV2.scala:175)\n\tat com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$withAdditionalSnapshotInitializationUsageLogData$1(DeltaLog.scala:968)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n\tat com.databricks.sql.transaction.tahoe.DeltaLog$.withAdditionalSnapshotInitializationUsageLogData(DeltaLog.scala:969)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$2(DeltaTableV2.scala:169)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:700)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$1(DeltaTableV2.scala:150)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:83)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.deltaLog$lzycompute(DeltaTableV2.scala:150)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.deltaLog(DeltaTableV2.scala:148)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$8(DeltaTableV2.scala:264)\n\tat com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$withAdditionalSnapshotInitializationUsageLogData$1(DeltaLog.scala:968)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n\tat com.databricks.sql.transaction.tahoe.DeltaLog$.withAdditionalSnapshotInitializationUsageLogData(DeltaLog.scala:969)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$7(DeltaTableV2.scala:263)\n\tat scala.Option.orElse(Option.scala:447)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$5(DeltaTableV2.scala:263)\n\tat scala.Option.orElse(Option.scala:447)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$2(DeltaTableV2.scala:260)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:83)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$1(DeltaTableV2.scala:290)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:700)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot$lzycompute(DeltaTableV2.scala:240)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot(DeltaTableV2.scala:238)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$tableSchema$2(DeltaTableV2.scala:317)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.tableSchema$lzycompute(DeltaTableV2.scala:317)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.tableSchema(DeltaTableV2.scala:315)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.schema(DeltaTableV2.scala:322)\n\tat org.apache.spark.sql.connector.catalog.Table.columns(Table.java:68)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.columns(DeltaTableV2.scala:83)\n\tat org.apache.spark.sql.catalyst.analysis.ResolvedTable$.create(v2ResolutionPlans.scala:231)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$4(Analyzer.scala:1600)\n\tat scala.Option.map(Option.scala:230)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$3(Analyzer.scala:1592)\n\tat scala.Option.orElse(Option.scala:447)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableOrView(Analyzer.scala:1587)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1556)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1466)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:182)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:171)\n\tat scala.PartialFunction.applyOrElse(PartialFunction.scala:127)\n\tat scala.PartialFunction.applyOrElse$(PartialFunction.scala:126)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.applyOrElse(AnalysisHelper.scala:171)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:138)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1330)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1329)\n\tat org.apache.spark.sql.catalyst.plans.logical.DescribeRelation.mapChildren(v2Commands.scala:726)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning(AnalysisHelper.scala:186)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning$(AnalysisHelper.scala:167)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithSubqueriesAndPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.resolveOperatorsUpWithSubqueriesAndPruningByConf(Analyzer.scala:1674)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply0(Analyzer.scala:1466)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1419)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1365)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:480)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:629)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:613)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:131)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:480)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:479)\n\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:475)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:452)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22(RuleExecutor.scala:585)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22$adapted(RuleExecutor.scala:585)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:585)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:349)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:498)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:491)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:397)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:491)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:416)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:341)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:219)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:341)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:252)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:96)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:131)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:87)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:478)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:478)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:294)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:548)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:668)\n\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:151)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:668)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1307)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:661)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:658)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:658)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:288)\n\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:287)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:321)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:267)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$3(Dataset.scala:149)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n\tat org.apache.spark.sql.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1469)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1469)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:141)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:1048)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:1001)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:1071)\n\tat jdk.internal.reflect.GeneratedMethodAccessor593.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:197)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:117)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\n\t\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:87)\n\t\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:63)\n\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:251)\n\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapExceptionBase(ManagedCatalogClientImpl.scala:7378)\n\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:7364)\n\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.getTableCredentials(ManagedCatalogClientImpl.scala:3951)\n\t\tat com.databricks.sql.managedcatalog.ManagedCatalogClient.getTemporaryCredentials(ManagedCatalogClient.scala:2620)\n\t\tat com.databricks.sql.managedcatalog.ManagedCatalogClient.getTemporaryCredentials$(ManagedCatalogClient.scala:2605)\n\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.getTemporaryCredentials(ManagedCatalogClientImpl.scala:251)\n\t\tat com.databricks.unity.TempCredCache.$anonfun$getInternal$7(TemporaryCredentials.scala:448)\n\t\tat com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4724)\n\t\tat com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3522)\n\t\tat com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2315)\n\t\tat com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2278)\n\t\tat com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2193)\n\t\tat com.google.common.cache.LocalCache.get(LocalCache.java:3932)\n\t\tat com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4721)\n\t\tat com.databricks.unity.TempCredCache.liftedTree1$1(TemporaryCredentials.scala:447)\n\t\tat com.databricks.unity.TempCredCache.getInternal(TemporaryCredentials.scala:446)\n\t\tat com.databricks.unity.TempCredCache.get(TemporaryCredentials.scala:373)\n\t\tat com.databricks.unity.UnityCredentialManager.getTemporaryCredentials(CredentialManager.scala:488)\n\t\tat com.databricks.unity.CredentialManager$.getTemporaryCredentials(CredentialManager.scala:881)\n\t\tat com.databricks.unity.SAM.setConfFromCred$1(SAM.scala:338)\n\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$8(SAM.scala:353)\n\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$8$adapted(SAM.scala:352)\n\t\tat com.databricks.unity.TempCredRequest$.$anonfun$withFallbackOnOverlappingForParentPath$3(TemporaryCredentials.scala:147)\n\t\tat scala.util.Try$.apply(Try.scala:213)\n\t\tat com.databricks.unity.TempCredRequest$.withFallbackOnOverlappingForParentPath(TemporaryCredentials.scala:147)\n\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$7(SAM.scala:352)\n\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$7$adapted(SAM.scala:350)\n\t\tat scala.Option.map(Option.scala:230)\n\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$6(SAM.scala:350)\n\t\tat scala.Option.flatMap(Option.scala:271)\n\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$1(SAM.scala:349)\n\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$1$adapted(SAM.scala:336)\n\t\tat scala.Option.foreach(Option.scala:407)\n\t\tat com.databricks.unity.SAM.setFileSystemConfigs(SAM.scala:336)\n\t\tat com.databricks.unity.SAM.createDelegate(SAM.scala:388)\n\t\tat com.databricks.unity.SAM.createDelegate$(SAM.scala:372)\n\t\tat com.databricks.unity.TableSAM.createDelegate(SAM.scala:695)\n\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.createDelegate(CredentialScopeFileSystem.scala:74)\n\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.$anonfun$setDelegates$1(CredentialScopeFileSystem.scala:127)\n\t\tat com.databricks.sql.acl.fs.Lazy.apply(DelegatingFileSystem.scala:310)\n\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.readDelegateMakeQualified(CredentialScopeFileSystem.scala:282)\n\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.optimizedMakeQualifiedPath(CredentialScopeFileSystem.scala:277)\n\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.makeQualified(CredentialScopeFileSystem.scala:294)\n\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.apply(DeltaLog.scala:1284)\n\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.forTable(DeltaLog.scala:1087)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$5(DeltaTableV2.scala:175)\n\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$withAdditionalSnapshotInitializationUsageLogData$1(DeltaLog.scala:968)\n\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.withAdditionalSnapshotInitializationUsageLogData(DeltaLog.scala:969)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$2(DeltaTableV2.scala:169)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:700)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$1(DeltaTableV2.scala:150)\n\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\t\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n\t\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:83)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.deltaLog$lzycompute(DeltaTableV2.scala:150)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.deltaLog(DeltaTableV2.scala:148)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$8(DeltaTableV2.scala:264)\n\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$withAdditionalSnapshotInitializationUsageLogData$1(DeltaLog.scala:968)\n\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.withAdditionalSnapshotInitializationUsageLogData(DeltaLog.scala:969)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$7(DeltaTableV2.scala:263)\n\t\tat scala.Option.orElse(Option.scala:447)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$5(DeltaTableV2.scala:263)\n\t\tat scala.Option.orElse(Option.scala:447)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$2(DeltaTableV2.scala:260)\n\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\t\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n\t\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:83)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$1(DeltaTableV2.scala:290)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:700)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot$lzycompute(DeltaTableV2.scala:240)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot(DeltaTableV2.scala:238)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$tableSchema$2(DeltaTableV2.scala:317)\n\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.tableSchema$lzycompute(DeltaTableV2.scala:317)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.tableSchema(DeltaTableV2.scala:315)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.schema(DeltaTableV2.scala:322)\n\t\tat org.apache.spark.sql.connector.catalog.Table.columns(Table.java:68)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.columns(DeltaTableV2.scala:83)\n\t\tat org.apache.spark.sql.catalyst.analysis.ResolvedTable$.create(v2ResolutionPlans.scala:231)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$4(Analyzer.scala:1600)\n\t\tat scala.Option.map(Option.scala:230)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$3(Analyzer.scala:1592)\n\t\tat scala.Option.orElse(Option.scala:447)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableOrView(Analyzer.scala:1587)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1556)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1466)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:182)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:171)\n\t\tat scala.PartialFunction.applyOrElse(PartialFunction.scala:127)\n\t\tat scala.PartialFunction.applyOrElse$(PartialFunction.scala:126)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.applyOrElse(AnalysisHelper.scala:171)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)\n\t\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:138)\n\t\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1330)\n\t\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1329)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.DescribeRelation.mapChildren(v2Commands.scala:726)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning(AnalysisHelper.scala:186)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning$(AnalysisHelper.scala:167)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithSubqueriesAndPruning(LogicalPlan.scala:42)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.resolveOperatorsUpWithSubqueriesAndPruningByConf(Analyzer.scala:1674)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply0(Analyzer.scala:1466)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1419)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1365)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:480)\n\t\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:629)\n\t\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:613)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:131)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:480)\n\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:479)\n\t\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n\t\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n\t\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:475)\n\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:452)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22(RuleExecutor.scala:585)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22$adapted(RuleExecutor.scala:585)\n\t\tat scala.collection.immutable.List.foreach(List.scala:431)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:585)\n\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:349)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:498)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:491)\n\t\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:397)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:491)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:416)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:341)\n\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:219)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:341)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:252)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:96)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:131)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:87)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:478)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:478)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:294)\n\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:548)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:668)\n\t\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:151)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:668)\n\t\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1307)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:661)\n\t\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:658)\n\t\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n\t\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:658)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:288)\n\t\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:287)\n\t\tat scala.util.Try$.apply(Try.scala:213)\n\t\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)\n\t\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n\t\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n\t\t... 24 more\n",
         "2025-08-13T14:38:00.548404Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "table",
         "`dr_eo_amperity`.`silver`.`unified_transactions`",
         "clone",
         "error",
         "Failed to clone table `dr_eo_amperity`.`silver`.`unified_transactions`.. Error: [DELTA_CLONE_UNSUPPORTED_SOURCE] Unsupported DEEP clone source '_eo_amperity.silver.unified_transactions', whose format is deltasharing.\nThe supported formats are 'delta', 'iceberg' and 'parquet'.",
         "2025-08-13T14:37:56.910888Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "table",
         "`dr_eo_amperity`.`silver`.`unified_scores`",
         "clone",
         "error",
         "Failed to clone table `dr_eo_amperity`.`silver`.`unified_scores`.. Error: [DELTA_CLONE_UNSUPPORTED_SOURCE] Unsupported DEEP clone source '_eo_amperity.silver.unified_scores', whose format is deltasharing.\nThe supported formats are 'delta', 'iceberg' and 'parquet'.",
         "2025-08-13T14:37:56.736993Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "table/view",
         "`accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample`.`forecast`.`us_postal_daily_metric`",
         "clone/create",
         "error",
         "Failed to process table/view `accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample`.`forecast`.`us_postal_daily_metric`. Error: An error occurred while calling o427.sql.\n: com.databricks.sql.managedcatalog.acl.UnauthorizedAccessException: PERMISSION_DENIED: User does not have USE CATALOG on Catalog 'accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample'.\n\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:87)\n\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:63)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:251)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapExceptionBase(ManagedCatalogClientImpl.scala:7378)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:7364)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.getTableCredentials(ManagedCatalogClientImpl.scala:3951)\n\tat com.databricks.sql.managedcatalog.ManagedCatalogClient.getTemporaryCredentials(ManagedCatalogClient.scala:2620)\n\tat com.databricks.sql.managedcatalog.ManagedCatalogClient.getTemporaryCredentials$(ManagedCatalogClient.scala:2605)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.getTemporaryCredentials(ManagedCatalogClientImpl.scala:251)\n\tat com.databricks.unity.TempCredCache.$anonfun$getInternal$7(TemporaryCredentials.scala:448)\n\tat com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4724)\n\tat com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3522)\n\tat com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2315)\n\tat com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2278)\n\tat com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2193)\n\tat com.google.common.cache.LocalCache.get(LocalCache.java:3932)\n\tat com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4721)\n\tat com.databricks.unity.TempCredCache.liftedTree1$1(TemporaryCredentials.scala:447)\n\tat com.databricks.unity.TempCredCache.getInternal(TemporaryCredentials.scala:446)\n\tat com.databricks.unity.TempCredCache.get(TemporaryCredentials.scala:373)\n\tat com.databricks.unity.UnityCredentialManager.getTemporaryCredentials(CredentialManager.scala:488)\n\tat com.databricks.unity.CredentialManager$.getTemporaryCredentials(CredentialManager.scala:881)\n\tat com.databricks.unity.SAM.setConfFromCred$1(SAM.scala:338)\n\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$8(SAM.scala:353)\n\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$8$adapted(SAM.scala:352)\n\tat com.databricks.unity.TempCredRequest$.$anonfun$withFallbackOnOverlappingForParentPath$3(TemporaryCredentials.scala:147)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.databricks.unity.TempCredRequest$.withFallbackOnOverlappingForParentPath(TemporaryCredentials.scala:147)\n\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$7(SAM.scala:352)\n\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$7$adapted(SAM.scala:350)\n\tat scala.Option.map(Option.scala:230)\n\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$6(SAM.scala:350)\n\tat scala.Option.flatMap(Option.scala:271)\n\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$1(SAM.scala:349)\n\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$1$adapted(SAM.scala:336)\n\tat scala.Option.foreach(Option.scala:407)\n\tat com.databricks.unity.SAM.setFileSystemConfigs(SAM.scala:336)\n\tat com.databricks.unity.SAM.createDelegate(SAM.scala:388)\n\tat com.databricks.unity.SAM.createDelegate$(SAM.scala:372)\n\tat com.databricks.unity.TableSAM.createDelegate(SAM.scala:695)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.createDelegate(CredentialScopeFileSystem.scala:74)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.$anonfun$setDelegates$1(CredentialScopeFileSystem.scala:127)\n\tat com.databricks.sql.acl.fs.Lazy.apply(DelegatingFileSystem.scala:310)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.readDelegateMakeQualified(CredentialScopeFileSystem.scala:282)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.optimizedMakeQualifiedPath(CredentialScopeFileSystem.scala:277)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.makeQualified(CredentialScopeFileSystem.scala:294)\n\tat com.databricks.sql.transaction.tahoe.DeltaLog$.apply(DeltaLog.scala:1284)\n\tat com.databricks.sql.transaction.tahoe.DeltaLog$.forTable(DeltaLog.scala:1087)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$5(DeltaTableV2.scala:175)\n\tat com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$withAdditionalSnapshotInitializationUsageLogData$1(DeltaLog.scala:968)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n\tat com.databricks.sql.transaction.tahoe.DeltaLog$.withAdditionalSnapshotInitializationUsageLogData(DeltaLog.scala:969)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$2(DeltaTableV2.scala:169)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:700)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$1(DeltaTableV2.scala:150)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:83)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.deltaLog$lzycompute(DeltaTableV2.scala:150)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.deltaLog(DeltaTableV2.scala:148)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$8(DeltaTableV2.scala:264)\n\tat com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$withAdditionalSnapshotInitializationUsageLogData$1(DeltaLog.scala:968)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n\tat com.databricks.sql.transaction.tahoe.DeltaLog$.withAdditionalSnapshotInitializationUsageLogData(DeltaLog.scala:969)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$7(DeltaTableV2.scala:263)\n\tat scala.Option.orElse(Option.scala:447)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$5(DeltaTableV2.scala:263)\n\tat scala.Option.orElse(Option.scala:447)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$2(DeltaTableV2.scala:260)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:83)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$1(DeltaTableV2.scala:290)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:700)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot$lzycompute(DeltaTableV2.scala:240)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot(DeltaTableV2.scala:238)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$tableSchema$2(DeltaTableV2.scala:317)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.tableSchema$lzycompute(DeltaTableV2.scala:317)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.tableSchema(DeltaTableV2.scala:315)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.schema(DeltaTableV2.scala:322)\n\tat org.apache.spark.sql.connector.catalog.Table.columns(Table.java:68)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.columns(DeltaTableV2.scala:83)\n\tat org.apache.spark.sql.catalyst.analysis.ResolvedTable$.create(v2ResolutionPlans.scala:231)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$4(Analyzer.scala:1600)\n\tat scala.Option.map(Option.scala:230)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$3(Analyzer.scala:1592)\n\tat scala.Option.orElse(Option.scala:447)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableOrView(Analyzer.scala:1587)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1556)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1466)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:182)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:171)\n\tat scala.PartialFunction.applyOrElse(PartialFunction.scala:127)\n\tat scala.PartialFunction.applyOrElse$(PartialFunction.scala:126)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.applyOrElse(AnalysisHelper.scala:171)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:138)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1330)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1329)\n\tat org.apache.spark.sql.catalyst.plans.logical.DescribeRelation.mapChildren(v2Commands.scala:726)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning(AnalysisHelper.scala:186)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning$(AnalysisHelper.scala:167)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithSubqueriesAndPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.resolveOperatorsUpWithSubqueriesAndPruningByConf(Analyzer.scala:1674)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply0(Analyzer.scala:1466)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1419)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1365)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:480)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:629)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:613)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:131)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:480)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:479)\n\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:475)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:452)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22(RuleExecutor.scala:585)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22$adapted(RuleExecutor.scala:585)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:585)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:349)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:498)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:491)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:397)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:491)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:416)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:341)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:219)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:341)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:252)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:96)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:131)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:87)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:478)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:478)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:294)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:548)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:668)\n\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:151)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:668)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1307)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:661)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:658)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:658)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:288)\n\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:287)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:321)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:267)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$3(Dataset.scala:149)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n\tat org.apache.spark.sql.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1469)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1469)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:141)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:1048)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:1001)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:1071)\n\tat jdk.internal.reflect.GeneratedMethodAccessor593.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:197)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:117)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\n\t\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:87)\n\t\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:63)\n\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:251)\n\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapExceptionBase(ManagedCatalogClientImpl.scala:7378)\n\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:7364)\n\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.getTableCredentials(ManagedCatalogClientImpl.scala:3951)\n\t\tat com.databricks.sql.managedcatalog.ManagedCatalogClient.getTemporaryCredentials(ManagedCatalogClient.scala:2620)\n\t\tat com.databricks.sql.managedcatalog.ManagedCatalogClient.getTemporaryCredentials$(ManagedCatalogClient.scala:2605)\n\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.getTemporaryCredentials(ManagedCatalogClientImpl.scala:251)\n\t\tat com.databricks.unity.TempCredCache.$anonfun$getInternal$7(TemporaryCredentials.scala:448)\n\t\tat com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4724)\n\t\tat com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3522)\n\t\tat com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2315)\n\t\tat com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2278)\n\t\tat com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2193)\n\t\tat com.google.common.cache.LocalCache.get(LocalCache.java:3932)\n\t\tat com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4721)\n\t\tat com.databricks.unity.TempCredCache.liftedTree1$1(TemporaryCredentials.scala:447)\n\t\tat com.databricks.unity.TempCredCache.getInternal(TemporaryCredentials.scala:446)\n\t\tat com.databricks.unity.TempCredCache.get(TemporaryCredentials.scala:373)\n\t\tat com.databricks.unity.UnityCredentialManager.getTemporaryCredentials(CredentialManager.scala:488)\n\t\tat com.databricks.unity.CredentialManager$.getTemporaryCredentials(CredentialManager.scala:881)\n\t\tat com.databricks.unity.SAM.setConfFromCred$1(SAM.scala:338)\n\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$8(SAM.scala:353)\n\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$8$adapted(SAM.scala:352)\n\t\tat com.databricks.unity.TempCredRequest$.$anonfun$withFallbackOnOverlappingForParentPath$3(TemporaryCredentials.scala:147)\n\t\tat scala.util.Try$.apply(Try.scala:213)\n\t\tat com.databricks.unity.TempCredRequest$.withFallbackOnOverlappingForParentPath(TemporaryCredentials.scala:147)\n\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$7(SAM.scala:352)\n\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$7$adapted(SAM.scala:350)\n\t\tat scala.Option.map(Option.scala:230)\n\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$6(SAM.scala:350)\n\t\tat scala.Option.flatMap(Option.scala:271)\n\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$1(SAM.scala:349)\n\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$1$adapted(SAM.scala:336)\n\t\tat scala.Option.foreach(Option.scala:407)\n\t\tat com.databricks.unity.SAM.setFileSystemConfigs(SAM.scala:336)\n\t\tat com.databricks.unity.SAM.createDelegate(SAM.scala:388)\n\t\tat com.databricks.unity.SAM.createDelegate$(SAM.scala:372)\n\t\tat com.databricks.unity.TableSAM.createDelegate(SAM.scala:695)\n\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.createDelegate(CredentialScopeFileSystem.scala:74)\n\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.$anonfun$setDelegates$1(CredentialScopeFileSystem.scala:127)\n\t\tat com.databricks.sql.acl.fs.Lazy.apply(DelegatingFileSystem.scala:310)\n\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.readDelegateMakeQualified(CredentialScopeFileSystem.scala:282)\n\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.optimizedMakeQualifiedPath(CredentialScopeFileSystem.scala:277)\n\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.makeQualified(CredentialScopeFileSystem.scala:294)\n\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.apply(DeltaLog.scala:1284)\n\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.forTable(DeltaLog.scala:1087)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$5(DeltaTableV2.scala:175)\n\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$withAdditionalSnapshotInitializationUsageLogData$1(DeltaLog.scala:968)\n\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.withAdditionalSnapshotInitializationUsageLogData(DeltaLog.scala:969)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$2(DeltaTableV2.scala:169)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:700)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$1(DeltaTableV2.scala:150)\n\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\t\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n\t\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:83)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.deltaLog$lzycompute(DeltaTableV2.scala:150)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.deltaLog(DeltaTableV2.scala:148)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$8(DeltaTableV2.scala:264)\n\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$withAdditionalSnapshotInitializationUsageLogData$1(DeltaLog.scala:968)\n\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.withAdditionalSnapshotInitializationUsageLogData(DeltaLog.scala:969)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$7(DeltaTableV2.scala:263)\n\t\tat scala.Option.orElse(Option.scala:447)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$5(DeltaTableV2.scala:263)\n\t\tat scala.Option.orElse(Option.scala:447)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$2(DeltaTableV2.scala:260)\n\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\t\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n\t\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:83)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$1(DeltaTableV2.scala:290)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:700)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot$lzycompute(DeltaTableV2.scala:240)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot(DeltaTableV2.scala:238)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$tableSchema$2(DeltaTableV2.scala:317)\n\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.tableSchema$lzycompute(DeltaTableV2.scala:317)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.tableSchema(DeltaTableV2.scala:315)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.schema(DeltaTableV2.scala:322)\n\t\tat org.apache.spark.sql.connector.catalog.Table.columns(Table.java:68)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.columns(DeltaTableV2.scala:83)\n\t\tat org.apache.spark.sql.catalyst.analysis.ResolvedTable$.create(v2ResolutionPlans.scala:231)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$4(Analyzer.scala:1600)\n\t\tat scala.Option.map(Option.scala:230)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$3(Analyzer.scala:1592)\n\t\tat scala.Option.orElse(Option.scala:447)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableOrView(Analyzer.scala:1587)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1556)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1466)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:182)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:171)\n\t\tat scala.PartialFunction.applyOrElse(PartialFunction.scala:127)\n\t\tat scala.PartialFunction.applyOrElse$(PartialFunction.scala:126)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.applyOrElse(AnalysisHelper.scala:171)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)\n\t\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:138)\n\t\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1330)\n\t\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1329)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.DescribeRelation.mapChildren(v2Commands.scala:726)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning(AnalysisHelper.scala:186)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning$(AnalysisHelper.scala:167)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithSubqueriesAndPruning(LogicalPlan.scala:42)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.resolveOperatorsUpWithSubqueriesAndPruningByConf(Analyzer.scala:1674)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply0(Analyzer.scala:1466)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1419)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1365)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:480)\n\t\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:629)\n\t\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:613)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:131)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:480)\n\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:479)\n\t\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n\t\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n\t\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:475)\n\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:452)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22(RuleExecutor.scala:585)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22$adapted(RuleExecutor.scala:585)\n\t\tat scala.collection.immutable.List.foreach(List.scala:431)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:585)\n\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:349)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:498)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:491)\n\t\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:397)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:491)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:416)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:341)\n\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:219)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:341)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:252)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:96)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:131)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:87)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:478)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:478)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:294)\n\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:548)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:668)\n\t\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:151)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:668)\n\t\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1307)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:661)\n\t\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:658)\n\t\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n\t\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:658)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:288)\n\t\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:287)\n\t\tat scala.util.Try$.apply(Try.scala:213)\n\t\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)\n\t\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n\t\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n\t\t... 24 more\n",
         "2025-08-13T14:37:56.420323Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "table/view",
         "`accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample`.`forecast`.`us_postal_daily_imperial`",
         "clone/create",
         "error",
         "Failed to process table/view `accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample`.`forecast`.`us_postal_daily_imperial`. Error: An error occurred while calling o427.sql.\n: com.databricks.sql.managedcatalog.acl.UnauthorizedAccessException: PERMISSION_DENIED: User does not have USE CATALOG on Catalog 'accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample'.\n\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:87)\n\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:63)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:251)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapExceptionBase(ManagedCatalogClientImpl.scala:7378)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:7364)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.getTableCredentials(ManagedCatalogClientImpl.scala:3951)\n\tat com.databricks.sql.managedcatalog.ManagedCatalogClient.getTemporaryCredentials(ManagedCatalogClient.scala:2620)\n\tat com.databricks.sql.managedcatalog.ManagedCatalogClient.getTemporaryCredentials$(ManagedCatalogClient.scala:2605)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.getTemporaryCredentials(ManagedCatalogClientImpl.scala:251)\n\tat com.databricks.unity.TempCredCache.$anonfun$getInternal$7(TemporaryCredentials.scala:448)\n\tat com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4724)\n\tat com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3522)\n\tat com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2315)\n\tat com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2278)\n\tat com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2193)\n\tat com.google.common.cache.LocalCache.get(LocalCache.java:3932)\n\tat com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4721)\n\tat com.databricks.unity.TempCredCache.liftedTree1$1(TemporaryCredentials.scala:447)\n\tat com.databricks.unity.TempCredCache.getInternal(TemporaryCredentials.scala:446)\n\tat com.databricks.unity.TempCredCache.get(TemporaryCredentials.scala:373)\n\tat com.databricks.unity.UnityCredentialManager.getTemporaryCredentials(CredentialManager.scala:488)\n\tat com.databricks.unity.CredentialManager$.getTemporaryCredentials(CredentialManager.scala:881)\n\tat com.databricks.unity.SAM.setConfFromCred$1(SAM.scala:338)\n\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$8(SAM.scala:353)\n\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$8$adapted(SAM.scala:352)\n\tat com.databricks.unity.TempCredRequest$.$anonfun$withFallbackOnOverlappingForParentPath$3(TemporaryCredentials.scala:147)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.databricks.unity.TempCredRequest$.withFallbackOnOverlappingForParentPath(TemporaryCredentials.scala:147)\n\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$7(SAM.scala:352)\n\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$7$adapted(SAM.scala:350)\n\tat scala.Option.map(Option.scala:230)\n\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$6(SAM.scala:350)\n\tat scala.Option.flatMap(Option.scala:271)\n\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$1(SAM.scala:349)\n\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$1$adapted(SAM.scala:336)\n\tat scala.Option.foreach(Option.scala:407)\n\tat com.databricks.unity.SAM.setFileSystemConfigs(SAM.scala:336)\n\tat com.databricks.unity.SAM.createDelegate(SAM.scala:388)\n\tat com.databricks.unity.SAM.createDelegate$(SAM.scala:372)\n\tat com.databricks.unity.TableSAM.createDelegate(SAM.scala:695)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.createDelegate(CredentialScopeFileSystem.scala:74)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.$anonfun$setDelegates$1(CredentialScopeFileSystem.scala:127)\n\tat com.databricks.sql.acl.fs.Lazy.apply(DelegatingFileSystem.scala:310)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.readDelegateMakeQualified(CredentialScopeFileSystem.scala:282)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.optimizedMakeQualifiedPath(CredentialScopeFileSystem.scala:277)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.makeQualified(CredentialScopeFileSystem.scala:294)\n\tat com.databricks.sql.transaction.tahoe.DeltaLog$.apply(DeltaLog.scala:1284)\n\tat com.databricks.sql.transaction.tahoe.DeltaLog$.forTable(DeltaLog.scala:1087)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$5(DeltaTableV2.scala:175)\n\tat com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$withAdditionalSnapshotInitializationUsageLogData$1(DeltaLog.scala:968)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n\tat com.databricks.sql.transaction.tahoe.DeltaLog$.withAdditionalSnapshotInitializationUsageLogData(DeltaLog.scala:969)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$2(DeltaTableV2.scala:169)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:700)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$1(DeltaTableV2.scala:150)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:83)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.deltaLog$lzycompute(DeltaTableV2.scala:150)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.deltaLog(DeltaTableV2.scala:148)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$8(DeltaTableV2.scala:264)\n\tat com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$withAdditionalSnapshotInitializationUsageLogData$1(DeltaLog.scala:968)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n\tat com.databricks.sql.transaction.tahoe.DeltaLog$.withAdditionalSnapshotInitializationUsageLogData(DeltaLog.scala:969)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$7(DeltaTableV2.scala:263)\n\tat scala.Option.orElse(Option.scala:447)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$5(DeltaTableV2.scala:263)\n\tat scala.Option.orElse(Option.scala:447)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$2(DeltaTableV2.scala:260)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:83)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$1(DeltaTableV2.scala:290)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:700)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot$lzycompute(DeltaTableV2.scala:240)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot(DeltaTableV2.scala:238)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$tableSchema$2(DeltaTableV2.scala:317)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.tableSchema$lzycompute(DeltaTableV2.scala:317)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.tableSchema(DeltaTableV2.scala:315)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.schema(DeltaTableV2.scala:322)\n\tat org.apache.spark.sql.connector.catalog.Table.columns(Table.java:68)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.columns(DeltaTableV2.scala:83)\n\tat org.apache.spark.sql.catalyst.analysis.ResolvedTable$.create(v2ResolutionPlans.scala:231)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$4(Analyzer.scala:1600)\n\tat scala.Option.map(Option.scala:230)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$3(Analyzer.scala:1592)\n\tat scala.Option.orElse(Option.scala:447)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableOrView(Analyzer.scala:1587)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1556)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1466)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:182)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:171)\n\tat scala.PartialFunction.applyOrElse(PartialFunction.scala:127)\n\tat scala.PartialFunction.applyOrElse$(PartialFunction.scala:126)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.applyOrElse(AnalysisHelper.scala:171)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:138)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1330)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1329)\n\tat org.apache.spark.sql.catalyst.plans.logical.DescribeRelation.mapChildren(v2Commands.scala:726)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning(AnalysisHelper.scala:186)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning$(AnalysisHelper.scala:167)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithSubqueriesAndPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.resolveOperatorsUpWithSubqueriesAndPruningByConf(Analyzer.scala:1674)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply0(Analyzer.scala:1466)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1419)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1365)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:480)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:629)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:613)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:131)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:480)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:479)\n\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:475)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:452)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22(RuleExecutor.scala:585)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22$adapted(RuleExecutor.scala:585)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:585)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:349)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:498)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:491)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:397)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:491)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:416)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:341)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:219)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:341)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:252)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:96)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:131)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:87)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:478)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:478)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:294)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:548)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:668)\n\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:151)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:668)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1307)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:661)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:658)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:658)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:288)\n\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:287)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:321)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:267)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$3(Dataset.scala:149)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n\tat org.apache.spark.sql.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1469)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1469)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:141)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:1048)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:1001)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:1071)\n\tat jdk.internal.reflect.GeneratedMethodAccessor593.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:197)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:117)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\n\t\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:87)\n\t\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:63)\n\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:251)\n\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapExceptionBase(ManagedCatalogClientImpl.scala:7378)\n\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:7364)\n\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.getTableCredentials(ManagedCatalogClientImpl.scala:3951)\n\t\tat com.databricks.sql.managedcatalog.ManagedCatalogClient.getTemporaryCredentials(ManagedCatalogClient.scala:2620)\n\t\tat com.databricks.sql.managedcatalog.ManagedCatalogClient.getTemporaryCredentials$(ManagedCatalogClient.scala:2605)\n\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.getTemporaryCredentials(ManagedCatalogClientImpl.scala:251)\n\t\tat com.databricks.unity.TempCredCache.$anonfun$getInternal$7(TemporaryCredentials.scala:448)\n\t\tat com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4724)\n\t\tat com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3522)\n\t\tat com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2315)\n\t\tat com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2278)\n\t\tat com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2193)\n\t\tat com.google.common.cache.LocalCache.get(LocalCache.java:3932)\n\t\tat com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4721)\n\t\tat com.databricks.unity.TempCredCache.liftedTree1$1(TemporaryCredentials.scala:447)\n\t\tat com.databricks.unity.TempCredCache.getInternal(TemporaryCredentials.scala:446)\n\t\tat com.databricks.unity.TempCredCache.get(TemporaryCredentials.scala:373)\n\t\tat com.databricks.unity.UnityCredentialManager.getTemporaryCredentials(CredentialManager.scala:488)\n\t\tat com.databricks.unity.CredentialManager$.getTemporaryCredentials(CredentialManager.scala:881)\n\t\tat com.databricks.unity.SAM.setConfFromCred$1(SAM.scala:338)\n\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$8(SAM.scala:353)\n\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$8$adapted(SAM.scala:352)\n\t\tat com.databricks.unity.TempCredRequest$.$anonfun$withFallbackOnOverlappingForParentPath$3(TemporaryCredentials.scala:147)\n\t\tat scala.util.Try$.apply(Try.scala:213)\n\t\tat com.databricks.unity.TempCredRequest$.withFallbackOnOverlappingForParentPath(TemporaryCredentials.scala:147)\n\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$7(SAM.scala:352)\n\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$7$adapted(SAM.scala:350)\n\t\tat scala.Option.map(Option.scala:230)\n\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$6(SAM.scala:350)\n\t\tat scala.Option.flatMap(Option.scala:271)\n\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$1(SAM.scala:349)\n\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$1$adapted(SAM.scala:336)\n\t\tat scala.Option.foreach(Option.scala:407)\n\t\tat com.databricks.unity.SAM.setFileSystemConfigs(SAM.scala:336)\n\t\tat com.databricks.unity.SAM.createDelegate(SAM.scala:388)\n\t\tat com.databricks.unity.SAM.createDelegate$(SAM.scala:372)\n\t\tat com.databricks.unity.TableSAM.createDelegate(SAM.scala:695)\n\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.createDelegate(CredentialScopeFileSystem.scala:74)\n\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.$anonfun$setDelegates$1(CredentialScopeFileSystem.scala:127)\n\t\tat com.databricks.sql.acl.fs.Lazy.apply(DelegatingFileSystem.scala:310)\n\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.readDelegateMakeQualified(CredentialScopeFileSystem.scala:282)\n\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.optimizedMakeQualifiedPath(CredentialScopeFileSystem.scala:277)\n\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.makeQualified(CredentialScopeFileSystem.scala:294)\n\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.apply(DeltaLog.scala:1284)\n\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.forTable(DeltaLog.scala:1087)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$5(DeltaTableV2.scala:175)\n\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$withAdditionalSnapshotInitializationUsageLogData$1(DeltaLog.scala:968)\n\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.withAdditionalSnapshotInitializationUsageLogData(DeltaLog.scala:969)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$2(DeltaTableV2.scala:169)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:700)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$1(DeltaTableV2.scala:150)\n\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\t\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n\t\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:83)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.deltaLog$lzycompute(DeltaTableV2.scala:150)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.deltaLog(DeltaTableV2.scala:148)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$8(DeltaTableV2.scala:264)\n\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$withAdditionalSnapshotInitializationUsageLogData$1(DeltaLog.scala:968)\n\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.withAdditionalSnapshotInitializationUsageLogData(DeltaLog.scala:969)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$7(DeltaTableV2.scala:263)\n\t\tat scala.Option.orElse(Option.scala:447)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$5(DeltaTableV2.scala:263)\n\t\tat scala.Option.orElse(Option.scala:447)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$2(DeltaTableV2.scala:260)\n\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\t\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n\t\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:83)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$1(DeltaTableV2.scala:290)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:700)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot$lzycompute(DeltaTableV2.scala:240)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot(DeltaTableV2.scala:238)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$tableSchema$2(DeltaTableV2.scala:317)\n\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.tableSchema$lzycompute(DeltaTableV2.scala:317)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.tableSchema(DeltaTableV2.scala:315)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.schema(DeltaTableV2.scala:322)\n\t\tat org.apache.spark.sql.connector.catalog.Table.columns(Table.java:68)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.columns(DeltaTableV2.scala:83)\n\t\tat org.apache.spark.sql.catalyst.analysis.ResolvedTable$.create(v2ResolutionPlans.scala:231)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$4(Analyzer.scala:1600)\n\t\tat scala.Option.map(Option.scala:230)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$3(Analyzer.scala:1592)\n\t\tat scala.Option.orElse(Option.scala:447)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableOrView(Analyzer.scala:1587)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1556)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1466)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:182)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:171)\n\t\tat scala.PartialFunction.applyOrElse(PartialFunction.scala:127)\n\t\tat scala.PartialFunction.applyOrElse$(PartialFunction.scala:126)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.applyOrElse(AnalysisHelper.scala:171)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)\n\t\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:138)\n\t\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1330)\n\t\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1329)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.DescribeRelation.mapChildren(v2Commands.scala:726)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning(AnalysisHelper.scala:186)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning$(AnalysisHelper.scala:167)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithSubqueriesAndPruning(LogicalPlan.scala:42)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.resolveOperatorsUpWithSubqueriesAndPruningByConf(Analyzer.scala:1674)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply0(Analyzer.scala:1466)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1419)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1365)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:480)\n\t\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:629)\n\t\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:613)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:131)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:480)\n\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:479)\n\t\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n\t\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n\t\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:475)\n\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:452)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22(RuleExecutor.scala:585)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22$adapted(RuleExecutor.scala:585)\n\t\tat scala.collection.immutable.List.foreach(List.scala:431)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:585)\n\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:349)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:498)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:491)\n\t\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:397)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:491)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:416)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:341)\n\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:219)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:341)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:252)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:96)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:131)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:87)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:478)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:478)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:294)\n\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:548)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:668)\n\t\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:151)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:668)\n\t\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1307)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:661)\n\t\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:658)\n\t\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n\t\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:658)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:288)\n\t\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:287)\n\t\tat scala.util.Try$.apply(Try.scala:213)\n\t\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)\n\t\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n\t\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n\t\t... 24 more\n",
         "2025-08-13T14:37:56.416376Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "table",
         "`dr_eo_amperity`.`silver`.`unified_coalesced`",
         "clone",
         "success",
         "Table `dr_eo_amperity`.`silver`.`unified_coalesced` cloned successfully.",
         "2025-08-13T14:37:56.37547Z",
         [
          [
           "source_table_size",
           "25008603680"
          ],
          [
           "source_num_of_files",
           "234"
          ],
          [
           "num_of_synced_transactions",
           "None"
          ],
          [
           "num_removed_files",
           "0"
          ],
          [
           "num_copied_files",
           "0"
          ],
          [
           "removed_files_size",
           "0"
          ],
          [
           "copied_files_size",
           "0"
          ]
         ]
        ],
        [
         "DR_Clone_data_v2",
         "table",
         "`dr_eo_amperity`.`silver`.`unified_merged`",
         "clone",
         "error",
         "Failed to clone table `dr_eo_amperity`.`silver`.`unified_merged`.. Error: [DELTA_CLONE_UNSUPPORTED_SOURCE] Unsupported DEEP clone source '_eo_amperity.silver.unified_merged', whose format is deltasharing.\nThe supported formats are 'delta', 'iceberg' and 'parquet'.",
         "2025-08-13T14:37:52.889861Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "table",
         "`dr_eo_amperity`.`silver`.`unified_loyalty_events`",
         "clone",
         "error",
         "Failed to clone table `dr_eo_amperity`.`silver`.`unified_loyalty_events`.. Error: [DELTA_CLONE_UNSUPPORTED_SOURCE] Unsupported DEEP clone source '_eo_amperity.silver.unified_loyalty_events', whose format is deltasharing.\nThe supported formats are 'delta', 'iceberg' and 'parquet'.",
         "2025-08-13T14:37:52.87844Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "table",
         "`dr_eo_amperity`.`silver`.`unified_paid_media`",
         "clone",
         "error",
         "Failed to clone table `dr_eo_amperity`.`silver`.`unified_paid_media`.. Error: [DELTA_CLONE_UNSUPPORTED_SOURCE] Unsupported DEEP clone source '_eo_amperity.silver.unified_paid_media', whose format is deltasharing.\nThe supported formats are 'delta', 'iceberg' and 'parquet'.",
         "2025-08-13T14:37:52.818532Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "table",
         "`dr_eo_amperity`.`silver`.`unified_loyalty`",
         "clone",
         "error",
         "Failed to clone table `dr_eo_amperity`.`silver`.`unified_loyalty`.. Error: [DELTA_CLONE_UNSUPPORTED_SOURCE] Unsupported DEEP clone source '_eo_amperity.silver.unified_loyalty', whose format is deltasharing.\nThe supported formats are 'delta', 'iceberg' and 'parquet'.",
         "2025-08-13T14:37:52.489629Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "table",
         "`dr_eo_amperity`.`silver`.`unified_itemized_transactions`",
         "clone",
         "error",
         "Failed to clone table `dr_eo_amperity`.`silver`.`unified_itemized_transactions`.. Error: [DELTA_CLONE_UNSUPPORTED_SOURCE] Unsupported DEEP clone source '_eo_amperity.silver.unified_itemized_transactions', whose format is deltasharing.\nThe supported formats are 'delta', 'iceberg' and 'parquet'.",
         "2025-08-13T14:37:33.427965Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "table",
         "`dr_eo_amperity`.`silver`.`unified_household`",
         "clone",
         "error",
         "Failed to clone table `dr_eo_amperity`.`silver`.`unified_household`.. Error: [DELTA_CLONE_UNSUPPORTED_SOURCE] Unsupported DEEP clone source '_eo_amperity.silver.unified_household', whose format is deltasharing.\nThe supported formats are 'delta', 'iceberg' and 'parquet'.",
         "2025-08-13T14:37:33.285288Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "table",
         "`dr_eo_amperity`.`silver`.`unified_compliance`",
         "clone",
         "error",
         "Failed to clone table `dr_eo_amperity`.`silver`.`unified_compliance`.. Error: [DELTA_CLONE_UNSUPPORTED_SOURCE] Unsupported DEEP clone source '_eo_amperity.silver.unified_compliance', whose format is deltasharing.\nThe supported formats are 'delta', 'iceberg' and 'parquet'.",
         "2025-08-13T14:37:33.285242Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "table",
         "`dr_eo_amperity`.`silver`.`unified_customer`",
         "clone",
         "error",
         "Failed to clone table `dr_eo_amperity`.`silver`.`unified_customer`.. Error: [DELTA_CLONE_UNSUPPORTED_SOURCE] Unsupported DEEP clone source '_eo_amperity.silver.unified_customer', whose format is deltasharing.\nThe supported formats are 'delta', 'iceberg' and 'parquet'.",
         "2025-08-13T14:37:33.204961Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "table",
         "`dr_eo_amperity`.`silver`.`fiscal_calendar`",
         "clone",
         "error",
         "Failed to clone table `dr_eo_amperity`.`silver`.`fiscal_calendar`.. Error: [DELTA_CLONE_UNSUPPORTED_SOURCE] Unsupported DEEP clone source '_eo_amperity.silver.fiscal_calendar', whose format is deltasharing.\nThe supported formats are 'delta', 'iceberg' and 'parquet'.",
         "2025-08-13T14:37:28.923444Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "table",
         "`dr_eo_amperity`.`default`.`unified_transactions_reporting`",
         "clone",
         "error",
         "Failed to clone table `dr_eo_amperity`.`default`.`unified_transactions_reporting`.. Error: [DELTA_CLONE_UNSUPPORTED_SOURCE] Unsupported DEEP clone source '_eo_amperity.default.unified_transactions_reporting', whose format is deltasharing.\nThe supported formats are 'delta', 'iceberg' and 'parquet'.",
         "2025-08-13T14:37:28.882035Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "table",
         "`dr_eo_amperity`.`default`.`transaction_attributes_extended`",
         "clone",
         "error",
         "Failed to clone table `dr_eo_amperity`.`default`.`transaction_attributes_extended`.. Error: [DELTA_CLONE_UNSUPPORTED_SOURCE] Unsupported DEEP clone source '_eo_amperity.default.transaction_attributes_extended', whose format is deltasharing.\nThe supported formats are 'delta', 'iceberg' and 'parquet'.",
         "2025-08-13T14:37:28.81271Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "table",
         "`dr_eo_amperity`.`default`.`transaction_attributes`",
         "clone",
         "error",
         "Failed to clone table `dr_eo_amperity`.`default`.`transaction_attributes`.. Error: [DELTA_CLONE_UNSUPPORTED_SOURCE] Unsupported DEEP clone source '_eo_amperity.default.transaction_attributes', whose format is deltasharing.\nThe supported formats are 'delta', 'iceberg' and 'parquet'.",
         "2025-08-13T14:37:28.741128Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "table",
         "`dr_eo_amperity`.`default`.`predicted_clv_attributes`",
         "clone",
         "error",
         "Failed to clone table `dr_eo_amperity`.`default`.`predicted_clv_attributes`.. Error: [DELTA_CLONE_UNSUPPORTED_SOURCE] Unsupported DEEP clone source '_eo_amperity.default.predicted_clv_attributes', whose format is deltasharing.\nThe supported formats are 'delta', 'iceberg' and 'parquet'.",
         "2025-08-13T14:37:28.582454Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "table",
         "`dr_eo_amperity`.`bronze`.`wifi_connections`",
         "clone",
         "success",
         "Table `dr_eo_amperity`.`bronze`.`wifi_connections` cloned successfully.",
         "2025-08-13T14:37:25.094829Z",
         [
          [
           "source_table_size",
           "7882742359"
          ],
          [
           "source_num_of_files",
           "52"
          ],
          [
           "num_of_synced_transactions",
           "None"
          ],
          [
           "num_removed_files",
           "0"
          ],
          [
           "num_copied_files",
           "0"
          ],
          [
           "removed_files_size",
           "0"
          ],
          [
           "copied_files_size",
           "0"
          ]
         ]
        ],
        [
         "DR_Clone_data_v2",
         "table",
         "`dr_eo_amperity`.`bronze`.`web_visitors`",
         "clone",
         "success",
         "Table `dr_eo_amperity`.`bronze`.`web_visitors` cloned successfully.",
         "2025-08-13T14:37:24.807545Z",
         [
          [
           "source_table_size",
           "334349516"
          ],
          [
           "source_num_of_files",
           "7"
          ],
          [
           "num_of_synced_transactions",
           "None"
          ],
          [
           "num_removed_files",
           "0"
          ],
          [
           "num_copied_files",
           "0"
          ],
          [
           "removed_files_size",
           "0"
          ],
          [
           "copied_files_size",
           "0"
          ]
         ]
        ],
        [
         "DR_Clone_data_v2",
         "table",
         "`dr_eo_amperity`.`default`.`predicted_affinity`",
         "clone",
         "error",
         "Failed to clone table `dr_eo_amperity`.`default`.`predicted_affinity`.. Error: [DELTA_CLONE_UNSUPPORTED_SOURCE] Unsupported DEEP clone source '_eo_amperity.default.predicted_affinity', whose format is deltasharing.\nThe supported formats are 'delta', 'iceberg' and 'parquet'.",
         "2025-08-13T14:37:24.461206Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "table",
         "`dr_eo_amperity`.`default`.`email_engagement_summary`",
         "clone",
         "error",
         "Failed to clone table `dr_eo_amperity`.`default`.`email_engagement_summary`.. Error: [DELTA_CLONE_UNSUPPORTED_SOURCE] Unsupported DEEP clone source '_eo_amperity.default.email_engagement_summary', whose format is deltasharing.\nThe supported formats are 'delta', 'iceberg' and 'parquet'.",
         "2025-08-13T14:37:24.407177Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "table",
         "`dr_eo_amperity`.`default`.`email_engagement_attributes`",
         "clone",
         "error",
         "Failed to clone table `dr_eo_amperity`.`default`.`email_engagement_attributes`.. Error: [DELTA_CLONE_UNSUPPORTED_SOURCE] Unsupported DEEP clone source '_eo_amperity.default.email_engagement_attributes', whose format is deltasharing.\nThe supported formats are 'delta', 'iceberg' and 'parquet'.",
         "2025-08-13T14:37:24.319345Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "table",
         "`dr_eo_amperity`.`default`.`analytics_tae`",
         "clone",
         "error",
         "Failed to clone table `dr_eo_amperity`.`default`.`analytics_tae`.. Error: [DELTA_CLONE_UNSUPPORTED_SOURCE] Unsupported DEEP clone source '_eo_amperity.default.analytics_tae', whose format is deltasharing.\nThe supported formats are 'delta', 'iceberg' and 'parquet'.",
         "2025-08-13T14:36:54.177016Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "table",
         "`dr_eo_amperity`.`default`.`customer_attributes`",
         "clone",
         "error",
         "Failed to clone table `dr_eo_amperity`.`default`.`customer_attributes`.. Error: [DELTA_CLONE_UNSUPPORTED_SOURCE] Unsupported DEEP clone source '_eo_amperity.default.customer_attributes', whose format is deltasharing.\nThe supported formats are 'delta', 'iceberg' and 'parquet'.",
         "2025-08-13T14:36:54.1364Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "table",
         "`dr_eo_amperity`.`default`.`customer_360`",
         "clone",
         "error",
         "Failed to clone table `dr_eo_amperity`.`default`.`customer_360`.. Error: [DELTA_CLONE_UNSUPPORTED_SOURCE] Unsupported DEEP clone source '_eo_amperity.default.customer_360', whose format is deltasharing.\nThe supported formats are 'delta', 'iceberg' and 'parquet'.",
         "2025-08-13T14:36:54.102789Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "table",
         "`dr_eo_amperity`.`bronze`.`pos_customers`",
         "clone",
         "error",
         "Failed to clone table `dr_eo_amperity`.`bronze`.`pos_customers`.. Error: [DELTA_CLONE_UNSUPPORTED_SOURCE] Unsupported DEEP clone source '_eo_amperity.bronze.pos_customers', whose format is deltasharing.\nThe supported formats are 'delta', 'iceberg' and 'parquet'.",
         "2025-08-13T14:36:50.056267Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "table",
         "`dr_eo_amperity`.`bronze`.`e_commerce_customers`",
         "clone",
         "error",
         "Failed to clone table `dr_eo_amperity`.`bronze`.`e_commerce_customers`.. Error: [DELTA_CLONE_UNSUPPORTED_SOURCE] Unsupported DEEP clone source '_eo_amperity.bronze.e_commerce_customers', whose format is deltasharing.\nThe supported formats are 'delta', 'iceberg' and 'parquet'.",
         "2025-08-13T14:36:50.053434Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "table",
         "`dr_eo_amperity`.`bronze`.`loyalty_members`",
         "clone",
         "error",
         "Failed to clone table `dr_eo_amperity`.`bronze`.`loyalty_members`.. Error: [DELTA_CLONE_UNSUPPORTED_SOURCE] Unsupported DEEP clone source '_eo_amperity.bronze.loyalty_members', whose format is deltasharing.\nThe supported formats are 'delta', 'iceberg' and 'parquet'.",
         "2025-08-13T14:36:50.030959Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "catalog_pair",
         "satya_share -> satya_dr_catalog",
         "process",
         "sequential_complete",
         "Finished sequential processing for catalog pair: satya_share -> satya_dr_catalog",
         "2025-08-13T14:36:47.566078Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "schema",
         "`satya_dr_catalog`.`satyendranath_sure`",
         "change_ownership",
         "skipped",
         "No ownership mapping found for this schema. Skipping ownership change.",
         "2025-08-13T14:36:46.094778Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "schema",
         "`satya_dr_catalog`.`satyendranath_sure`",
         "create",
         "success",
         "Schema `satya_dr_catalog`.`satyendranath_sure` created or already exists.",
         "2025-08-13T14:36:44.714022Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "schema",
         "`satya_share`.`information_schema`",
         "skip",
         "success",
         "Skipping `information_schema`.",
         "2025-08-13T14:36:43.224012Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "catalog",
         "`satya_dr_catalog`",
         "create",
         "success",
         "Catalog `satya_dr_catalog` created or already exists.",
         "2025-08-13T14:36:40.594553Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "catalog_pair",
         "satya_share -> satya_dr_catalog",
         "process",
         "start",
         "Starting processing for catalog pair: satya_share -> satya_dr_catalog",
         "2025-08-13T14:36:38.839076Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "catalog",
         "`test_not`",
         "get_schemas",
         "error",
         "Failed to get schemas from source catalog `test_not`. Error: [NO_SUCH_CATALOG_EXCEPTION] Catalog 'test_not' was not found. Please verify the catalog name and then retry the query or command again. SQLSTATE: 42704",
         "2025-08-13T14:36:37.307457Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "catalog",
         "`dr_test_not`",
         "create",
         "success",
         "Catalog `dr_test_not` created or already exists.",
         "2025-08-13T14:36:35.677033Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "catalog_pair",
         "test_not -> dr_test_not",
         "process",
         "start",
         "Starting processing for catalog pair: test_not -> dr_test_not",
         "2025-08-13T14:36:33.857224Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "catalog_pair",
         "accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample -> dr_accuweather",
         "process",
         "sequential_complete",
         "Finished sequential processing for catalog pair: accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample -> dr_accuweather",
         "2025-08-13T14:36:32.457098Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "schema",
         "`accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample`.`information_schema`",
         "skip",
         "success",
         "Skipping `information_schema`.",
         "2025-08-13T14:36:31.116006Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "schema",
         "`dr_accuweather`.`forecast`",
         "change_ownership",
         "skipped",
         "No ownership mapping found for this schema. Skipping ownership change.",
         "2025-08-13T14:36:29.498239Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "schema",
         "`dr_accuweather`.`forecast`",
         "create",
         "success",
         "Schema `dr_accuweather`.`forecast` created or already exists.",
         "2025-08-13T14:36:28.109957Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "catalog",
         "`dr_accuweather`",
         "create",
         "success",
         "Catalog `dr_accuweather` created or already exists.",
         "2025-08-13T14:36:25.530754Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "catalog_pair",
         "accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample -> dr_accuweather",
         "process",
         "start",
         "Starting processing for catalog pair: accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample -> dr_accuweather",
         "2025-08-13T14:36:23.955123Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "catalog_pair",
         "_eo_amperity -> dr_eo_amperity",
         "process",
         "sequential_complete",
         "Finished sequential processing for catalog pair: _eo_amperity -> dr_eo_amperity",
         "2025-08-13T14:36:21.862531Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "schema",
         "`dr_eo_amperity`.`silver`",
         "change_ownership",
         "success",
         "Ownership of schema `dr_eo_amperity`.`silver` changed to `grp_test_clt_usa`.",
         "2025-08-13T14:36:20.152484Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "schema",
         "`dr_eo_amperity`.`silver`",
         "create",
         "success",
         "Schema `dr_eo_amperity`.`silver` created or already exists.",
         "2025-08-13T14:36:18.376482Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "schema",
         "`_eo_amperity`.`information_schema`",
         "skip",
         "success",
         "Skipping `information_schema`.",
         "2025-08-13T14:36:16.607961Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "schema",
         "`dr_eo_amperity`.`default`",
         "change_ownership",
         "success",
         "Ownership of schema `dr_eo_amperity`.`default` changed to `grp_test_clt_usa`.",
         "2025-08-13T14:36:11.496898Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "schema",
         "`dr_eo_amperity`.`default`",
         "create",
         "success",
         "Schema `dr_eo_amperity`.`default` created or already exists.",
         "2025-08-13T14:36:09.844828Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "schema",
         "`dr_eo_amperity`.`bronze`",
         "change_ownership",
         "success",
         "Ownership of schema `dr_eo_amperity`.`bronze` changed to `grp_test_clt_usa`.",
         "2025-08-13T14:36:08.030969Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "schema",
         "`dr_eo_amperity`.`bronze`",
         "create",
         "success",
         "Schema `dr_eo_amperity`.`bronze` created or already exists.",
         "2025-08-13T14:36:06.289496Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "catalog",
         "`dr_eo_amperity`",
         "create",
         "success",
         "Catalog `dr_eo_amperity` created or already exists.",
         "2025-08-13T14:36:03.906827Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "catalog_pair",
         "_eo_amperity -> dr_eo_amperity",
         "process",
         "start",
         "Starting processing for catalog pair: _eo_amperity -> dr_eo_amperity",
         "2025-08-13T14:36:00.443416Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "table",
         "`satya_dr_catalog`.`satyendranath_sure`.`satya_external_table`",
         "clone",
         "success",
         "Table `satya_dr_catalog`.`satyendranath_sure`.`satya_external_table` cloned successfully.",
         "2025-08-13T14:34:18.160459Z",
         [
          [
           "source_table_size",
           "826"
          ],
          [
           "source_num_of_files",
           "1"
          ],
          [
           "num_of_synced_transactions",
           "None"
          ],
          [
           "num_removed_files",
           "0"
          ],
          [
           "num_copied_files",
           "1"
          ],
          [
           "removed_files_size",
           "0"
          ],
          [
           "copied_files_size",
           "826"
          ]
         ]
        ],
        [
         "DR_Clone_data_v2",
         "table",
         "`satya_dr_catalog`.`satyendranath_sure`.`satya_managed_table`",
         "clone",
         "success",
         "Table `satya_dr_catalog`.`satyendranath_sure`.`satya_managed_table` cloned successfully.",
         "2025-08-13T14:34:16.161858Z",
         [
          [
           "source_table_size",
           "827"
          ],
          [
           "source_num_of_files",
           "1"
          ],
          [
           "num_of_synced_transactions",
           "None"
          ],
          [
           "num_removed_files",
           "0"
          ],
          [
           "num_copied_files",
           "1"
          ],
          [
           "removed_files_size",
           "0"
          ],
          [
           "copied_files_size",
           "827"
          ]
         ]
        ],
        [
         "DR_Clone_data_v2",
         "table/view",
         "`accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample`.`forecast`.`us_postal_hourly_metric`",
         "clone/create",
         "error",
         "Failed to process table/view `accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample`.`forecast`.`us_postal_hourly_metric`. Error: An error occurred while calling o427.sql.\n: com.databricks.sql.managedcatalog.acl.UnauthorizedAccessException: PERMISSION_DENIED: User does not have USE CATALOG on Catalog 'accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample'.\n\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:87)\n\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:63)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:251)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapExceptionBase(ManagedCatalogClientImpl.scala:7378)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:7364)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.getTableCredentials(ManagedCatalogClientImpl.scala:3951)\n\tat com.databricks.sql.managedcatalog.ManagedCatalogClient.getTemporaryCredentials(ManagedCatalogClient.scala:2620)\n\tat com.databricks.sql.managedcatalog.ManagedCatalogClient.getTemporaryCredentials$(ManagedCatalogClient.scala:2605)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.getTemporaryCredentials(ManagedCatalogClientImpl.scala:251)\n\tat com.databricks.unity.TempCredCache.$anonfun$getInternal$7(TemporaryCredentials.scala:448)\n\tat com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4724)\n\tat com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3522)\n\tat com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2315)\n\tat com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2278)\n\tat com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2193)\n\tat com.google.common.cache.LocalCache.get(LocalCache.java:3932)\n\tat com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4721)\n\tat com.databricks.unity.TempCredCache.liftedTree1$1(TemporaryCredentials.scala:447)\n\tat com.databricks.unity.TempCredCache.getInternal(TemporaryCredentials.scala:446)\n\tat com.databricks.unity.TempCredCache.get(TemporaryCredentials.scala:373)\n\tat com.databricks.unity.UnityCredentialManager.getTemporaryCredentials(CredentialManager.scala:488)\n\tat com.databricks.unity.CredentialManager$.getTemporaryCredentials(CredentialManager.scala:881)\n\tat com.databricks.unity.SAM.setConfFromCred$1(SAM.scala:338)\n\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$8(SAM.scala:353)\n\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$8$adapted(SAM.scala:352)\n\tat com.databricks.unity.TempCredRequest$.$anonfun$withFallbackOnOverlappingForParentPath$3(TemporaryCredentials.scala:147)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.databricks.unity.TempCredRequest$.withFallbackOnOverlappingForParentPath(TemporaryCredentials.scala:147)\n\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$7(SAM.scala:352)\n\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$7$adapted(SAM.scala:350)\n\tat scala.Option.map(Option.scala:230)\n\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$6(SAM.scala:350)\n\tat scala.Option.flatMap(Option.scala:271)\n\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$1(SAM.scala:349)\n\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$1$adapted(SAM.scala:336)\n\tat scala.Option.foreach(Option.scala:407)\n\tat com.databricks.unity.SAM.setFileSystemConfigs(SAM.scala:336)\n\tat com.databricks.unity.SAM.createDelegate(SAM.scala:388)\n\tat com.databricks.unity.SAM.createDelegate$(SAM.scala:372)\n\tat com.databricks.unity.TableSAM.createDelegate(SAM.scala:695)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.createDelegate(CredentialScopeFileSystem.scala:74)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.$anonfun$setDelegates$1(CredentialScopeFileSystem.scala:127)\n\tat com.databricks.sql.acl.fs.Lazy.apply(DelegatingFileSystem.scala:310)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.readDelegateMakeQualified(CredentialScopeFileSystem.scala:282)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.optimizedMakeQualifiedPath(CredentialScopeFileSystem.scala:277)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.makeQualified(CredentialScopeFileSystem.scala:294)\n\tat com.databricks.sql.transaction.tahoe.DeltaLog$.apply(DeltaLog.scala:1284)\n\tat com.databricks.sql.transaction.tahoe.DeltaLog$.forTable(DeltaLog.scala:1087)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$5(DeltaTableV2.scala:175)\n\tat com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$withAdditionalSnapshotInitializationUsageLogData$1(DeltaLog.scala:968)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n\tat com.databricks.sql.transaction.tahoe.DeltaLog$.withAdditionalSnapshotInitializationUsageLogData(DeltaLog.scala:969)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$2(DeltaTableV2.scala:169)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:700)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$1(DeltaTableV2.scala:150)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:83)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.deltaLog$lzycompute(DeltaTableV2.scala:150)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.deltaLog(DeltaTableV2.scala:148)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$8(DeltaTableV2.scala:264)\n\tat com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$withAdditionalSnapshotInitializationUsageLogData$1(DeltaLog.scala:968)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n\tat com.databricks.sql.transaction.tahoe.DeltaLog$.withAdditionalSnapshotInitializationUsageLogData(DeltaLog.scala:969)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$7(DeltaTableV2.scala:263)\n\tat scala.Option.orElse(Option.scala:447)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$5(DeltaTableV2.scala:263)\n\tat scala.Option.orElse(Option.scala:447)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$2(DeltaTableV2.scala:260)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:83)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$1(DeltaTableV2.scala:290)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:700)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot$lzycompute(DeltaTableV2.scala:240)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot(DeltaTableV2.scala:238)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$tableSchema$2(DeltaTableV2.scala:317)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.tableSchema$lzycompute(DeltaTableV2.scala:317)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.tableSchema(DeltaTableV2.scala:315)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.schema(DeltaTableV2.scala:322)\n\tat org.apache.spark.sql.connector.catalog.Table.columns(Table.java:68)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.columns(DeltaTableV2.scala:83)\n\tat org.apache.spark.sql.catalyst.analysis.ResolvedTable$.create(v2ResolutionPlans.scala:231)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$4(Analyzer.scala:1600)\n\tat scala.Option.map(Option.scala:230)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$3(Analyzer.scala:1592)\n\tat scala.Option.orElse(Option.scala:447)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableOrView(Analyzer.scala:1587)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1556)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1466)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:182)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:171)\n\tat scala.PartialFunction.applyOrElse(PartialFunction.scala:127)\n\tat scala.PartialFunction.applyOrElse$(PartialFunction.scala:126)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.applyOrElse(AnalysisHelper.scala:171)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:138)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1330)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1329)\n\tat org.apache.spark.sql.catalyst.plans.logical.DescribeRelation.mapChildren(v2Commands.scala:726)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning(AnalysisHelper.scala:186)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning$(AnalysisHelper.scala:167)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithSubqueriesAndPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.resolveOperatorsUpWithSubqueriesAndPruningByConf(Analyzer.scala:1674)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply0(Analyzer.scala:1466)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1419)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1365)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:480)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:629)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:613)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:131)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:480)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:479)\n\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:475)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:452)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22(RuleExecutor.scala:585)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22$adapted(RuleExecutor.scala:585)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:585)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:349)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:498)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:491)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:397)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:491)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:416)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:341)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:219)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:341)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:252)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:96)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:131)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:87)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:478)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:478)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:294)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:548)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:668)\n\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:151)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:668)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1307)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:661)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:658)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:658)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:288)\n\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:287)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:321)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:267)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$3(Dataset.scala:149)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n\tat org.apache.spark.sql.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1469)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1469)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:141)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:1048)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:1001)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:1071)\n\tat jdk.internal.reflect.GeneratedMethodAccessor593.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:197)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:117)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\n\t\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:87)\n\t\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:63)\n\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:251)\n\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapExceptionBase(ManagedCatalogClientImpl.scala:7378)\n\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:7364)\n\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.getTableCredentials(ManagedCatalogClientImpl.scala:3951)\n\t\tat com.databricks.sql.managedcatalog.ManagedCatalogClient.getTemporaryCredentials(ManagedCatalogClient.scala:2620)\n\t\tat com.databricks.sql.managedcatalog.ManagedCatalogClient.getTemporaryCredentials$(ManagedCatalogClient.scala:2605)\n\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.getTemporaryCredentials(ManagedCatalogClientImpl.scala:251)\n\t\tat com.databricks.unity.TempCredCache.$anonfun$getInternal$7(TemporaryCredentials.scala:448)\n\t\tat com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4724)\n\t\tat com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3522)\n\t\tat com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2315)\n\t\tat com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2278)\n\t\tat com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2193)\n\t\tat com.google.common.cache.LocalCache.get(LocalCache.java:3932)\n\t\tat com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4721)\n\t\tat com.databricks.unity.TempCredCache.liftedTree1$1(TemporaryCredentials.scala:447)\n\t\tat com.databricks.unity.TempCredCache.getInternal(TemporaryCredentials.scala:446)\n\t\tat com.databricks.unity.TempCredCache.get(TemporaryCredentials.scala:373)\n\t\tat com.databricks.unity.UnityCredentialManager.getTemporaryCredentials(CredentialManager.scala:488)\n\t\tat com.databricks.unity.CredentialManager$.getTemporaryCredentials(CredentialManager.scala:881)\n\t\tat com.databricks.unity.SAM.setConfFromCred$1(SAM.scala:338)\n\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$8(SAM.scala:353)\n\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$8$adapted(SAM.scala:352)\n\t\tat com.databricks.unity.TempCredRequest$.$anonfun$withFallbackOnOverlappingForParentPath$3(TemporaryCredentials.scala:147)\n\t\tat scala.util.Try$.apply(Try.scala:213)\n\t\tat com.databricks.unity.TempCredRequest$.withFallbackOnOverlappingForParentPath(TemporaryCredentials.scala:147)\n\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$7(SAM.scala:352)\n\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$7$adapted(SAM.scala:350)\n\t\tat scala.Option.map(Option.scala:230)\n\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$6(SAM.scala:350)\n\t\tat scala.Option.flatMap(Option.scala:271)\n\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$1(SAM.scala:349)\n\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$1$adapted(SAM.scala:336)\n\t\tat scala.Option.foreach(Option.scala:407)\n\t\tat com.databricks.unity.SAM.setFileSystemConfigs(SAM.scala:336)\n\t\tat com.databricks.unity.SAM.createDelegate(SAM.scala:388)\n\t\tat com.databricks.unity.SAM.createDelegate$(SAM.scala:372)\n\t\tat com.databricks.unity.TableSAM.createDelegate(SAM.scala:695)\n\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.createDelegate(CredentialScopeFileSystem.scala:74)\n\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.$anonfun$setDelegates$1(CredentialScopeFileSystem.scala:127)\n\t\tat com.databricks.sql.acl.fs.Lazy.apply(DelegatingFileSystem.scala:310)\n\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.readDelegateMakeQualified(CredentialScopeFileSystem.scala:282)\n\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.optimizedMakeQualifiedPath(CredentialScopeFileSystem.scala:277)\n\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.makeQualified(CredentialScopeFileSystem.scala:294)\n\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.apply(DeltaLog.scala:1284)\n\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.forTable(DeltaLog.scala:1087)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$5(DeltaTableV2.scala:175)\n\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$withAdditionalSnapshotInitializationUsageLogData$1(DeltaLog.scala:968)\n\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.withAdditionalSnapshotInitializationUsageLogData(DeltaLog.scala:969)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$2(DeltaTableV2.scala:169)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:700)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$1(DeltaTableV2.scala:150)\n\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\t\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n\t\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:83)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.deltaLog$lzycompute(DeltaTableV2.scala:150)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.deltaLog(DeltaTableV2.scala:148)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$8(DeltaTableV2.scala:264)\n\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$withAdditionalSnapshotInitializationUsageLogData$1(DeltaLog.scala:968)\n\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.withAdditionalSnapshotInitializationUsageLogData(DeltaLog.scala:969)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$7(DeltaTableV2.scala:263)\n\t\tat scala.Option.orElse(Option.scala:447)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$5(DeltaTableV2.scala:263)\n\t\tat scala.Option.orElse(Option.scala:447)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$2(DeltaTableV2.scala:260)\n\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\t\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n\t\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:83)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$1(DeltaTableV2.scala:290)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:700)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot$lzycompute(DeltaTableV2.scala:240)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot(DeltaTableV2.scala:238)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$tableSchema$2(DeltaTableV2.scala:317)\n\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.tableSchema$lzycompute(DeltaTableV2.scala:317)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.tableSchema(DeltaTableV2.scala:315)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.schema(DeltaTableV2.scala:322)\n\t\tat org.apache.spark.sql.connector.catalog.Table.columns(Table.java:68)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.columns(DeltaTableV2.scala:83)\n\t\tat org.apache.spark.sql.catalyst.analysis.ResolvedTable$.create(v2ResolutionPlans.scala:231)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$4(Analyzer.scala:1600)\n\t\tat scala.Option.map(Option.scala:230)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$3(Analyzer.scala:1592)\n\t\tat scala.Option.orElse(Option.scala:447)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableOrView(Analyzer.scala:1587)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1556)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1466)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:182)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:171)\n\t\tat scala.PartialFunction.applyOrElse(PartialFunction.scala:127)\n\t\tat scala.PartialFunction.applyOrElse$(PartialFunction.scala:126)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.applyOrElse(AnalysisHelper.scala:171)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)\n\t\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:138)\n\t\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1330)\n\t\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1329)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.DescribeRelation.mapChildren(v2Commands.scala:726)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning(AnalysisHelper.scala:186)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning$(AnalysisHelper.scala:167)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithSubqueriesAndPruning(LogicalPlan.scala:42)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.resolveOperatorsUpWithSubqueriesAndPruningByConf(Analyzer.scala:1674)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply0(Analyzer.scala:1466)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1419)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1365)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:480)\n\t\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:629)\n\t\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:613)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:131)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:480)\n\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:479)\n\t\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n\t\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n\t\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:475)\n\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:452)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22(RuleExecutor.scala:585)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22$adapted(RuleExecutor.scala:585)\n\t\tat scala.collection.immutable.List.foreach(List.scala:431)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:585)\n\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:349)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:498)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:491)\n\t\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:397)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:491)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:416)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:341)\n\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:219)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:341)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:252)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:96)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:131)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:87)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:478)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:478)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:294)\n\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:548)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:668)\n\t\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:151)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:668)\n\t\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1307)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:661)\n\t\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:658)\n\t\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n\t\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:658)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:288)\n\t\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:287)\n\t\tat scala.util.Try$.apply(Try.scala:213)\n\t\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)\n\t\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n\t\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n\t\t... 24 more\n",
         "2025-08-13T14:34:05.612553Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "table/view",
         "`accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample`.`forecast`.`us_postal_hourly_imperial`",
         "clone/create",
         "error",
         "Failed to process table/view `accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample`.`forecast`.`us_postal_hourly_imperial`. Error: An error occurred while calling o427.sql.\n: com.databricks.sql.managedcatalog.acl.UnauthorizedAccessException: PERMISSION_DENIED: User does not have USE CATALOG on Catalog 'accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample'.\n\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:87)\n\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:63)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:251)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapExceptionBase(ManagedCatalogClientImpl.scala:7378)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:7364)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.getTableCredentials(ManagedCatalogClientImpl.scala:3951)\n\tat com.databricks.sql.managedcatalog.ManagedCatalogClient.getTemporaryCredentials(ManagedCatalogClient.scala:2620)\n\tat com.databricks.sql.managedcatalog.ManagedCatalogClient.getTemporaryCredentials$(ManagedCatalogClient.scala:2605)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.getTemporaryCredentials(ManagedCatalogClientImpl.scala:251)\n\tat com.databricks.unity.TempCredCache.$anonfun$getInternal$7(TemporaryCredentials.scala:448)\n\tat com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4724)\n\tat com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3522)\n\tat com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2315)\n\tat com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2278)\n\tat com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2193)\n\tat com.google.common.cache.LocalCache.get(LocalCache.java:3932)\n\tat com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4721)\n\tat com.databricks.unity.TempCredCache.liftedTree1$1(TemporaryCredentials.scala:447)\n\tat com.databricks.unity.TempCredCache.getInternal(TemporaryCredentials.scala:446)\n\tat com.databricks.unity.TempCredCache.get(TemporaryCredentials.scala:373)\n\tat com.databricks.unity.UnityCredentialManager.getTemporaryCredentials(CredentialManager.scala:488)\n\tat com.databricks.unity.CredentialManager$.getTemporaryCredentials(CredentialManager.scala:881)\n\tat com.databricks.unity.SAM.setConfFromCred$1(SAM.scala:338)\n\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$8(SAM.scala:353)\n\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$8$adapted(SAM.scala:352)\n\tat com.databricks.unity.TempCredRequest$.$anonfun$withFallbackOnOverlappingForParentPath$3(TemporaryCredentials.scala:147)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.databricks.unity.TempCredRequest$.withFallbackOnOverlappingForParentPath(TemporaryCredentials.scala:147)\n\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$7(SAM.scala:352)\n\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$7$adapted(SAM.scala:350)\n\tat scala.Option.map(Option.scala:230)\n\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$6(SAM.scala:350)\n\tat scala.Option.flatMap(Option.scala:271)\n\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$1(SAM.scala:349)\n\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$1$adapted(SAM.scala:336)\n\tat scala.Option.foreach(Option.scala:407)\n\tat com.databricks.unity.SAM.setFileSystemConfigs(SAM.scala:336)\n\tat com.databricks.unity.SAM.createDelegate(SAM.scala:388)\n\tat com.databricks.unity.SAM.createDelegate$(SAM.scala:372)\n\tat com.databricks.unity.TableSAM.createDelegate(SAM.scala:695)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.createDelegate(CredentialScopeFileSystem.scala:74)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.$anonfun$setDelegates$1(CredentialScopeFileSystem.scala:127)\n\tat com.databricks.sql.acl.fs.Lazy.apply(DelegatingFileSystem.scala:310)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.readDelegateMakeQualified(CredentialScopeFileSystem.scala:282)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.optimizedMakeQualifiedPath(CredentialScopeFileSystem.scala:277)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.makeQualified(CredentialScopeFileSystem.scala:294)\n\tat com.databricks.sql.transaction.tahoe.DeltaLog$.apply(DeltaLog.scala:1284)\n\tat com.databricks.sql.transaction.tahoe.DeltaLog$.forTable(DeltaLog.scala:1087)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$5(DeltaTableV2.scala:175)\n\tat com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$withAdditionalSnapshotInitializationUsageLogData$1(DeltaLog.scala:968)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n\tat com.databricks.sql.transaction.tahoe.DeltaLog$.withAdditionalSnapshotInitializationUsageLogData(DeltaLog.scala:969)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$2(DeltaTableV2.scala:169)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:700)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$1(DeltaTableV2.scala:150)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:83)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.deltaLog$lzycompute(DeltaTableV2.scala:150)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.deltaLog(DeltaTableV2.scala:148)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$8(DeltaTableV2.scala:264)\n\tat com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$withAdditionalSnapshotInitializationUsageLogData$1(DeltaLog.scala:968)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n\tat com.databricks.sql.transaction.tahoe.DeltaLog$.withAdditionalSnapshotInitializationUsageLogData(DeltaLog.scala:969)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$7(DeltaTableV2.scala:263)\n\tat scala.Option.orElse(Option.scala:447)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$5(DeltaTableV2.scala:263)\n\tat scala.Option.orElse(Option.scala:447)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$2(DeltaTableV2.scala:260)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:83)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$1(DeltaTableV2.scala:290)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:700)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot$lzycompute(DeltaTableV2.scala:240)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot(DeltaTableV2.scala:238)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$tableSchema$2(DeltaTableV2.scala:317)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.tableSchema$lzycompute(DeltaTableV2.scala:317)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.tableSchema(DeltaTableV2.scala:315)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.schema(DeltaTableV2.scala:322)\n\tat org.apache.spark.sql.connector.catalog.Table.columns(Table.java:68)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.columns(DeltaTableV2.scala:83)\n\tat org.apache.spark.sql.catalyst.analysis.ResolvedTable$.create(v2ResolutionPlans.scala:231)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$4(Analyzer.scala:1600)\n\tat scala.Option.map(Option.scala:230)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$3(Analyzer.scala:1592)\n\tat scala.Option.orElse(Option.scala:447)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableOrView(Analyzer.scala:1587)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1556)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1466)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:182)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:171)\n\tat scala.PartialFunction.applyOrElse(PartialFunction.scala:127)\n\tat scala.PartialFunction.applyOrElse$(PartialFunction.scala:126)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.applyOrElse(AnalysisHelper.scala:171)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:138)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1330)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1329)\n\tat org.apache.spark.sql.catalyst.plans.logical.DescribeRelation.mapChildren(v2Commands.scala:726)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning(AnalysisHelper.scala:186)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning$(AnalysisHelper.scala:167)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithSubqueriesAndPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.resolveOperatorsUpWithSubqueriesAndPruningByConf(Analyzer.scala:1674)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply0(Analyzer.scala:1466)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1419)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1365)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:480)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:629)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:613)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:131)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:480)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:479)\n\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:475)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:452)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22(RuleExecutor.scala:585)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22$adapted(RuleExecutor.scala:585)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:585)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:349)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:498)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:491)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:397)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:491)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:416)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:341)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:219)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:341)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:252)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:96)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:131)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:87)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:478)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:478)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:294)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:548)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:668)\n\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:151)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:668)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1307)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:661)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:658)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:658)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:288)\n\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:287)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:321)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:267)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$3(Dataset.scala:149)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n\tat org.apache.spark.sql.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1469)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1469)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:141)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:1048)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:1001)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:1071)\n\tat jdk.internal.reflect.GeneratedMethodAccessor593.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:197)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:117)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\n\t\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:87)\n\t\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:63)\n\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:251)\n\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapExceptionBase(ManagedCatalogClientImpl.scala:7378)\n\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:7364)\n\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.getTableCredentials(ManagedCatalogClientImpl.scala:3951)\n\t\tat com.databricks.sql.managedcatalog.ManagedCatalogClient.getTemporaryCredentials(ManagedCatalogClient.scala:2620)\n\t\tat com.databricks.sql.managedcatalog.ManagedCatalogClient.getTemporaryCredentials$(ManagedCatalogClient.scala:2605)\n\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.getTemporaryCredentials(ManagedCatalogClientImpl.scala:251)\n\t\tat com.databricks.unity.TempCredCache.$anonfun$getInternal$7(TemporaryCredentials.scala:448)\n\t\tat com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4724)\n\t\tat com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3522)\n\t\tat com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2315)\n\t\tat com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2278)\n\t\tat com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2193)\n\t\tat com.google.common.cache.LocalCache.get(LocalCache.java:3932)\n\t\tat com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4721)\n\t\tat com.databricks.unity.TempCredCache.liftedTree1$1(TemporaryCredentials.scala:447)\n\t\tat com.databricks.unity.TempCredCache.getInternal(TemporaryCredentials.scala:446)\n\t\tat com.databricks.unity.TempCredCache.get(TemporaryCredentials.scala:373)\n\t\tat com.databricks.unity.UnityCredentialManager.getTemporaryCredentials(CredentialManager.scala:488)\n\t\tat com.databricks.unity.CredentialManager$.getTemporaryCredentials(CredentialManager.scala:881)\n\t\tat com.databricks.unity.SAM.setConfFromCred$1(SAM.scala:338)\n\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$8(SAM.scala:353)\n\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$8$adapted(SAM.scala:352)\n\t\tat com.databricks.unity.TempCredRequest$.$anonfun$withFallbackOnOverlappingForParentPath$3(TemporaryCredentials.scala:147)\n\t\tat scala.util.Try$.apply(Try.scala:213)\n\t\tat com.databricks.unity.TempCredRequest$.withFallbackOnOverlappingForParentPath(TemporaryCredentials.scala:147)\n\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$7(SAM.scala:352)\n\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$7$adapted(SAM.scala:350)\n\t\tat scala.Option.map(Option.scala:230)\n\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$6(SAM.scala:350)\n\t\tat scala.Option.flatMap(Option.scala:271)\n\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$1(SAM.scala:349)\n\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$1$adapted(SAM.scala:336)\n\t\tat scala.Option.foreach(Option.scala:407)\n\t\tat com.databricks.unity.SAM.setFileSystemConfigs(SAM.scala:336)\n\t\tat com.databricks.unity.SAM.createDelegate(SAM.scala:388)\n\t\tat com.databricks.unity.SAM.createDelegate$(SAM.scala:372)\n\t\tat com.databricks.unity.TableSAM.createDelegate(SAM.scala:695)\n\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.createDelegate(CredentialScopeFileSystem.scala:74)\n\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.$anonfun$setDelegates$1(CredentialScopeFileSystem.scala:127)\n\t\tat com.databricks.sql.acl.fs.Lazy.apply(DelegatingFileSystem.scala:310)\n\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.readDelegateMakeQualified(CredentialScopeFileSystem.scala:282)\n\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.optimizedMakeQualifiedPath(CredentialScopeFileSystem.scala:277)\n\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.makeQualified(CredentialScopeFileSystem.scala:294)\n\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.apply(DeltaLog.scala:1284)\n\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.forTable(DeltaLog.scala:1087)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$5(DeltaTableV2.scala:175)\n\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$withAdditionalSnapshotInitializationUsageLogData$1(DeltaLog.scala:968)\n\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.withAdditionalSnapshotInitializationUsageLogData(DeltaLog.scala:969)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$2(DeltaTableV2.scala:169)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:700)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$1(DeltaTableV2.scala:150)\n\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\t\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n\t\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:83)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.deltaLog$lzycompute(DeltaTableV2.scala:150)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.deltaLog(DeltaTableV2.scala:148)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$8(DeltaTableV2.scala:264)\n\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$withAdditionalSnapshotInitializationUsageLogData$1(DeltaLog.scala:968)\n\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.withAdditionalSnapshotInitializationUsageLogData(DeltaLog.scala:969)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$7(DeltaTableV2.scala:263)\n\t\tat scala.Option.orElse(Option.scala:447)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$5(DeltaTableV2.scala:263)\n\t\tat scala.Option.orElse(Option.scala:447)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$2(DeltaTableV2.scala:260)\n\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\t\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n\t\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:83)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$1(DeltaTableV2.scala:290)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:700)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot$lzycompute(DeltaTableV2.scala:240)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot(DeltaTableV2.scala:238)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$tableSchema$2(DeltaTableV2.scala:317)\n\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.tableSchema$lzycompute(DeltaTableV2.scala:317)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.tableSchema(DeltaTableV2.scala:315)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.schema(DeltaTableV2.scala:322)\n\t\tat org.apache.spark.sql.connector.catalog.Table.columns(Table.java:68)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.columns(DeltaTableV2.scala:83)\n\t\tat org.apache.spark.sql.catalyst.analysis.ResolvedTable$.create(v2ResolutionPlans.scala:231)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$4(Analyzer.scala:1600)\n\t\tat scala.Option.map(Option.scala:230)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$3(Analyzer.scala:1592)\n\t\tat scala.Option.orElse(Option.scala:447)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableOrView(Analyzer.scala:1587)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1556)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1466)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:182)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:171)\n\t\tat scala.PartialFunction.applyOrElse(PartialFunction.scala:127)\n\t\tat scala.PartialFunction.applyOrElse$(PartialFunction.scala:126)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.applyOrElse(AnalysisHelper.scala:171)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)\n\t\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:138)\n\t\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1330)\n\t\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1329)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.DescribeRelation.mapChildren(v2Commands.scala:726)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning(AnalysisHelper.scala:186)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning$(AnalysisHelper.scala:167)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithSubqueriesAndPruning(LogicalPlan.scala:42)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.resolveOperatorsUpWithSubqueriesAndPruningByConf(Analyzer.scala:1674)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply0(Analyzer.scala:1466)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1419)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1365)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:480)\n\t\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:629)\n\t\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:613)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:131)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:480)\n\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:479)\n\t\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n\t\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n\t\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:475)\n\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:452)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22(RuleExecutor.scala:585)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22$adapted(RuleExecutor.scala:585)\n\t\tat scala.collection.immutable.List.foreach(List.scala:431)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:585)\n\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:349)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:498)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:491)\n\t\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:397)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:491)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:416)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:341)\n\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:219)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:341)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:252)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:96)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:131)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:87)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:478)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:478)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:294)\n\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:548)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:668)\n\t\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:151)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:668)\n\t\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1307)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:661)\n\t\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:658)\n\t\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n\t\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:658)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:288)\n\t\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:287)\n\t\tat scala.util.Try$.apply(Try.scala:213)\n\t\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)\n\t\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n\t\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n\t\t... 24 more\n",
         "2025-08-13T14:34:05.336454Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "table/view",
         "`accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample`.`forecast`.`us_postal_daynight_metric`",
         "clone/create",
         "error",
         "Failed to process table/view `accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample`.`forecast`.`us_postal_daynight_metric`. Error: An error occurred while calling o427.sql.\n: com.databricks.sql.managedcatalog.acl.UnauthorizedAccessException: PERMISSION_DENIED: User does not have USE CATALOG on Catalog 'accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample'.\n\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:87)\n\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:63)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:251)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapExceptionBase(ManagedCatalogClientImpl.scala:7378)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:7364)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.getTableCredentials(ManagedCatalogClientImpl.scala:3951)\n\tat com.databricks.sql.managedcatalog.ManagedCatalogClient.getTemporaryCredentials(ManagedCatalogClient.scala:2620)\n\tat com.databricks.sql.managedcatalog.ManagedCatalogClient.getTemporaryCredentials$(ManagedCatalogClient.scala:2605)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.getTemporaryCredentials(ManagedCatalogClientImpl.scala:251)\n\tat com.databricks.unity.TempCredCache.$anonfun$getInternal$7(TemporaryCredentials.scala:448)\n\tat com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4724)\n\tat com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3522)\n\tat com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2315)\n\tat com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2278)\n\tat com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2193)\n\tat com.google.common.cache.LocalCache.get(LocalCache.java:3932)\n\tat com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4721)\n\tat com.databricks.unity.TempCredCache.liftedTree1$1(TemporaryCredentials.scala:447)\n\tat com.databricks.unity.TempCredCache.getInternal(TemporaryCredentials.scala:446)\n\tat com.databricks.unity.TempCredCache.get(TemporaryCredentials.scala:373)\n\tat com.databricks.unity.UnityCredentialManager.getTemporaryCredentials(CredentialManager.scala:488)\n\tat com.databricks.unity.CredentialManager$.getTemporaryCredentials(CredentialManager.scala:881)\n\tat com.databricks.unity.SAM.setConfFromCred$1(SAM.scala:338)\n\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$8(SAM.scala:353)\n\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$8$adapted(SAM.scala:352)\n\tat com.databricks.unity.TempCredRequest$.$anonfun$withFallbackOnOverlappingForParentPath$3(TemporaryCredentials.scala:147)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.databricks.unity.TempCredRequest$.withFallbackOnOverlappingForParentPath(TemporaryCredentials.scala:147)\n\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$7(SAM.scala:352)\n\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$7$adapted(SAM.scala:350)\n\tat scala.Option.map(Option.scala:230)\n\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$6(SAM.scala:350)\n\tat scala.Option.flatMap(Option.scala:271)\n\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$1(SAM.scala:349)\n\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$1$adapted(SAM.scala:336)\n\tat scala.Option.foreach(Option.scala:407)\n\tat com.databricks.unity.SAM.setFileSystemConfigs(SAM.scala:336)\n\tat com.databricks.unity.SAM.createDelegate(SAM.scala:388)\n\tat com.databricks.unity.SAM.createDelegate$(SAM.scala:372)\n\tat com.databricks.unity.TableSAM.createDelegate(SAM.scala:695)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.createDelegate(CredentialScopeFileSystem.scala:74)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.$anonfun$setDelegates$1(CredentialScopeFileSystem.scala:127)\n\tat com.databricks.sql.acl.fs.Lazy.apply(DelegatingFileSystem.scala:310)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.readDelegateMakeQualified(CredentialScopeFileSystem.scala:282)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.optimizedMakeQualifiedPath(CredentialScopeFileSystem.scala:277)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.makeQualified(CredentialScopeFileSystem.scala:294)\n\tat com.databricks.sql.transaction.tahoe.DeltaLog$.apply(DeltaLog.scala:1284)\n\tat com.databricks.sql.transaction.tahoe.DeltaLog$.forTable(DeltaLog.scala:1087)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$5(DeltaTableV2.scala:175)\n\tat com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$withAdditionalSnapshotInitializationUsageLogData$1(DeltaLog.scala:968)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n\tat com.databricks.sql.transaction.tahoe.DeltaLog$.withAdditionalSnapshotInitializationUsageLogData(DeltaLog.scala:969)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$2(DeltaTableV2.scala:169)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:700)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$1(DeltaTableV2.scala:150)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:83)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.deltaLog$lzycompute(DeltaTableV2.scala:150)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.deltaLog(DeltaTableV2.scala:148)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$8(DeltaTableV2.scala:264)\n\tat com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$withAdditionalSnapshotInitializationUsageLogData$1(DeltaLog.scala:968)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n\tat com.databricks.sql.transaction.tahoe.DeltaLog$.withAdditionalSnapshotInitializationUsageLogData(DeltaLog.scala:969)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$7(DeltaTableV2.scala:263)\n\tat scala.Option.orElse(Option.scala:447)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$5(DeltaTableV2.scala:263)\n\tat scala.Option.orElse(Option.scala:447)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$2(DeltaTableV2.scala:260)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:83)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$1(DeltaTableV2.scala:290)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:700)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot$lzycompute(DeltaTableV2.scala:240)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot(DeltaTableV2.scala:238)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$tableSchema$2(DeltaTableV2.scala:317)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.tableSchema$lzycompute(DeltaTableV2.scala:317)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.tableSchema(DeltaTableV2.scala:315)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.schema(DeltaTableV2.scala:322)\n\tat org.apache.spark.sql.connector.catalog.Table.columns(Table.java:68)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.columns(DeltaTableV2.scala:83)\n\tat org.apache.spark.sql.catalyst.analysis.ResolvedTable$.create(v2ResolutionPlans.scala:231)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$4(Analyzer.scala:1600)\n\tat scala.Option.map(Option.scala:230)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$3(Analyzer.scala:1592)\n\tat scala.Option.orElse(Option.scala:447)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableOrView(Analyzer.scala:1587)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1556)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1466)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:182)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:171)\n\tat scala.PartialFunction.applyOrElse(PartialFunction.scala:127)\n\tat scala.PartialFunction.applyOrElse$(PartialFunction.scala:126)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.applyOrElse(AnalysisHelper.scala:171)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:138)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1330)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1329)\n\tat org.apache.spark.sql.catalyst.plans.logical.DescribeRelation.mapChildren(v2Commands.scala:726)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning(AnalysisHelper.scala:186)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning$(AnalysisHelper.scala:167)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithSubqueriesAndPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.resolveOperatorsUpWithSubqueriesAndPruningByConf(Analyzer.scala:1674)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply0(Analyzer.scala:1466)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1419)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1365)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:480)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:629)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:613)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:131)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:480)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:479)\n\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:475)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:452)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22(RuleExecutor.scala:585)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22$adapted(RuleExecutor.scala:585)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:585)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:349)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:498)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:491)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:397)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:491)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:416)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:341)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:219)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:341)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:252)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:96)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:131)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:87)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:478)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:478)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:294)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:548)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:668)\n\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:151)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:668)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1307)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:661)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:658)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:658)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:288)\n\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:287)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:321)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:267)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$3(Dataset.scala:149)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n\tat org.apache.spark.sql.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1469)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1469)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:141)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:1048)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:1001)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:1071)\n\tat jdk.internal.reflect.GeneratedMethodAccessor593.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:197)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:117)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\n\t\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:87)\n\t\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:63)\n\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:251)\n\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapExceptionBase(ManagedCatalogClientImpl.scala:7378)\n\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:7364)\n\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.getTableCredentials(ManagedCatalogClientImpl.scala:3951)\n\t\tat com.databricks.sql.managedcatalog.ManagedCatalogClient.getTemporaryCredentials(ManagedCatalogClient.scala:2620)\n\t\tat com.databricks.sql.managedcatalog.ManagedCatalogClient.getTemporaryCredentials$(ManagedCatalogClient.scala:2605)\n\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.getTemporaryCredentials(ManagedCatalogClientImpl.scala:251)\n\t\tat com.databricks.unity.TempCredCache.$anonfun$getInternal$7(TemporaryCredentials.scala:448)\n\t\tat com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4724)\n\t\tat com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3522)\n\t\tat com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2315)\n\t\tat com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2278)\n\t\tat com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2193)\n\t\tat com.google.common.cache.LocalCache.get(LocalCache.java:3932)\n\t\tat com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4721)\n\t\tat com.databricks.unity.TempCredCache.liftedTree1$1(TemporaryCredentials.scala:447)\n\t\tat com.databricks.unity.TempCredCache.getInternal(TemporaryCredentials.scala:446)\n\t\tat com.databricks.unity.TempCredCache.get(TemporaryCredentials.scala:373)\n\t\tat com.databricks.unity.UnityCredentialManager.getTemporaryCredentials(CredentialManager.scala:488)\n\t\tat com.databricks.unity.CredentialManager$.getTemporaryCredentials(CredentialManager.scala:881)\n\t\tat com.databricks.unity.SAM.setConfFromCred$1(SAM.scala:338)\n\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$8(SAM.scala:353)\n\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$8$adapted(SAM.scala:352)\n\t\tat com.databricks.unity.TempCredRequest$.$anonfun$withFallbackOnOverlappingForParentPath$3(TemporaryCredentials.scala:147)\n\t\tat scala.util.Try$.apply(Try.scala:213)\n\t\tat com.databricks.unity.TempCredRequest$.withFallbackOnOverlappingForParentPath(TemporaryCredentials.scala:147)\n\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$7(SAM.scala:352)\n\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$7$adapted(SAM.scala:350)\n\t\tat scala.Option.map(Option.scala:230)\n\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$6(SAM.scala:350)\n\t\tat scala.Option.flatMap(Option.scala:271)\n\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$1(SAM.scala:349)\n\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$1$adapted(SAM.scala:336)\n\t\tat scala.Option.foreach(Option.scala:407)\n\t\tat com.databricks.unity.SAM.setFileSystemConfigs(SAM.scala:336)\n\t\tat com.databricks.unity.SAM.createDelegate(SAM.scala:388)\n\t\tat com.databricks.unity.SAM.createDelegate$(SAM.scala:372)\n\t\tat com.databricks.unity.TableSAM.createDelegate(SAM.scala:695)\n\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.createDelegate(CredentialScopeFileSystem.scala:74)\n\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.$anonfun$setDelegates$1(CredentialScopeFileSystem.scala:127)\n\t\tat com.databricks.sql.acl.fs.Lazy.apply(DelegatingFileSystem.scala:310)\n\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.readDelegateMakeQualified(CredentialScopeFileSystem.scala:282)\n\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.optimizedMakeQualifiedPath(CredentialScopeFileSystem.scala:277)\n\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.makeQualified(CredentialScopeFileSystem.scala:294)\n\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.apply(DeltaLog.scala:1284)\n\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.forTable(DeltaLog.scala:1087)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$5(DeltaTableV2.scala:175)\n\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$withAdditionalSnapshotInitializationUsageLogData$1(DeltaLog.scala:968)\n\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.withAdditionalSnapshotInitializationUsageLogData(DeltaLog.scala:969)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$2(DeltaTableV2.scala:169)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:700)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$1(DeltaTableV2.scala:150)\n\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\t\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n\t\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:83)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.deltaLog$lzycompute(DeltaTableV2.scala:150)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.deltaLog(DeltaTableV2.scala:148)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$8(DeltaTableV2.scala:264)\n\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$withAdditionalSnapshotInitializationUsageLogData$1(DeltaLog.scala:968)\n\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.withAdditionalSnapshotInitializationUsageLogData(DeltaLog.scala:969)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$7(DeltaTableV2.scala:263)\n\t\tat scala.Option.orElse(Option.scala:447)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$5(DeltaTableV2.scala:263)\n\t\tat scala.Option.orElse(Option.scala:447)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$2(DeltaTableV2.scala:260)\n\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\t\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n\t\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:83)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$1(DeltaTableV2.scala:290)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:700)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot$lzycompute(DeltaTableV2.scala:240)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot(DeltaTableV2.scala:238)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$tableSchema$2(DeltaTableV2.scala:317)\n\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.tableSchema$lzycompute(DeltaTableV2.scala:317)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.tableSchema(DeltaTableV2.scala:315)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.schema(DeltaTableV2.scala:322)\n\t\tat org.apache.spark.sql.connector.catalog.Table.columns(Table.java:68)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.columns(DeltaTableV2.scala:83)\n\t\tat org.apache.spark.sql.catalyst.analysis.ResolvedTable$.create(v2ResolutionPlans.scala:231)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$4(Analyzer.scala:1600)\n\t\tat scala.Option.map(Option.scala:230)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$3(Analyzer.scala:1592)\n\t\tat scala.Option.orElse(Option.scala:447)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableOrView(Analyzer.scala:1587)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1556)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1466)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:182)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:171)\n\t\tat scala.PartialFunction.applyOrElse(PartialFunction.scala:127)\n\t\tat scala.PartialFunction.applyOrElse$(PartialFunction.scala:126)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.applyOrElse(AnalysisHelper.scala:171)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)\n\t\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:138)\n\t\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1330)\n\t\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1329)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.DescribeRelation.mapChildren(v2Commands.scala:726)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning(AnalysisHelper.scala:186)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning$(AnalysisHelper.scala:167)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithSubqueriesAndPruning(LogicalPlan.scala:42)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.resolveOperatorsUpWithSubqueriesAndPruningByConf(Analyzer.scala:1674)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply0(Analyzer.scala:1466)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1419)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1365)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:480)\n\t\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:629)\n\t\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:613)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:131)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:480)\n\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:479)\n\t\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n\t\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n\t\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:475)\n\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:452)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22(RuleExecutor.scala:585)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22$adapted(RuleExecutor.scala:585)\n\t\tat scala.collection.immutable.List.foreach(List.scala:431)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:585)\n\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:349)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:498)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:491)\n\t\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:397)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:491)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:416)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:341)\n\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:219)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:341)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:252)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:96)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:131)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:87)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:478)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:478)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:294)\n\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:548)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:668)\n\t\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:151)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:668)\n\t\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1307)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:661)\n\t\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:658)\n\t\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n\t\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:658)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:288)\n\t\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:287)\n\t\tat scala.util.Try$.apply(Try.scala:213)\n\t\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)\n\t\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n\t\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n\t\t... 24 more\n",
         "2025-08-13T14:34:02.00451Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "table",
         "`dr_eo_amperity`.`silver`.`unified_transactions`",
         "clone",
         "error",
         "Failed to clone table `dr_eo_amperity`.`silver`.`unified_transactions`.. Error: [DELTA_CLONE_UNSUPPORTED_SOURCE] Unsupported DEEP clone source '_eo_amperity.silver.unified_transactions', whose format is deltasharing.\nThe supported formats are 'delta', 'iceberg' and 'parquet'.",
         "2025-08-13T14:34:00.880227Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "table/view",
         "`accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample`.`forecast`.`us_postal_daynight_imperial`",
         "clone/create",
         "error",
         "Failed to process table/view `accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample`.`forecast`.`us_postal_daynight_imperial`. Error: An error occurred while calling o427.sql.\n: com.databricks.sql.managedcatalog.acl.UnauthorizedAccessException: PERMISSION_DENIED: User does not have USE CATALOG on Catalog 'accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample'.\n\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:87)\n\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:63)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:251)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapExceptionBase(ManagedCatalogClientImpl.scala:7378)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:7364)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.getTableCredentials(ManagedCatalogClientImpl.scala:3951)\n\tat com.databricks.sql.managedcatalog.ManagedCatalogClient.getTemporaryCredentials(ManagedCatalogClient.scala:2620)\n\tat com.databricks.sql.managedcatalog.ManagedCatalogClient.getTemporaryCredentials$(ManagedCatalogClient.scala:2605)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.getTemporaryCredentials(ManagedCatalogClientImpl.scala:251)\n\tat com.databricks.unity.TempCredCache.$anonfun$getInternal$7(TemporaryCredentials.scala:448)\n\tat com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4724)\n\tat com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3522)\n\tat com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2315)\n\tat com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2278)\n\tat com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2193)\n\tat com.google.common.cache.LocalCache.get(LocalCache.java:3932)\n\tat com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4721)\n\tat com.databricks.unity.TempCredCache.liftedTree1$1(TemporaryCredentials.scala:447)\n\tat com.databricks.unity.TempCredCache.getInternal(TemporaryCredentials.scala:446)\n\tat com.databricks.unity.TempCredCache.get(TemporaryCredentials.scala:373)\n\tat com.databricks.unity.UnityCredentialManager.getTemporaryCredentials(CredentialManager.scala:488)\n\tat com.databricks.unity.CredentialManager$.getTemporaryCredentials(CredentialManager.scala:881)\n\tat com.databricks.unity.SAM.setConfFromCred$1(SAM.scala:338)\n\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$8(SAM.scala:353)\n\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$8$adapted(SAM.scala:352)\n\tat com.databricks.unity.TempCredRequest$.$anonfun$withFallbackOnOverlappingForParentPath$3(TemporaryCredentials.scala:147)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.databricks.unity.TempCredRequest$.withFallbackOnOverlappingForParentPath(TemporaryCredentials.scala:147)\n\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$7(SAM.scala:352)\n\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$7$adapted(SAM.scala:350)\n\tat scala.Option.map(Option.scala:230)\n\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$6(SAM.scala:350)\n\tat scala.Option.flatMap(Option.scala:271)\n\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$1(SAM.scala:349)\n\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$1$adapted(SAM.scala:336)\n\tat scala.Option.foreach(Option.scala:407)\n\tat com.databricks.unity.SAM.setFileSystemConfigs(SAM.scala:336)\n\tat com.databricks.unity.SAM.createDelegate(SAM.scala:388)\n\tat com.databricks.unity.SAM.createDelegate$(SAM.scala:372)\n\tat com.databricks.unity.TableSAM.createDelegate(SAM.scala:695)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.createDelegate(CredentialScopeFileSystem.scala:74)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.$anonfun$setDelegates$1(CredentialScopeFileSystem.scala:127)\n\tat com.databricks.sql.acl.fs.Lazy.apply(DelegatingFileSystem.scala:310)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.readDelegateMakeQualified(CredentialScopeFileSystem.scala:282)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.optimizedMakeQualifiedPath(CredentialScopeFileSystem.scala:277)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.makeQualified(CredentialScopeFileSystem.scala:294)\n\tat com.databricks.sql.transaction.tahoe.DeltaLog$.apply(DeltaLog.scala:1284)\n\tat com.databricks.sql.transaction.tahoe.DeltaLog$.forTable(DeltaLog.scala:1087)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$5(DeltaTableV2.scala:175)\n\tat com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$withAdditionalSnapshotInitializationUsageLogData$1(DeltaLog.scala:968)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n\tat com.databricks.sql.transaction.tahoe.DeltaLog$.withAdditionalSnapshotInitializationUsageLogData(DeltaLog.scala:969)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$2(DeltaTableV2.scala:169)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:700)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$1(DeltaTableV2.scala:150)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:83)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.deltaLog$lzycompute(DeltaTableV2.scala:150)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.deltaLog(DeltaTableV2.scala:148)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$8(DeltaTableV2.scala:264)\n\tat com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$withAdditionalSnapshotInitializationUsageLogData$1(DeltaLog.scala:968)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n\tat com.databricks.sql.transaction.tahoe.DeltaLog$.withAdditionalSnapshotInitializationUsageLogData(DeltaLog.scala:969)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$7(DeltaTableV2.scala:263)\n\tat scala.Option.orElse(Option.scala:447)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$5(DeltaTableV2.scala:263)\n\tat scala.Option.orElse(Option.scala:447)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$2(DeltaTableV2.scala:260)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:83)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$1(DeltaTableV2.scala:290)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:700)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot$lzycompute(DeltaTableV2.scala:240)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot(DeltaTableV2.scala:238)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$tableSchema$2(DeltaTableV2.scala:317)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.tableSchema$lzycompute(DeltaTableV2.scala:317)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.tableSchema(DeltaTableV2.scala:315)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.schema(DeltaTableV2.scala:322)\n\tat org.apache.spark.sql.connector.catalog.Table.columns(Table.java:68)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.columns(DeltaTableV2.scala:83)\n\tat org.apache.spark.sql.catalyst.analysis.ResolvedTable$.create(v2ResolutionPlans.scala:231)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$4(Analyzer.scala:1600)\n\tat scala.Option.map(Option.scala:230)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$3(Analyzer.scala:1592)\n\tat scala.Option.orElse(Option.scala:447)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableOrView(Analyzer.scala:1587)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1556)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1466)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:182)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:171)\n\tat scala.PartialFunction.applyOrElse(PartialFunction.scala:127)\n\tat scala.PartialFunction.applyOrElse$(PartialFunction.scala:126)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.applyOrElse(AnalysisHelper.scala:171)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:138)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1330)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1329)\n\tat org.apache.spark.sql.catalyst.plans.logical.DescribeRelation.mapChildren(v2Commands.scala:726)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning(AnalysisHelper.scala:186)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning$(AnalysisHelper.scala:167)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithSubqueriesAndPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.resolveOperatorsUpWithSubqueriesAndPruningByConf(Analyzer.scala:1674)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply0(Analyzer.scala:1466)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1419)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1365)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:480)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:629)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:613)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:131)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:480)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:479)\n\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:475)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:452)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22(RuleExecutor.scala:585)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22$adapted(RuleExecutor.scala:585)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:585)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:349)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:498)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:491)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:397)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:491)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:416)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:341)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:219)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:341)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:252)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:96)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:131)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:87)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:478)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:478)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:294)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:548)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:668)\n\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:151)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:668)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1307)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:661)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:658)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:658)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:288)\n\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:287)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:321)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:267)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$3(Dataset.scala:149)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n\tat org.apache.spark.sql.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1469)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1469)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:141)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:1048)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:1001)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:1071)\n\tat jdk.internal.reflect.GeneratedMethodAccessor593.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:197)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:117)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\n\t\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:87)\n\t\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:63)\n\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:251)\n\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapExceptionBase(ManagedCatalogClientImpl.scala:7378)\n\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:7364)\n\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.getTableCredentials(ManagedCatalogClientImpl.scala:3951)\n\t\tat com.databricks.sql.managedcatalog.ManagedCatalogClient.getTemporaryCredentials(ManagedCatalogClient.scala:2620)\n\t\tat com.databricks.sql.managedcatalog.ManagedCatalogClient.getTemporaryCredentials$(ManagedCatalogClient.scala:2605)\n\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.getTemporaryCredentials(ManagedCatalogClientImpl.scala:251)\n\t\tat com.databricks.unity.TempCredCache.$anonfun$getInternal$7(TemporaryCredentials.scala:448)\n\t\tat com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4724)\n\t\tat com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3522)\n\t\tat com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2315)\n\t\tat com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2278)\n\t\tat com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2193)\n\t\tat com.google.common.cache.LocalCache.get(LocalCache.java:3932)\n\t\tat com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4721)\n\t\tat com.databricks.unity.TempCredCache.liftedTree1$1(TemporaryCredentials.scala:447)\n\t\tat com.databricks.unity.TempCredCache.getInternal(TemporaryCredentials.scala:446)\n\t\tat com.databricks.unity.TempCredCache.get(TemporaryCredentials.scala:373)\n\t\tat com.databricks.unity.UnityCredentialManager.getTemporaryCredentials(CredentialManager.scala:488)\n\t\tat com.databricks.unity.CredentialManager$.getTemporaryCredentials(CredentialManager.scala:881)\n\t\tat com.databricks.unity.SAM.setConfFromCred$1(SAM.scala:338)\n\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$8(SAM.scala:353)\n\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$8$adapted(SAM.scala:352)\n\t\tat com.databricks.unity.TempCredRequest$.$anonfun$withFallbackOnOverlappingForParentPath$3(TemporaryCredentials.scala:147)\n\t\tat scala.util.Try$.apply(Try.scala:213)\n\t\tat com.databricks.unity.TempCredRequest$.withFallbackOnOverlappingForParentPath(TemporaryCredentials.scala:147)\n\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$7(SAM.scala:352)\n\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$7$adapted(SAM.scala:350)\n\t\tat scala.Option.map(Option.scala:230)\n\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$6(SAM.scala:350)\n\t\tat scala.Option.flatMap(Option.scala:271)\n\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$1(SAM.scala:349)\n\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$1$adapted(SAM.scala:336)\n\t\tat scala.Option.foreach(Option.scala:407)\n\t\tat com.databricks.unity.SAM.setFileSystemConfigs(SAM.scala:336)\n\t\tat com.databricks.unity.SAM.createDelegate(SAM.scala:388)\n\t\tat com.databricks.unity.SAM.createDelegate$(SAM.scala:372)\n\t\tat com.databricks.unity.TableSAM.createDelegate(SAM.scala:695)\n\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.createDelegate(CredentialScopeFileSystem.scala:74)\n\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.$anonfun$setDelegates$1(CredentialScopeFileSystem.scala:127)\n\t\tat com.databricks.sql.acl.fs.Lazy.apply(DelegatingFileSystem.scala:310)\n\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.readDelegateMakeQualified(CredentialScopeFileSystem.scala:282)\n\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.optimizedMakeQualifiedPath(CredentialScopeFileSystem.scala:277)\n\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.makeQualified(CredentialScopeFileSystem.scala:294)\n\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.apply(DeltaLog.scala:1284)\n\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.forTable(DeltaLog.scala:1087)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$5(DeltaTableV2.scala:175)\n\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$withAdditionalSnapshotInitializationUsageLogData$1(DeltaLog.scala:968)\n\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.withAdditionalSnapshotInitializationUsageLogData(DeltaLog.scala:969)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$2(DeltaTableV2.scala:169)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:700)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$1(DeltaTableV2.scala:150)\n\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\t\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n\t\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:83)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.deltaLog$lzycompute(DeltaTableV2.scala:150)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.deltaLog(DeltaTableV2.scala:148)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$8(DeltaTableV2.scala:264)\n\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$withAdditionalSnapshotInitializationUsageLogData$1(DeltaLog.scala:968)\n\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.withAdditionalSnapshotInitializationUsageLogData(DeltaLog.scala:969)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$7(DeltaTableV2.scala:263)\n\t\tat scala.Option.orElse(Option.scala:447)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$5(DeltaTableV2.scala:263)\n\t\tat scala.Option.orElse(Option.scala:447)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$2(DeltaTableV2.scala:260)\n\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\t\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n\t\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:83)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$1(DeltaTableV2.scala:290)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:700)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot$lzycompute(DeltaTableV2.scala:240)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot(DeltaTableV2.scala:238)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$tableSchema$2(DeltaTableV2.scala:317)\n\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.tableSchema$lzycompute(DeltaTableV2.scala:317)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.tableSchema(DeltaTableV2.scala:315)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.schema(DeltaTableV2.scala:322)\n\t\tat org.apache.spark.sql.connector.catalog.Table.columns(Table.java:68)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.columns(DeltaTableV2.scala:83)\n\t\tat org.apache.spark.sql.catalyst.analysis.ResolvedTable$.create(v2ResolutionPlans.scala:231)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$4(Analyzer.scala:1600)\n\t\tat scala.Option.map(Option.scala:230)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$3(Analyzer.scala:1592)\n\t\tat scala.Option.orElse(Option.scala:447)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableOrView(Analyzer.scala:1587)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1556)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1466)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:182)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:171)\n\t\tat scala.PartialFunction.applyOrElse(PartialFunction.scala:127)\n\t\tat scala.PartialFunction.applyOrElse$(PartialFunction.scala:126)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.applyOrElse(AnalysisHelper.scala:171)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)\n\t\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:138)\n\t\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1330)\n\t\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1329)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.DescribeRelation.mapChildren(v2Commands.scala:726)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning(AnalysisHelper.scala:186)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning$(AnalysisHelper.scala:167)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithSubqueriesAndPruning(LogicalPlan.scala:42)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.resolveOperatorsUpWithSubqueriesAndPruningByConf(Analyzer.scala:1674)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply0(Analyzer.scala:1466)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1419)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1365)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:480)\n\t\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:629)\n\t\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:613)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:131)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:480)\n\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:479)\n\t\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n\t\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n\t\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:475)\n\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:452)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22(RuleExecutor.scala:585)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22$adapted(RuleExecutor.scala:585)\n\t\tat scala.collection.immutable.List.foreach(List.scala:431)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:585)\n\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:349)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:498)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:491)\n\t\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:397)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:491)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:416)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:341)\n\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:219)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:341)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:252)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:96)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:131)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:87)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:478)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:478)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:294)\n\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:548)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:668)\n\t\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:151)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:668)\n\t\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1307)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:661)\n\t\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:658)\n\t\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n\t\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:658)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:288)\n\t\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:287)\n\t\tat scala.util.Try$.apply(Try.scala:213)\n\t\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)\n\t\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n\t\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n\t\t... 24 more\n",
         "2025-08-13T14:34:00.796513Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "table/view",
         "`accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample`.`forecast`.`us_postal_daily_metric`",
         "clone/create",
         "error",
         "Failed to process table/view `accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample`.`forecast`.`us_postal_daily_metric`. Error: An error occurred while calling o427.sql.\n: com.databricks.sql.managedcatalog.acl.UnauthorizedAccessException: PERMISSION_DENIED: User does not have USE CATALOG on Catalog 'accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample'.\n\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:87)\n\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:63)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:251)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapExceptionBase(ManagedCatalogClientImpl.scala:7378)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:7364)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.getTableCredentials(ManagedCatalogClientImpl.scala:3951)\n\tat com.databricks.sql.managedcatalog.ManagedCatalogClient.getTemporaryCredentials(ManagedCatalogClient.scala:2620)\n\tat com.databricks.sql.managedcatalog.ManagedCatalogClient.getTemporaryCredentials$(ManagedCatalogClient.scala:2605)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.getTemporaryCredentials(ManagedCatalogClientImpl.scala:251)\n\tat com.databricks.unity.TempCredCache.$anonfun$getInternal$7(TemporaryCredentials.scala:448)\n\tat com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4724)\n\tat com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3522)\n\tat com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2315)\n\tat com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2278)\n\tat com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2193)\n\tat com.google.common.cache.LocalCache.get(LocalCache.java:3932)\n\tat com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4721)\n\tat com.databricks.unity.TempCredCache.liftedTree1$1(TemporaryCredentials.scala:447)\n\tat com.databricks.unity.TempCredCache.getInternal(TemporaryCredentials.scala:446)\n\tat com.databricks.unity.TempCredCache.get(TemporaryCredentials.scala:373)\n\tat com.databricks.unity.UnityCredentialManager.getTemporaryCredentials(CredentialManager.scala:488)\n\tat com.databricks.unity.CredentialManager$.getTemporaryCredentials(CredentialManager.scala:881)\n\tat com.databricks.unity.SAM.setConfFromCred$1(SAM.scala:338)\n\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$8(SAM.scala:353)\n\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$8$adapted(SAM.scala:352)\n\tat com.databricks.unity.TempCredRequest$.$anonfun$withFallbackOnOverlappingForParentPath$3(TemporaryCredentials.scala:147)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.databricks.unity.TempCredRequest$.withFallbackOnOverlappingForParentPath(TemporaryCredentials.scala:147)\n\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$7(SAM.scala:352)\n\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$7$adapted(SAM.scala:350)\n\tat scala.Option.map(Option.scala:230)\n\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$6(SAM.scala:350)\n\tat scala.Option.flatMap(Option.scala:271)\n\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$1(SAM.scala:349)\n\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$1$adapted(SAM.scala:336)\n\tat scala.Option.foreach(Option.scala:407)\n\tat com.databricks.unity.SAM.setFileSystemConfigs(SAM.scala:336)\n\tat com.databricks.unity.SAM.createDelegate(SAM.scala:388)\n\tat com.databricks.unity.SAM.createDelegate$(SAM.scala:372)\n\tat com.databricks.unity.TableSAM.createDelegate(SAM.scala:695)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.createDelegate(CredentialScopeFileSystem.scala:74)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.$anonfun$setDelegates$1(CredentialScopeFileSystem.scala:127)\n\tat com.databricks.sql.acl.fs.Lazy.apply(DelegatingFileSystem.scala:310)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.readDelegateMakeQualified(CredentialScopeFileSystem.scala:282)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.optimizedMakeQualifiedPath(CredentialScopeFileSystem.scala:277)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.makeQualified(CredentialScopeFileSystem.scala:294)\n\tat com.databricks.sql.transaction.tahoe.DeltaLog$.apply(DeltaLog.scala:1284)\n\tat com.databricks.sql.transaction.tahoe.DeltaLog$.forTable(DeltaLog.scala:1087)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$5(DeltaTableV2.scala:175)\n\tat com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$withAdditionalSnapshotInitializationUsageLogData$1(DeltaLog.scala:968)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n\tat com.databricks.sql.transaction.tahoe.DeltaLog$.withAdditionalSnapshotInitializationUsageLogData(DeltaLog.scala:969)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$2(DeltaTableV2.scala:169)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:700)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$1(DeltaTableV2.scala:150)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:83)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.deltaLog$lzycompute(DeltaTableV2.scala:150)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.deltaLog(DeltaTableV2.scala:148)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$8(DeltaTableV2.scala:264)\n\tat com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$withAdditionalSnapshotInitializationUsageLogData$1(DeltaLog.scala:968)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n\tat com.databricks.sql.transaction.tahoe.DeltaLog$.withAdditionalSnapshotInitializationUsageLogData(DeltaLog.scala:969)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$7(DeltaTableV2.scala:263)\n\tat scala.Option.orElse(Option.scala:447)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$5(DeltaTableV2.scala:263)\n\tat scala.Option.orElse(Option.scala:447)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$2(DeltaTableV2.scala:260)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:83)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$1(DeltaTableV2.scala:290)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:700)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot$lzycompute(DeltaTableV2.scala:240)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot(DeltaTableV2.scala:238)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$tableSchema$2(DeltaTableV2.scala:317)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.tableSchema$lzycompute(DeltaTableV2.scala:317)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.tableSchema(DeltaTableV2.scala:315)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.schema(DeltaTableV2.scala:322)\n\tat org.apache.spark.sql.connector.catalog.Table.columns(Table.java:68)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.columns(DeltaTableV2.scala:83)\n\tat org.apache.spark.sql.catalyst.analysis.ResolvedTable$.create(v2ResolutionPlans.scala:231)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$4(Analyzer.scala:1600)\n\tat scala.Option.map(Option.scala:230)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$3(Analyzer.scala:1592)\n\tat scala.Option.orElse(Option.scala:447)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableOrView(Analyzer.scala:1587)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1556)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1466)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:182)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:171)\n\tat scala.PartialFunction.applyOrElse(PartialFunction.scala:127)\n\tat scala.PartialFunction.applyOrElse$(PartialFunction.scala:126)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.applyOrElse(AnalysisHelper.scala:171)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:138)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1330)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1329)\n\tat org.apache.spark.sql.catalyst.plans.logical.DescribeRelation.mapChildren(v2Commands.scala:726)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning(AnalysisHelper.scala:186)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning$(AnalysisHelper.scala:167)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithSubqueriesAndPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.resolveOperatorsUpWithSubqueriesAndPruningByConf(Analyzer.scala:1674)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply0(Analyzer.scala:1466)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1419)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1365)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:480)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:629)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:613)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:131)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:480)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:479)\n\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:475)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:452)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22(RuleExecutor.scala:585)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22$adapted(RuleExecutor.scala:585)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:585)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:349)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:498)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:491)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:397)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:491)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:416)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:341)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:219)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:341)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:252)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:96)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:131)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:87)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:478)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:478)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:294)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:548)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:668)\n\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:151)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:668)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1307)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:661)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:658)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:658)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:288)\n\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:287)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:321)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:267)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$3(Dataset.scala:149)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n\tat org.apache.spark.sql.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1469)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1469)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:141)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:1048)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:1001)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:1071)\n\tat jdk.internal.reflect.GeneratedMethodAccessor593.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:197)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:117)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\n\t\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:87)\n\t\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:63)\n\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:251)\n\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapExceptionBase(ManagedCatalogClientImpl.scala:7378)\n\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:7364)\n\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.getTableCredentials(ManagedCatalogClientImpl.scala:3951)\n\t\tat com.databricks.sql.managedcatalog.ManagedCatalogClient.getTemporaryCredentials(ManagedCatalogClient.scala:2620)\n\t\tat com.databricks.sql.managedcatalog.ManagedCatalogClient.getTemporaryCredentials$(ManagedCatalogClient.scala:2605)\n\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.getTemporaryCredentials(ManagedCatalogClientImpl.scala:251)\n\t\tat com.databricks.unity.TempCredCache.$anonfun$getInternal$7(TemporaryCredentials.scala:448)\n\t\tat com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4724)\n\t\tat com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3522)\n\t\tat com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2315)\n\t\tat com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2278)\n\t\tat com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2193)\n\t\tat com.google.common.cache.LocalCache.get(LocalCache.java:3932)\n\t\tat com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4721)\n\t\tat com.databricks.unity.TempCredCache.liftedTree1$1(TemporaryCredentials.scala:447)\n\t\tat com.databricks.unity.TempCredCache.getInternal(TemporaryCredentials.scala:446)\n\t\tat com.databricks.unity.TempCredCache.get(TemporaryCredentials.scala:373)\n\t\tat com.databricks.unity.UnityCredentialManager.getTemporaryCredentials(CredentialManager.scala:488)\n\t\tat com.databricks.unity.CredentialManager$.getTemporaryCredentials(CredentialManager.scala:881)\n\t\tat com.databricks.unity.SAM.setConfFromCred$1(SAM.scala:338)\n\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$8(SAM.scala:353)\n\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$8$adapted(SAM.scala:352)\n\t\tat com.databricks.unity.TempCredRequest$.$anonfun$withFallbackOnOverlappingForParentPath$3(TemporaryCredentials.scala:147)\n\t\tat scala.util.Try$.apply(Try.scala:213)\n\t\tat com.databricks.unity.TempCredRequest$.withFallbackOnOverlappingForParentPath(TemporaryCredentials.scala:147)\n\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$7(SAM.scala:352)\n\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$7$adapted(SAM.scala:350)\n\t\tat scala.Option.map(Option.scala:230)\n\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$6(SAM.scala:350)\n\t\tat scala.Option.flatMap(Option.scala:271)\n\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$1(SAM.scala:349)\n\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$1$adapted(SAM.scala:336)\n\t\tat scala.Option.foreach(Option.scala:407)\n\t\tat com.databricks.unity.SAM.setFileSystemConfigs(SAM.scala:336)\n\t\tat com.databricks.unity.SAM.createDelegate(SAM.scala:388)\n\t\tat com.databricks.unity.SAM.createDelegate$(SAM.scala:372)\n\t\tat com.databricks.unity.TableSAM.createDelegate(SAM.scala:695)\n\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.createDelegate(CredentialScopeFileSystem.scala:74)\n\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.$anonfun$setDelegates$1(CredentialScopeFileSystem.scala:127)\n\t\tat com.databricks.sql.acl.fs.Lazy.apply(DelegatingFileSystem.scala:310)\n\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.readDelegateMakeQualified(CredentialScopeFileSystem.scala:282)\n\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.optimizedMakeQualifiedPath(CredentialScopeFileSystem.scala:277)\n\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.makeQualified(CredentialScopeFileSystem.scala:294)\n\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.apply(DeltaLog.scala:1284)\n\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.forTable(DeltaLog.scala:1087)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$5(DeltaTableV2.scala:175)\n\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$withAdditionalSnapshotInitializationUsageLogData$1(DeltaLog.scala:968)\n\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.withAdditionalSnapshotInitializationUsageLogData(DeltaLog.scala:969)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$2(DeltaTableV2.scala:169)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:700)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$1(DeltaTableV2.scala:150)\n\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\t\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n\t\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:83)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.deltaLog$lzycompute(DeltaTableV2.scala:150)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.deltaLog(DeltaTableV2.scala:148)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$8(DeltaTableV2.scala:264)\n\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$withAdditionalSnapshotInitializationUsageLogData$1(DeltaLog.scala:968)\n\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.withAdditionalSnapshotInitializationUsageLogData(DeltaLog.scala:969)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$7(DeltaTableV2.scala:263)\n\t\tat scala.Option.orElse(Option.scala:447)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$5(DeltaTableV2.scala:263)\n\t\tat scala.Option.orElse(Option.scala:447)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$2(DeltaTableV2.scala:260)\n\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\t\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n\t\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:83)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$1(DeltaTableV2.scala:290)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:700)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot$lzycompute(DeltaTableV2.scala:240)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot(DeltaTableV2.scala:238)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$tableSchema$2(DeltaTableV2.scala:317)\n\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.tableSchema$lzycompute(DeltaTableV2.scala:317)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.tableSchema(DeltaTableV2.scala:315)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.schema(DeltaTableV2.scala:322)\n\t\tat org.apache.spark.sql.connector.catalog.Table.columns(Table.java:68)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.columns(DeltaTableV2.scala:83)\n\t\tat org.apache.spark.sql.catalyst.analysis.ResolvedTable$.create(v2ResolutionPlans.scala:231)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$4(Analyzer.scala:1600)\n\t\tat scala.Option.map(Option.scala:230)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$3(Analyzer.scala:1592)\n\t\tat scala.Option.orElse(Option.scala:447)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableOrView(Analyzer.scala:1587)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1556)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1466)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:182)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:171)\n\t\tat scala.PartialFunction.applyOrElse(PartialFunction.scala:127)\n\t\tat scala.PartialFunction.applyOrElse$(PartialFunction.scala:126)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.applyOrElse(AnalysisHelper.scala:171)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)\n\t\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:138)\n\t\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1330)\n\t\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1329)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.DescribeRelation.mapChildren(v2Commands.scala:726)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning(AnalysisHelper.scala:186)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning$(AnalysisHelper.scala:167)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithSubqueriesAndPruning(LogicalPlan.scala:42)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.resolveOperatorsUpWithSubqueriesAndPruningByConf(Analyzer.scala:1674)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply0(Analyzer.scala:1466)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1419)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1365)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:480)\n\t\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:629)\n\t\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:613)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:131)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:480)\n\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:479)\n\t\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n\t\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n\t\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:475)\n\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:452)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22(RuleExecutor.scala:585)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22$adapted(RuleExecutor.scala:585)\n\t\tat scala.collection.immutable.List.foreach(List.scala:431)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:585)\n\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:349)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:498)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:491)\n\t\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:397)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:491)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:416)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:341)\n\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:219)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:341)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:252)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:96)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:131)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:87)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:478)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:478)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:294)\n\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:548)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:668)\n\t\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:151)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:668)\n\t\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1307)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:661)\n\t\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:658)\n\t\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n\t\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:658)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:288)\n\t\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:287)\n\t\tat scala.util.Try$.apply(Try.scala:213)\n\t\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)\n\t\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n\t\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n\t\t... 24 more\n",
         "2025-08-13T14:34:00.416501Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "table/view",
         "`accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample`.`forecast`.`us_postal_daily_imperial`",
         "clone/create",
         "error",
         "Failed to process table/view `accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample`.`forecast`.`us_postal_daily_imperial`. Error: An error occurred while calling o427.sql.\n: com.databricks.sql.managedcatalog.acl.UnauthorizedAccessException: PERMISSION_DENIED: User does not have USE CATALOG on Catalog 'accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample'.\n\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:87)\n\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:63)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:251)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapExceptionBase(ManagedCatalogClientImpl.scala:7378)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:7364)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.getTableCredentials(ManagedCatalogClientImpl.scala:3951)\n\tat com.databricks.sql.managedcatalog.ManagedCatalogClient.getTemporaryCredentials(ManagedCatalogClient.scala:2620)\n\tat com.databricks.sql.managedcatalog.ManagedCatalogClient.getTemporaryCredentials$(ManagedCatalogClient.scala:2605)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.getTemporaryCredentials(ManagedCatalogClientImpl.scala:251)\n\tat com.databricks.unity.TempCredCache.$anonfun$getInternal$7(TemporaryCredentials.scala:448)\n\tat com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4724)\n\tat com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3522)\n\tat com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2315)\n\tat com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2278)\n\tat com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2193)\n\tat com.google.common.cache.LocalCache.get(LocalCache.java:3932)\n\tat com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4721)\n\tat com.databricks.unity.TempCredCache.liftedTree1$1(TemporaryCredentials.scala:447)\n\tat com.databricks.unity.TempCredCache.getInternal(TemporaryCredentials.scala:446)\n\tat com.databricks.unity.TempCredCache.get(TemporaryCredentials.scala:373)\n\tat com.databricks.unity.UnityCredentialManager.getTemporaryCredentials(CredentialManager.scala:488)\n\tat com.databricks.unity.CredentialManager$.getTemporaryCredentials(CredentialManager.scala:881)\n\tat com.databricks.unity.SAM.setConfFromCred$1(SAM.scala:338)\n\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$8(SAM.scala:353)\n\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$8$adapted(SAM.scala:352)\n\tat com.databricks.unity.TempCredRequest$.$anonfun$withFallbackOnOverlappingForParentPath$3(TemporaryCredentials.scala:147)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.databricks.unity.TempCredRequest$.withFallbackOnOverlappingForParentPath(TemporaryCredentials.scala:147)\n\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$7(SAM.scala:352)\n\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$7$adapted(SAM.scala:350)\n\tat scala.Option.map(Option.scala:230)\n\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$6(SAM.scala:350)\n\tat scala.Option.flatMap(Option.scala:271)\n\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$1(SAM.scala:349)\n\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$1$adapted(SAM.scala:336)\n\tat scala.Option.foreach(Option.scala:407)\n\tat com.databricks.unity.SAM.setFileSystemConfigs(SAM.scala:336)\n\tat com.databricks.unity.SAM.createDelegate(SAM.scala:388)\n\tat com.databricks.unity.SAM.createDelegate$(SAM.scala:372)\n\tat com.databricks.unity.TableSAM.createDelegate(SAM.scala:695)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.createDelegate(CredentialScopeFileSystem.scala:74)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.$anonfun$setDelegates$1(CredentialScopeFileSystem.scala:127)\n\tat com.databricks.sql.acl.fs.Lazy.apply(DelegatingFileSystem.scala:310)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.readDelegateMakeQualified(CredentialScopeFileSystem.scala:282)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.optimizedMakeQualifiedPath(CredentialScopeFileSystem.scala:277)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.makeQualified(CredentialScopeFileSystem.scala:294)\n\tat com.databricks.sql.transaction.tahoe.DeltaLog$.apply(DeltaLog.scala:1284)\n\tat com.databricks.sql.transaction.tahoe.DeltaLog$.forTable(DeltaLog.scala:1087)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$5(DeltaTableV2.scala:175)\n\tat com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$withAdditionalSnapshotInitializationUsageLogData$1(DeltaLog.scala:968)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n\tat com.databricks.sql.transaction.tahoe.DeltaLog$.withAdditionalSnapshotInitializationUsageLogData(DeltaLog.scala:969)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$2(DeltaTableV2.scala:169)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:700)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$1(DeltaTableV2.scala:150)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:83)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.deltaLog$lzycompute(DeltaTableV2.scala:150)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.deltaLog(DeltaTableV2.scala:148)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$8(DeltaTableV2.scala:264)\n\tat com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$withAdditionalSnapshotInitializationUsageLogData$1(DeltaLog.scala:968)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n\tat com.databricks.sql.transaction.tahoe.DeltaLog$.withAdditionalSnapshotInitializationUsageLogData(DeltaLog.scala:969)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$7(DeltaTableV2.scala:263)\n\tat scala.Option.orElse(Option.scala:447)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$5(DeltaTableV2.scala:263)\n\tat scala.Option.orElse(Option.scala:447)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$2(DeltaTableV2.scala:260)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:83)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$1(DeltaTableV2.scala:290)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:700)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot$lzycompute(DeltaTableV2.scala:240)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot(DeltaTableV2.scala:238)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$tableSchema$2(DeltaTableV2.scala:317)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.tableSchema$lzycompute(DeltaTableV2.scala:317)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.tableSchema(DeltaTableV2.scala:315)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.schema(DeltaTableV2.scala:322)\n\tat org.apache.spark.sql.connector.catalog.Table.columns(Table.java:68)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.columns(DeltaTableV2.scala:83)\n\tat org.apache.spark.sql.catalyst.analysis.ResolvedTable$.create(v2ResolutionPlans.scala:231)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$4(Analyzer.scala:1600)\n\tat scala.Option.map(Option.scala:230)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$3(Analyzer.scala:1592)\n\tat scala.Option.orElse(Option.scala:447)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableOrView(Analyzer.scala:1587)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1556)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1466)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:182)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:171)\n\tat scala.PartialFunction.applyOrElse(PartialFunction.scala:127)\n\tat scala.PartialFunction.applyOrElse$(PartialFunction.scala:126)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.applyOrElse(AnalysisHelper.scala:171)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:138)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1330)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1329)\n\tat org.apache.spark.sql.catalyst.plans.logical.DescribeRelation.mapChildren(v2Commands.scala:726)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning(AnalysisHelper.scala:186)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning$(AnalysisHelper.scala:167)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithSubqueriesAndPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.resolveOperatorsUpWithSubqueriesAndPruningByConf(Analyzer.scala:1674)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply0(Analyzer.scala:1466)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1419)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1365)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:480)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:629)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:613)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:131)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:480)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:479)\n\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:475)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:452)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22(RuleExecutor.scala:585)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22$adapted(RuleExecutor.scala:585)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:585)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:349)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:498)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:491)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:397)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:491)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:416)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:341)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:219)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:341)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:252)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:96)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:131)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:87)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:478)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:478)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:294)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:548)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:668)\n\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:151)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:668)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1307)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:661)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:658)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:658)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:288)\n\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:287)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:321)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:267)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$3(Dataset.scala:149)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n\tat org.apache.spark.sql.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1469)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1469)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:141)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:1048)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:1001)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:1071)\n\tat jdk.internal.reflect.GeneratedMethodAccessor593.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:197)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:117)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\n\t\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:87)\n\t\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:63)\n\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:251)\n\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapExceptionBase(ManagedCatalogClientImpl.scala:7378)\n\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:7364)\n\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.getTableCredentials(ManagedCatalogClientImpl.scala:3951)\n\t\tat com.databricks.sql.managedcatalog.ManagedCatalogClient.getTemporaryCredentials(ManagedCatalogClient.scala:2620)\n\t\tat com.databricks.sql.managedcatalog.ManagedCatalogClient.getTemporaryCredentials$(ManagedCatalogClient.scala:2605)\n\t\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.getTemporaryCredentials(ManagedCatalogClientImpl.scala:251)\n\t\tat com.databricks.unity.TempCredCache.$anonfun$getInternal$7(TemporaryCredentials.scala:448)\n\t\tat com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4724)\n\t\tat com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3522)\n\t\tat com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2315)\n\t\tat com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2278)\n\t\tat com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2193)\n\t\tat com.google.common.cache.LocalCache.get(LocalCache.java:3932)\n\t\tat com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4721)\n\t\tat com.databricks.unity.TempCredCache.liftedTree1$1(TemporaryCredentials.scala:447)\n\t\tat com.databricks.unity.TempCredCache.getInternal(TemporaryCredentials.scala:446)\n\t\tat com.databricks.unity.TempCredCache.get(TemporaryCredentials.scala:373)\n\t\tat com.databricks.unity.UnityCredentialManager.getTemporaryCredentials(CredentialManager.scala:488)\n\t\tat com.databricks.unity.CredentialManager$.getTemporaryCredentials(CredentialManager.scala:881)\n\t\tat com.databricks.unity.SAM.setConfFromCred$1(SAM.scala:338)\n\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$8(SAM.scala:353)\n\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$8$adapted(SAM.scala:352)\n\t\tat com.databricks.unity.TempCredRequest$.$anonfun$withFallbackOnOverlappingForParentPath$3(TemporaryCredentials.scala:147)\n\t\tat scala.util.Try$.apply(Try.scala:213)\n\t\tat com.databricks.unity.TempCredRequest$.withFallbackOnOverlappingForParentPath(TemporaryCredentials.scala:147)\n\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$7(SAM.scala:352)\n\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$7$adapted(SAM.scala:350)\n\t\tat scala.Option.map(Option.scala:230)\n\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$6(SAM.scala:350)\n\t\tat scala.Option.flatMap(Option.scala:271)\n\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$1(SAM.scala:349)\n\t\tat com.databricks.unity.SAM.$anonfun$setFileSystemConfigs$1$adapted(SAM.scala:336)\n\t\tat scala.Option.foreach(Option.scala:407)\n\t\tat com.databricks.unity.SAM.setFileSystemConfigs(SAM.scala:336)\n\t\tat com.databricks.unity.SAM.createDelegate(SAM.scala:388)\n\t\tat com.databricks.unity.SAM.createDelegate$(SAM.scala:372)\n\t\tat com.databricks.unity.TableSAM.createDelegate(SAM.scala:695)\n\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.createDelegate(CredentialScopeFileSystem.scala:74)\n\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.$anonfun$setDelegates$1(CredentialScopeFileSystem.scala:127)\n\t\tat com.databricks.sql.acl.fs.Lazy.apply(DelegatingFileSystem.scala:310)\n\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.readDelegateMakeQualified(CredentialScopeFileSystem.scala:282)\n\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.optimizedMakeQualifiedPath(CredentialScopeFileSystem.scala:277)\n\t\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.makeQualified(CredentialScopeFileSystem.scala:294)\n\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.apply(DeltaLog.scala:1284)\n\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.forTable(DeltaLog.scala:1087)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$5(DeltaTableV2.scala:175)\n\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$withAdditionalSnapshotInitializationUsageLogData$1(DeltaLog.scala:968)\n\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.withAdditionalSnapshotInitializationUsageLogData(DeltaLog.scala:969)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$2(DeltaTableV2.scala:169)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:700)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$deltaLog$1(DeltaTableV2.scala:150)\n\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\t\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n\t\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:83)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.deltaLog$lzycompute(DeltaTableV2.scala:150)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.deltaLog(DeltaTableV2.scala:148)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$8(DeltaTableV2.scala:264)\n\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.$anonfun$withAdditionalSnapshotInitializationUsageLogData$1(DeltaLog.scala:968)\n\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n\t\tat com.databricks.sql.transaction.tahoe.DeltaLog$.withAdditionalSnapshotInitializationUsageLogData(DeltaLog.scala:969)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$7(DeltaTableV2.scala:263)\n\t\tat scala.Option.orElse(Option.scala:447)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$5(DeltaTableV2.scala:263)\n\t\tat scala.Option.orElse(Option.scala:447)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$2(DeltaTableV2.scala:260)\n\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\t\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n\t\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:83)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$1(DeltaTableV2.scala:290)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:700)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot$lzycompute(DeltaTableV2.scala:240)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot(DeltaTableV2.scala:238)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$tableSchema$2(DeltaTableV2.scala:317)\n\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.tableSchema$lzycompute(DeltaTableV2.scala:317)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.tableSchema(DeltaTableV2.scala:315)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.schema(DeltaTableV2.scala:322)\n\t\tat org.apache.spark.sql.connector.catalog.Table.columns(Table.java:68)\n\t\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.columns(DeltaTableV2.scala:83)\n\t\tat org.apache.spark.sql.catalyst.analysis.ResolvedTable$.create(v2ResolutionPlans.scala:231)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$4(Analyzer.scala:1600)\n\t\tat scala.Option.map(Option.scala:230)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$3(Analyzer.scala:1592)\n\t\tat scala.Option.orElse(Option.scala:447)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableOrView(Analyzer.scala:1587)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1556)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1466)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:182)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:171)\n\t\tat scala.PartialFunction.applyOrElse(PartialFunction.scala:127)\n\t\tat scala.PartialFunction.applyOrElse$(PartialFunction.scala:126)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.applyOrElse(AnalysisHelper.scala:171)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)\n\t\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:138)\n\t\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1330)\n\t\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1329)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.DescribeRelation.mapChildren(v2Commands.scala:726)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning(AnalysisHelper.scala:186)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning$(AnalysisHelper.scala:167)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithSubqueriesAndPruning(LogicalPlan.scala:42)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.resolveOperatorsUpWithSubqueriesAndPruningByConf(Analyzer.scala:1674)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply0(Analyzer.scala:1466)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1419)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1365)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:480)\n\t\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:629)\n\t\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:613)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:131)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:480)\n\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:479)\n\t\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n\t\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n\t\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:475)\n\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:452)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22(RuleExecutor.scala:585)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22$adapted(RuleExecutor.scala:585)\n\t\tat scala.collection.immutable.List.foreach(List.scala:431)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:585)\n\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:349)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:498)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:491)\n\t\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:397)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:491)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:416)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:341)\n\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:219)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:341)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:252)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:96)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:131)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:87)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:478)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:478)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:294)\n\t\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:548)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:668)\n\t\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:151)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:668)\n\t\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1307)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:661)\n\t\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:658)\n\t\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n\t\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:658)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:288)\n\t\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:287)\n\t\tat scala.util.Try$.apply(Try.scala:213)\n\t\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)\n\t\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n\t\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n\t\t... 24 more\n",
         "2025-08-13T14:34:00.30046Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "table",
         "`dr_eo_amperity`.`silver`.`unified_paid_media`",
         "clone",
         "error",
         "Failed to clone table `dr_eo_amperity`.`silver`.`unified_paid_media`.. Error: [DELTA_CLONE_UNSUPPORTED_SOURCE] Unsupported DEEP clone source '_eo_amperity.silver.unified_paid_media', whose format is deltasharing.\nThe supported formats are 'delta', 'iceberg' and 'parquet'.",
         "2025-08-13T14:33:56.226377Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "table",
         "`dr_eo_amperity`.`silver`.`unified_scores`",
         "clone",
         "error",
         "Failed to clone table `dr_eo_amperity`.`silver`.`unified_scores`.. Error: [DELTA_CLONE_UNSUPPORTED_SOURCE] Unsupported DEEP clone source '_eo_amperity.silver.unified_scores', whose format is deltasharing.\nThe supported formats are 'delta', 'iceberg' and 'parquet'.",
         "2025-08-13T14:33:56.137999Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "table",
         "`dr_eo_amperity`.`silver`.`unified_merged`",
         "clone",
         "error",
         "Failed to clone table `dr_eo_amperity`.`silver`.`unified_merged`.. Error: [DELTA_CLONE_UNSUPPORTED_SOURCE] Unsupported DEEP clone source '_eo_amperity.silver.unified_merged', whose format is deltasharing.\nThe supported formats are 'delta', 'iceberg' and 'parquet'.",
         "2025-08-13T14:33:55.785192Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "table",
         "`dr_eo_amperity`.`silver`.`unified_loyalty_events`",
         "clone",
         "error",
         "Failed to clone table `dr_eo_amperity`.`silver`.`unified_loyalty_events`.. Error: [DELTA_CLONE_UNSUPPORTED_SOURCE] Unsupported DEEP clone source '_eo_amperity.silver.unified_loyalty_events', whose format is deltasharing.\nThe supported formats are 'delta', 'iceberg' and 'parquet'.",
         "2025-08-13T14:33:51.634003Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "table",
         "`dr_eo_amperity`.`silver`.`unified_loyalty`",
         "clone",
         "error",
         "Failed to clone table `dr_eo_amperity`.`silver`.`unified_loyalty`.. Error: [DELTA_CLONE_UNSUPPORTED_SOURCE] Unsupported DEEP clone source '_eo_amperity.silver.unified_loyalty', whose format is deltasharing.\nThe supported formats are 'delta', 'iceberg' and 'parquet'.",
         "2025-08-13T14:33:51.346011Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "table",
         "`dr_eo_amperity`.`silver`.`unified_coalesced`",
         "clone",
         "success",
         "Table `dr_eo_amperity`.`silver`.`unified_coalesced` cloned successfully.",
         "2025-08-13T14:33:51.172862Z",
         [
          [
           "source_table_size",
           "25008603680"
          ],
          [
           "source_num_of_files",
           "234"
          ],
          [
           "num_of_synced_transactions",
           "None"
          ],
          [
           "num_removed_files",
           "0"
          ],
          [
           "num_copied_files",
           "234"
          ],
          [
           "removed_files_size",
           "0"
          ],
          [
           "copied_files_size",
           "25008603680"
          ]
         ]
        ],
        [
         "DR_Clone_data_v2",
         "table",
         "`dr_eo_amperity`.`silver`.`unified_itemized_transactions`",
         "clone",
         "error",
         "Failed to clone table `dr_eo_amperity`.`silver`.`unified_itemized_transactions`.. Error: [DELTA_CLONE_UNSUPPORTED_SOURCE] Unsupported DEEP clone source '_eo_amperity.silver.unified_itemized_transactions', whose format is deltasharing.\nThe supported formats are 'delta', 'iceberg' and 'parquet'.",
         "2025-08-13T14:28:34.785531Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "table",
         "`dr_eo_amperity`.`silver`.`unified_customer`",
         "clone",
         "error",
         "Failed to clone table `dr_eo_amperity`.`silver`.`unified_customer`.. Error: [DELTA_CLONE_UNSUPPORTED_SOURCE] Unsupported DEEP clone source '_eo_amperity.silver.unified_customer', whose format is deltasharing.\nThe supported formats are 'delta', 'iceberg' and 'parquet'.",
         "2025-08-13T14:28:34.54264Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "table",
         "`dr_eo_amperity`.`silver`.`unified_household`",
         "clone",
         "error",
         "Failed to clone table `dr_eo_amperity`.`silver`.`unified_household`.. Error: [DELTA_CLONE_UNSUPPORTED_SOURCE] Unsupported DEEP clone source '_eo_amperity.silver.unified_household', whose format is deltasharing.\nThe supported formats are 'delta', 'iceberg' and 'parquet'.",
         "2025-08-13T14:28:34.530074Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "table",
         "`dr_eo_amperity`.`silver`.`unified_compliance`",
         "clone",
         "error",
         "Failed to clone table `dr_eo_amperity`.`silver`.`unified_compliance`.. Error: [DELTA_CLONE_UNSUPPORTED_SOURCE] Unsupported DEEP clone source '_eo_amperity.silver.unified_compliance', whose format is deltasharing.\nThe supported formats are 'delta', 'iceberg' and 'parquet'.",
         "2025-08-13T14:28:34.33462Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "table",
         "`dr_eo_amperity`.`silver`.`fiscal_calendar`",
         "clone",
         "error",
         "Failed to clone table `dr_eo_amperity`.`silver`.`fiscal_calendar`.. Error: [DELTA_CLONE_UNSUPPORTED_SOURCE] Unsupported DEEP clone source '_eo_amperity.silver.fiscal_calendar', whose format is deltasharing.\nThe supported formats are 'delta', 'iceberg' and 'parquet'.",
         "2025-08-13T14:28:29.570012Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "table",
         "`dr_eo_amperity`.`default`.`unified_transactions_reporting`",
         "clone",
         "error",
         "Failed to clone table `dr_eo_amperity`.`default`.`unified_transactions_reporting`.. Error: [DELTA_CLONE_UNSUPPORTED_SOURCE] Unsupported DEEP clone source '_eo_amperity.default.unified_transactions_reporting', whose format is deltasharing.\nThe supported formats are 'delta', 'iceberg' and 'parquet'.",
         "2025-08-13T14:28:29.515228Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "table",
         "`dr_eo_amperity`.`default`.`transaction_attributes_extended`",
         "clone",
         "error",
         "Failed to clone table `dr_eo_amperity`.`default`.`transaction_attributes_extended`.. Error: [DELTA_CLONE_UNSUPPORTED_SOURCE] Unsupported DEEP clone source '_eo_amperity.default.transaction_attributes_extended', whose format is deltasharing.\nThe supported formats are 'delta', 'iceberg' and 'parquet'.",
         "2025-08-13T14:28:29.444302Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "table",
         "`dr_eo_amperity`.`default`.`transaction_attributes`",
         "clone",
         "error",
         "Failed to clone table `dr_eo_amperity`.`default`.`transaction_attributes`.. Error: [DELTA_CLONE_UNSUPPORTED_SOURCE] Unsupported DEEP clone source '_eo_amperity.default.transaction_attributes', whose format is deltasharing.\nThe supported formats are 'delta', 'iceberg' and 'parquet'.",
         "2025-08-13T14:28:29.150441Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "table",
         "`dr_eo_amperity`.`default`.`predicted_clv_attributes`",
         "clone",
         "error",
         "Failed to clone table `dr_eo_amperity`.`default`.`predicted_clv_attributes`.. Error: [DELTA_CLONE_UNSUPPORTED_SOURCE] Unsupported DEEP clone source '_eo_amperity.default.predicted_clv_attributes', whose format is deltasharing.\nThe supported formats are 'delta', 'iceberg' and 'parquet'.",
         "2025-08-13T14:28:29.007067Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "table",
         "`dr_eo_amperity`.`bronze`.`web_visitors`",
         "clone",
         "success",
         "Table `dr_eo_amperity`.`bronze`.`web_visitors` cloned successfully.",
         "2025-08-13T14:28:24.794847Z",
         [
          [
           "source_table_size",
           "334349516"
          ],
          [
           "source_num_of_files",
           "7"
          ],
          [
           "num_of_synced_transactions",
           "None"
          ],
          [
           "num_removed_files",
           "0"
          ],
          [
           "num_copied_files",
           "7"
          ],
          [
           "removed_files_size",
           "0"
          ],
          [
           "copied_files_size",
           "334349516"
          ]
         ]
        ],
        [
         "DR_Clone_data_v2",
         "table",
         "`dr_eo_amperity`.`bronze`.`wifi_connections`",
         "clone",
         "success",
         "Table `dr_eo_amperity`.`bronze`.`wifi_connections` cloned successfully.",
         "2025-08-13T14:28:24.790242Z",
         [
          [
           "source_table_size",
           "7882742359"
          ],
          [
           "source_num_of_files",
           "52"
          ],
          [
           "num_of_synced_transactions",
           "None"
          ],
          [
           "num_removed_files",
           "0"
          ],
          [
           "num_copied_files",
           "52"
          ],
          [
           "removed_files_size",
           "0"
          ],
          [
           "copied_files_size",
           "7882742359"
          ]
         ]
        ],
        [
         "DR_Clone_data_v2",
         "table",
         "`dr_eo_amperity`.`default`.`predicted_affinity`",
         "clone",
         "error",
         "Failed to clone table `dr_eo_amperity`.`default`.`predicted_affinity`.. Error: [DELTA_CLONE_UNSUPPORTED_SOURCE] Unsupported DEEP clone source '_eo_amperity.default.predicted_affinity', whose format is deltasharing.\nThe supported formats are 'delta', 'iceberg' and 'parquet'.",
         "2025-08-13T14:28:23.84342Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "table",
         "`dr_eo_amperity`.`default`.`email_engagement_summary`",
         "clone",
         "error",
         "Failed to clone table `dr_eo_amperity`.`default`.`email_engagement_summary`.. Error: [DELTA_CLONE_UNSUPPORTED_SOURCE] Unsupported DEEP clone source '_eo_amperity.default.email_engagement_summary', whose format is deltasharing.\nThe supported formats are 'delta', 'iceberg' and 'parquet'.",
         "2025-08-13T14:28:23.765253Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "table",
         "`dr_eo_amperity`.`default`.`email_engagement_attributes`",
         "clone",
         "error",
         "Failed to clone table `dr_eo_amperity`.`default`.`email_engagement_attributes`.. Error: [DELTA_CLONE_UNSUPPORTED_SOURCE] Unsupported DEEP clone source '_eo_amperity.default.email_engagement_attributes', whose format is deltasharing.\nThe supported formats are 'delta', 'iceberg' and 'parquet'.",
         "2025-08-13T14:28:23.595463Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "table",
         "`dr_eo_amperity`.`default`.`customer_attributes`",
         "clone",
         "error",
         "Failed to clone table `dr_eo_amperity`.`default`.`customer_attributes`.. Error: [DELTA_CLONE_UNSUPPORTED_SOURCE] Unsupported DEEP clone source '_eo_amperity.default.customer_attributes', whose format is deltasharing.\nThe supported formats are 'delta', 'iceberg' and 'parquet'.",
         "2025-08-13T14:26:14.538529Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "table",
         "`dr_eo_amperity`.`default`.`customer_360`",
         "clone",
         "error",
         "Failed to clone table `dr_eo_amperity`.`default`.`customer_360`.. Error: [DELTA_CLONE_UNSUPPORTED_SOURCE] Unsupported DEEP clone source '_eo_amperity.default.customer_360', whose format is deltasharing.\nThe supported formats are 'delta', 'iceberg' and 'parquet'.",
         "2025-08-13T14:26:14.526599Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "table",
         "`dr_eo_amperity`.`default`.`analytics_tae`",
         "clone",
         "error",
         "Failed to clone table `dr_eo_amperity`.`default`.`analytics_tae`.. Error: [DELTA_CLONE_UNSUPPORTED_SOURCE] Unsupported DEEP clone source '_eo_amperity.default.analytics_tae', whose format is deltasharing.\nThe supported formats are 'delta', 'iceberg' and 'parquet'.",
         "2025-08-13T14:26:14.104877Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "table",
         "`dr_eo_amperity`.`bronze`.`e_commerce_customers`",
         "clone",
         "error",
         "Failed to clone table `dr_eo_amperity`.`bronze`.`e_commerce_customers`.. Error: [DELTA_CLONE_UNSUPPORTED_SOURCE] Unsupported DEEP clone source '_eo_amperity.bronze.e_commerce_customers', whose format is deltasharing.\nThe supported formats are 'delta', 'iceberg' and 'parquet'.",
         "2025-08-13T14:25:56.750087Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "table",
         "`dr_eo_amperity`.`bronze`.`pos_customers`",
         "clone",
         "error",
         "Failed to clone table `dr_eo_amperity`.`bronze`.`pos_customers`.. Error: [DELTA_CLONE_UNSUPPORTED_SOURCE] Unsupported DEEP clone source '_eo_amperity.bronze.pos_customers', whose format is deltasharing.\nThe supported formats are 'delta', 'iceberg' and 'parquet'.",
         "2025-08-13T14:25:56.641197Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "table",
         "`dr_eo_amperity`.`bronze`.`loyalty_members`",
         "clone",
         "error",
         "Failed to clone table `dr_eo_amperity`.`bronze`.`loyalty_members`.. Error: [DELTA_CLONE_UNSUPPORTED_SOURCE] Unsupported DEEP clone source '_eo_amperity.bronze.loyalty_members', whose format is deltasharing.\nThe supported formats are 'delta', 'iceberg' and 'parquet'.",
         "2025-08-13T14:25:56.641138Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "catalog_pair",
         "satya_share -> satya_dr_catalog",
         "process",
         "sequential_complete",
         "Finished sequential processing for catalog pair: satya_share -> satya_dr_catalog",
         "2025-08-13T14:25:53.439758Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "schema",
         "`satya_dr_catalog`.`satyendranath_sure`",
         "change_ownership",
         "skipped",
         "No ownership mapping found for this schema. Skipping ownership change.",
         "2025-08-13T14:25:51.58873Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "schema",
         "`satya_dr_catalog`.`satyendranath_sure`",
         "create",
         "success",
         "Schema `satya_dr_catalog`.`satyendranath_sure` created or already exists.",
         "2025-08-13T14:25:50.008498Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "schema",
         "`satya_share`.`information_schema`",
         "skip",
         "success",
         "Skipping `information_schema`.",
         "2025-08-13T14:25:48.101563Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "catalog",
         "`satya_dr_catalog`",
         "create",
         "success",
         "Catalog `satya_dr_catalog` created or already exists.",
         "2025-08-13T14:25:45.08812Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "catalog_pair",
         "satya_share -> satya_dr_catalog",
         "process",
         "start",
         "Starting processing for catalog pair: satya_share -> satya_dr_catalog",
         "2025-08-13T14:25:42.396262Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "catalog",
         "`test_not`",
         "get_schemas",
         "error",
         "Failed to get schemas from source catalog `test_not`. Error: [NO_SUCH_CATALOG_EXCEPTION] Catalog 'test_not' was not found. Please verify the catalog name and then retry the query or command again. SQLSTATE: 42704",
         "2025-08-13T14:25:40.674286Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "catalog",
         "`dr_test_not`",
         "create",
         "success",
         "Catalog `dr_test_not` created or already exists.",
         "2025-08-13T14:25:38.514428Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "catalog_pair",
         "test_not -> dr_test_not",
         "process",
         "start",
         "Starting processing for catalog pair: test_not -> dr_test_not",
         "2025-08-13T14:25:35.74414Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "catalog_pair",
         "accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample -> dr_accuweather",
         "process",
         "sequential_complete",
         "Finished sequential processing for catalog pair: accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample -> dr_accuweather",
         "2025-08-13T14:25:33.833988Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "schema",
         "`accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample`.`information_schema`",
         "skip",
         "success",
         "Skipping `information_schema`.",
         "2025-08-13T14:25:32.000314Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "schema",
         "`dr_accuweather`.`forecast`",
         "change_ownership",
         "skipped",
         "No ownership mapping found for this schema. Skipping ownership change.",
         "2025-08-13T14:25:29.949426Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "schema",
         "`dr_accuweather`.`forecast`",
         "create",
         "success",
         "Schema `dr_accuweather`.`forecast` created or already exists.",
         "2025-08-13T14:25:28.070324Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "catalog",
         "`dr_accuweather`",
         "create",
         "success",
         "Catalog `dr_accuweather` created or already exists.",
         "2025-08-13T14:25:24.39967Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "catalog_pair",
         "accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample -> dr_accuweather",
         "process",
         "start",
         "Starting processing for catalog pair: accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample -> dr_accuweather",
         "2025-08-13T14:25:21.774662Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "catalog_pair",
         "_eo_amperity -> dr_eo_amperity",
         "process",
         "sequential_complete",
         "Finished sequential processing for catalog pair: _eo_amperity -> dr_eo_amperity",
         "2025-08-13T14:25:19.87602Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "schema",
         "`dr_eo_amperity`.`silver`",
         "change_ownership",
         "success",
         "Ownership of schema `dr_eo_amperity`.`silver` changed to `grp_test_clt_usa`.",
         "2025-08-13T14:25:17.099907Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "schema",
         "`dr_eo_amperity`.`silver`",
         "create",
         "success",
         "Schema `dr_eo_amperity`.`silver` created or already exists.",
         "2025-08-13T14:25:14.533455Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "schema",
         "`_eo_amperity`.`information_schema`",
         "skip",
         "success",
         "Skipping `information_schema`.",
         "2025-08-13T14:25:11.844947Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "schema",
         "`dr_eo_amperity`.`default`",
         "change_ownership",
         "success",
         "Ownership of schema `dr_eo_amperity`.`default` changed to `grp_test_clt_usa`.",
         "2025-08-13T14:25:09.542687Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "schema",
         "`dr_eo_amperity`.`default`",
         "create",
         "success",
         "Schema `dr_eo_amperity`.`default` created or already exists.",
         "2025-08-13T14:25:07.045578Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "schema",
         "`dr_eo_amperity`.`bronze`",
         "change_ownership",
         "success",
         "Ownership of schema `dr_eo_amperity`.`bronze` changed to `grp_test_clt_usa`.",
         "2025-08-13T14:25:04.35033Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "schema",
         "`dr_eo_amperity`.`bronze`",
         "create",
         "success",
         "Schema `dr_eo_amperity`.`bronze` created or already exists.",
         "2025-08-13T14:25:01.683511Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "catalog",
         "`dr_eo_amperity`",
         "create",
         "success",
         "Catalog `dr_eo_amperity` created or already exists.",
         "2025-08-13T14:24:58.068969Z",
         []
        ],
        [
         "DR_Clone_data_v2",
         "catalog_pair",
         "_eo_amperity -> dr_eo_amperity",
         "process",
         "start",
         "Starting processing for catalog pair: _eo_amperity -> dr_eo_amperity",
         "2025-08-13T14:24:47.473232Z",
         []
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "notebook_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "entity_type",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "entity_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "action",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "status",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "message",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "timestamp",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{}",
         "name": "results_data",
         "type": "{\"type\":\"array\",\"elementType\":{\"type\":\"struct\",\"fields\":[{\"name\":\"key\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"value\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]},\"containsNull\":true}"
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display logs for the last 24 hours\n",
    "print(f\"\\n--- Displaying logs from {log_table_name} for the last 24 hours ---\")\n",
    "# past_24_hours = datetime.now() - timedelta(hours=24)\n",
    "log_query_df = spark.sql(f\"\"\"\n",
    "    SELECT *\n",
    "    FROM {log_table_name}\n",
    "    ORDER BY timestamp DESC\n",
    "\"\"\")\n",
    "\n",
    "display(log_query_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c47613d-0b98-4928-ac8c-dc0f4a44054a",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1754946235548}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {
        "catalog_names": "_eo_amperity:dr_eo_amperity, accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample:dr_accuweather, test_not:dr_test_not, satya_share:satya_dr_catalog",
        "log_table_name": "users.satyendranath_sure.dr_log_table_name",
        "max_workers": "5",
        "target_schema_owner": "dr_eo_amperity.bronze:grp_test_clt_usa, dr_eo_amperity.silver:grp_test_clt_usa, dr_eo_amperity.default:grp_test_clt_usa"
       },
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %sql\n",
    "# drop table \n",
    "# -- select * from \n",
    "# -- users.satyendranath_sure.dr_log_table_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0c1e6bd-1c5a-4213-a6b3-3ceb7cb46cc8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d40b06da-3b23-412b-a7c5-e1356fb0336e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68de7672-75ff-4d70-b73a-43f3fd2173e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90aa59aa-7073-4fe7-9fed-47c6bc3318ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97ebd6ef-8e17-4a9f-bccf-f86aab80bb3e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9de1c82-4ed0-4f29-9965-81a2ea4dc5cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "dbe_41a50460-c90b-4840-9288-afcb847395d5",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8748648698906789,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "DR_Volume_data",
   "widgets": {
    "catalog_names": {
     "currentValue": "_eo_amperity:dr_eo_amperity, accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample:dr_accuweather, test_not:dr_test_not, satya_share:satya_dr_catalog",
     "nuid": "5f06e975-cdb8-43fe-a80b-4c55190ca561",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "catalog_names",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "catalog_names",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "log_table_name": {
     "currentValue": "users.satyendranath_sure.dr_log_table_name",
     "nuid": "6bec95bd-2265-43c8-b89b-7602c26ce306",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "users.satyendranath_sure.dr_log_table_name",
      "label": null,
      "name": "log_table_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "users.satyendranath_sure.dr_log_table_name",
      "label": null,
      "name": "log_table_name",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "max_workers": {
     "currentValue": "5",
     "nuid": "6d8539ad-a4d2-4f54-bf5c-9b7689d1d058",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "5",
      "label": null,
      "name": "max_workers",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "5",
      "label": null,
      "name": "max_workers",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "target_schema_owner": {
     "currentValue": "dr_eo_amperity.bronze:grp_test_clt_usa, dr_eo_amperity.silver:grp_test_clt_usa, dr_eo_amperity.default:grp_test_clt_usa",
     "nuid": "af424e30-5de1-47ac-8079-3fea6dfda1df",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "target_schema_owner",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "target_schema_owner",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}