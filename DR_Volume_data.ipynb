{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad0f10f1-c82e-4c1c-b1ad-bac2366c9e23",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Databricks Volume Disaster Recovery (DR) Sync Script\n",
    "\n",
    "## Overview\n",
    "This notebook provides an automated disaster recovery solution for synchronizing Databricks Unity Catalog volumes between source and target catalogs. The script performs efficient, incremental file synchronization with comprehensive logging and parallel processing capabilities.\n",
    "\n",
    "## Key Features\n",
    "- **Incremental Sync**: Only copies new or modified files based on modification timestamps\n",
    "- **Parallel Processing**: Uses ThreadPoolExecutor for concurrent volume synchronization\n",
    "- **Comprehensive Logging**: Tracks all operations with detailed status and metadata in a log table\n",
    "- **Recursive Directory Support**: Handles nested directory structures automatically\n",
    "- **Error Handling**: Robust exception handling with detailed error logging\n",
    "- **Unity Catalog Integration**: Creates target catalogs, schemas, and volumes as needed\n",
    "\n",
    "## Configuration Parameters\n",
    "\n",
    "| Parameter | Description | Example |\n",
    "|-----------|-------------|---------|\n",
    "| `catalog_names` | Comma-separated source:target catalog pairs | `source_cat:target_cat,cat2:dr_cat2` |\n",
    "| `target_schema_owner` | Schema ownership assignments for target catalogs | `target_cat.schema:owner_group` |\n",
    "| `max_workers` | Maximum number of concurrent threads for parallel processing | `5` |\n",
    "| `log_table_name` | Full name of the logging table to track operations | `users.admin.dr_log_table` |\n",
    "\n",
    "## How It Works\n",
    "\n",
    "1. **Initialization**: Creates log table and parses configuration parameters\n",
    "2. **Catalog Processing**: For each source:target catalog pair:\n",
    "   - Creates target catalog if it doesn't exist\n",
    "   - Iterates through all schemas in the source catalog\n",
    "3. **Schema Processing**: For each schema:\n",
    "   - Creates corresponding target schema\n",
    "   - Identifies all volumes within the schema\n",
    "4. **Volume Synchronization**: For each volume:\n",
    "   - Creates target volume if needed\n",
    "   - Retrieves previous run information for incremental sync\n",
    "   - Recursively copies only new/modified files\n",
    "   - Logs all operations with timestamps and metadata\n",
    "\n",
    "## Prerequisites\n",
    "- Unity Catalog enabled workspace\n",
    "- Appropriate permissions on source and target catalogs\n",
    "- Access to create and write to the specified log table\n",
    "- Databricks Runtime 13.3 LTS or above\n",
    "\n",
    "## Usage Notes\n",
    "- **First Run**: Copies all files from source to target volumes\n",
    "- **Subsequent Runs**: Only copies files that are new or have been modified\n",
    "- **Logging**: All operations are logged for auditing and troubleshooting\n",
    "- **Error Recovery**: Failed operations are logged but don't stop the entire sync process\n",
    "\n",
    "## Important Considerations\n",
    "- Ensure sufficient cluster resources for large volume synchronizations\n",
    "- Monitor the log table for any failed operations\n",
    "- Test with small volumes before running on production data\n",
    "- Consider network bandwidth and storage costs for large data transfers\n",
    "\n",
    "---\n",
    "**Created**: September 2025  \n",
    "**Version**: 1.0  \n",
    "**Runtime Compatibility**: Databricks Runtime 13.3 LTS+\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93ee5629-626d-4ad6-adb4-3a7605c391fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# dbutils.widgets.removeAll()\n",
    "dbutils.widgets.text(\"catalog_names\", \"\")#source and target catalog names like dictionary\n",
    "dbutils.widgets.text(\"target_schema_owner\", \"\")\n",
    "dbutils.widgets.text(\"max_workers\", \"5\")\n",
    "dbutils.widgets.text(\"log_table_name\", \"users.satyendranath_sure.dr_log_table_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff63f959-a4f1-46a1-ad76-524b7ef329fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "catalog_pair_names_list = [catalog_pair.strip() for catalog_pair in dbutils.widgets.get(\"catalog_names\").split(\",\")]\n",
    "target_schema_owner_list = [catalog_schema_pair.strip() for catalog_schema_pair in dbutils.widgets.get(\"target_schema_owner\").split(\",\")]\n",
    "max_workers = int(dbutils.widgets.get(\"max_workers\"))\n",
    "log_table_name = dbutils.widgets.get(\"log_table_name\")\n",
    "print(catalog_pair_names_list, target_schema_owner_list, max_workers, log_table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "946565cb-9b3c-4b5d-ad9d-f7274b949ca2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# for catalog_pair in catalog_pair_names_list:\n",
    "#     second_value = catalog_pair.split(\":\")[1]\n",
    "#     print(second_value)\n",
    "#     spark.sql(f\"DROP CATALOG IF EXISTS `{second_value}` CASCADE\")\n",
    "# spark.sql(f\"DROP table if exists `{log_table_name}`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe5b881f-feae-477f-a041-0e9f2aae4166",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "working - not deletes"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import traceback\n",
    "import concurrent.futures\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType, ArrayType\n",
    "from datetime import datetime\n",
    "import json\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "# The 'spark' and 'dbutils' objects are pre-defined in a Databricks environment.\n",
    "\n",
    "# Get the current notebook name\n",
    "notebook_path = dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get()\n",
    "notebook_name = notebook_path.split('/')[-1]\n",
    "\n",
    "# Schema definition for log table\n",
    "log_table_schema = StructType([\n",
    "    StructField(\"notebook_name\", StringType(), True),\n",
    "    StructField(\"entity_type\", StringType(), True),\n",
    "    StructField(\"entity_name\", StringType(), True),\n",
    "    StructField(\"action\", StringType(), True),\n",
    "    StructField(\"status\", StringType(), True),\n",
    "    StructField(\"message\", StringType(), True),\n",
    "    StructField(\"timestamp\", TimestampType(), True),\n",
    "    StructField(\"results_data\", ArrayType(StructType([\n",
    "        StructField(\"key\", StringType(), True),\n",
    "        StructField(\"value\", StringType(), True)\n",
    "    ])), True)\n",
    "])\n",
    "\n",
    "# Create log table\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {log_table_name}\n",
    "    (\n",
    "        notebook_name STRING,\n",
    "        entity_type STRING,\n",
    "        entity_name STRING,\n",
    "        action STRING,\n",
    "        status STRING,\n",
    "        message STRING,\n",
    "        timestamp TIMESTAMP,\n",
    "        results_data ARRAY<STRUCT<key: STRING, value: STRING>>\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "def insert_log_entry(log_data):\n",
    "    \"\"\"\n",
    "    Inserts a log entry into the designated log table.\n",
    "    \n",
    "    Args:\n",
    "        log_data (dict): A dictionary containing log information\n",
    "    \"\"\"\n",
    "    try:\n",
    "        log_data[\"notebook_name\"] = notebook_name\n",
    "        # Ensure results_data is a list of dictionaries as per schema\n",
    "        if \"results_data\" not in log_data or not isinstance(log_data[\"results_data\"], list):\n",
    "            log_data[\"results_data\"] = []\n",
    "        \n",
    "        df = spark.createDataFrame([log_data], schema=log_table_schema)\n",
    "        df.write.mode(\"append\").saveAsTable(log_table_name)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to insert log entry: {log_data}. Error: {e}\")\n",
    "\n",
    "def get_previous_run_info(source_path):\n",
    "    \"\"\"\n",
    "    Retrieves the modification times of files from the last successful run for a given source path.\n",
    "    \n",
    "    This is used to identify which files are new or have been modified since the last copy.\n",
    "    \n",
    "    Args:\n",
    "        source_path (str): The source path (directory) to check for previously copied files.\n",
    "    \n",
    "    Returns:\n",
    "        dict: A dictionary mapping absolute file paths to their last successful modification timestamp.\n",
    "    \"\"\"\n",
    "    spark_session = SparkSession.builder.getOrCreate()\n",
    "    try:\n",
    "        df = spark_session.sql(f\"\"\"\n",
    "            SELECT\n",
    "                entity_name,\n",
    "                results_data\n",
    "            FROM {log_table_name}\n",
    "            WHERE\n",
    "                entity_type = 'file' AND\n",
    "                action = 'copy' AND\n",
    "                status = 'success'\n",
    "            ORDER BY timestamp DESC\n",
    "        \"\"\")\n",
    "        \n",
    "        previous_runs = {}\n",
    "        for row in df.collect():\n",
    "            try:\n",
    "                file_path = row['entity_name']\n",
    "                results_data = row['results_data']\n",
    "                mod_time = None\n",
    "                for item in results_data:\n",
    "                    if item.key == 'modificationTime':\n",
    "                        mod_time = int(item.value)\n",
    "                        break\n",
    "                \n",
    "                if mod_time:\n",
    "                    # Use absolute file path instead of relative path\n",
    "                    if file_path not in previous_runs:\n",
    "                        previous_runs[file_path] = mod_time\n",
    "            except Exception as e:\n",
    "                print(f\"Error parsing log entry: {e}\")\n",
    "                continue\n",
    "        return previous_runs\n",
    "    except Exception as e:\n",
    "        log_entry = {\n",
    "            \"entity_type\": \"info\",\n",
    "            \"entity_name\": source_path,\n",
    "            \"action\": \"get_previous_run_info\",\n",
    "            \"status\": \"error\",\n",
    "            \"message\": f\"Error fetching previous run info. Error: {e}\",\n",
    "            \"timestamp\": datetime.now(),\n",
    "            \"results_data\": []\n",
    "        }\n",
    "        insert_log_entry(log_entry)\n",
    "        return {}\n",
    "\n",
    "\n",
    "def sync_volume_recursively(source_path, target_path, prev_run_info):\n",
    "    \"\"\"\n",
    "    Recursively copies files from a source path to a target path,\n",
    "    copying only new or modified files.\n",
    "    \n",
    "    Args:\n",
    "        source_path (str): The full source path (dbfs:/Volumes/...) to sync from.\n",
    "        target_path (str): The full target path (dbfs:/Volumes/...) to sync to.\n",
    "        prev_run_info (dict): A dictionary of previously copied files and their modification times.\n",
    "    \"\"\"\n",
    "    # Ensure the target directory exists before copying files into it.\n",
    "    try:\n",
    "        dbutils.fs.mkdirs(target_path)\n",
    "    except Exception as e:\n",
    "        log_entry = {\n",
    "            \"entity_type\": \"directory\",\n",
    "            \"entity_name\": target_path,\n",
    "            \"action\": \"create\",\n",
    "            \"status\": \"error\",\n",
    "            \"message\": f\"Failed to create target directory. Error: {e}\",\n",
    "            \"timestamp\": datetime.now(),\n",
    "            \"results_data\": []\n",
    "        }\n",
    "        insert_log_entry(log_entry)\n",
    "\n",
    "    try:\n",
    "        # List contents of the source path.\n",
    "        files = dbutils.fs.ls(source_path)\n",
    "    except Exception as e:\n",
    "        log_entry = {\n",
    "            \"entity_type\": \"directory\",\n",
    "            \"entity_name\": source_path,\n",
    "            \"action\": \"list\",\n",
    "            \"status\": \"error\",\n",
    "            \"message\": f\"Error listing contents of directory. Error: {e}\",\n",
    "            \"timestamp\": datetime.now(),\n",
    "            \"results_data\": []\n",
    "        }\n",
    "        insert_log_entry(log_entry)\n",
    "        return\n",
    "\n",
    "    for file_info in files:\n",
    "        # Check if the item is a directory by looking for a trailing slash in its name\n",
    "        if file_info.name.endswith('/'):\n",
    "            # It's a directory, so we need to process it recursively.\n",
    "            log_entry = {\n",
    "                \"entity_type\": \"directory\",\n",
    "                \"entity_name\": file_info.path,\n",
    "                \"action\": \"list\",\n",
    "                \"status\": \"success\",\n",
    "                \"message\": \"Found directory to traverse.\",\n",
    "                \"timestamp\": datetime.now(),\n",
    "                \"results_data\": []\n",
    "            }\n",
    "            insert_log_entry(log_entry)\n",
    "            # Construct the new target path correctly for the subdirectory\n",
    "            new_target_path = os.path.join(target_path, file_info.name)\n",
    "            sync_volume_recursively(file_info.path, new_target_path, prev_run_info)\n",
    "        else:\n",
    "            # It's a file. Check if it needs to be copied.\n",
    "            relative_path = file_info.path.replace(source_path, '', 1).lstrip('/')\n",
    "            \n",
    "            # Compare modification times.\n",
    "            if file_info.path not in prev_run_info or file_info.modificationTime > prev_run_info[file_info.path]:\n",
    "                try:\n",
    "                    # Attempt to copy the file.\n",
    "                    # The dbutils.fs.cp command copies the source file into the destination directory.\n",
    "                    dbutils.fs.cp(file_info.path, target_path, recurse=True)\n",
    "                    \n",
    "                    # Log successful copy with file modification time.\n",
    "                    results_data = [\n",
    "                        {\"key\": \"result\", \"value\": \"True\"},\n",
    "                        {\"key\": \"modificationTime\", \"value\": str(file_info.modificationTime)},\n",
    "                        {\"key\": \"fileSize\", \"value\": str(file_info.size)},\n",
    "                        {\"key\": \"source_path\", \"value\": file_info.path},\n",
    "                        {\"key\": \"target_path\", \"value\": target_path}\n",
    "                    ]\n",
    "                    log_entry = {\n",
    "                        \"entity_type\": \"file\",\n",
    "                        \"entity_name\": file_info.path,\n",
    "                        \"action\": \"copy\",\n",
    "                        \"status\": \"success\",\n",
    "                        \"message\": \"File copied successfully.\",\n",
    "                        \"timestamp\": datetime.now(),\n",
    "                        \"results_data\": results_data\n",
    "                    }\n",
    "                    insert_log_entry(log_entry)\n",
    "                except Exception as e:\n",
    "                    # Log a failure to copy the file.\n",
    "                    log_entry = {\n",
    "                        \"entity_type\": \"file\",\n",
    "                        \"entity_name\": file_info.path,\n",
    "                        \"action\": \"copy\",\n",
    "                        \"status\": \"error\",\n",
    "                        \"message\": f\"Error during file copy. Error: {e}\",\n",
    "                        \"timestamp\": datetime.now(),\n",
    "                        \"results_data\": []\n",
    "                    }\n",
    "                    insert_log_entry(log_entry)\n",
    "            else:\n",
    "                log_entry = {\n",
    "                    \"entity_type\": \"file\",\n",
    "                    \"entity_name\": file_info.path,\n",
    "                    \"action\": \"skip\",\n",
    "                    \"status\": \"success\",\n",
    "                    \"message\": \"File already exists with same or newer modification date. Skipping.\",\n",
    "                    \"timestamp\": datetime.now(),\n",
    "                    \"results_data\": []\n",
    "                }\n",
    "                insert_log_entry(log_entry)\n",
    "\n",
    "def execute_and_log_sql(sql_command, entity_type, entity_name, action, success_msg, error_msg):\n",
    "    \"\"\"\n",
    "    Executes a SQL command and logs the result.\n",
    "    \n",
    "    Args:\n",
    "        sql_command (str): The SQL command to execute.\n",
    "        entity_type (str): The type of entity being acted upon (e.g., 'catalog', 'schema').\n",
    "        entity_name (str): The name of the entity.\n",
    "        action (str): The action being performed (e.g., 'create').\n",
    "        success_msg (str): Message to log on success.\n",
    "        error_msg (str): Message to log on error.\n",
    "    \"\"\"\n",
    "    spark_session = SparkSession.builder.getOrCreate()\n",
    "    try:\n",
    "        spark_session.sql(sql_command)\n",
    "        log_entry = {\n",
    "            \"entity_type\": entity_type,\n",
    "            \"entity_name\": entity_name,\n",
    "            \"action\": action,\n",
    "            \"status\": \"success\",\n",
    "            \"message\": success_msg,\n",
    "            \"timestamp\": datetime.now(),\n",
    "            \"results_data\": []\n",
    "        }\n",
    "        insert_log_entry(log_entry)\n",
    "    except Exception as e:\n",
    "        log_entry = {\n",
    "            \"entity_type\": entity_type,\n",
    "            \"entity_name\": entity_name,\n",
    "            \"action\": action,\n",
    "            \"status\": \"error\",\n",
    "            \"message\": f\"{error_msg} Error: {e}\",\n",
    "            \"timestamp\": datetime.now(),\n",
    "            \"results_data\": []\n",
    "        }\n",
    "        insert_log_entry(log_entry)\n",
    "\n",
    "\n",
    "def process_volume_sync(source_catalog, source_schema, target_catalog, target_schema, volume_name):\n",
    "    \"\"\"\n",
    "    Processes the synchronization for a single volume.\n",
    "    \n",
    "    Args:\n",
    "        source_catalog (str): The source catalog name.\n",
    "        source_schema (str): The source schema name.\n",
    "        target_catalog (str): The target catalog name.\n",
    "        target_schema (str): The target schema name.\n",
    "        volume_name (str): The name of the volume to sync.\n",
    "    \"\"\"\n",
    "    print(f\"Starting sync for volume: {source_catalog}.{source_schema}.{volume_name}\")\n",
    "    spark_session = SparkSession.builder.getOrCreate()\n",
    "    try:\n",
    "        # 1. Create the target volume if it doesn't exist.\n",
    "        sql_command = f\"CREATE VOLUME IF NOT EXISTS `{target_catalog}`.`{target_schema}`.`{volume_name}`\"\n",
    "        try:\n",
    "            spark_session.sql(sql_command)\n",
    "            log_entry = {\n",
    "                \"entity_type\": \"volume\",\n",
    "                \"entity_name\": f\"{target_catalog}.{target_schema}.{volume_name}\",\n",
    "                \"action\": \"create\",\n",
    "                \"status\": \"success\",\n",
    "                \"message\": \"Volume created or already exists.\",\n",
    "                \"timestamp\": datetime.now(),\n",
    "                \"results_data\": []\n",
    "            }\n",
    "            insert_log_entry(log_entry)\n",
    "        except Exception as e:\n",
    "            log_entry = {\n",
    "                \"entity_type\": \"volume\",\n",
    "                \"entity_name\": f\"{target_catalog}.{target_schema}.{volume_name}\",\n",
    "                \"action\": \"create\",\n",
    "                \"status\": \"error\",\n",
    "                \"message\": f\"Failed to create volume. Error: {e}\",\n",
    "                \"timestamp\": datetime.now(),\n",
    "                \"results_data\": []\n",
    "            }\n",
    "            insert_log_entry(log_entry)\n",
    "            return\n",
    "\n",
    "        # 2. Construct source and target paths for file system operations.\n",
    "        source_path = os.path.join(\"/Volumes\", source_catalog, source_schema, volume_name)\n",
    "        target_path = os.path.join(\"/Volumes\", target_catalog, target_schema, volume_name)\n",
    "        \n",
    "        # 3. Get modification dates from previous runs to enable efficient sync.\n",
    "        prev_run_info = get_previous_run_info(source_path)\n",
    "\n",
    "        # 4. Start the recursive file sync.\n",
    "        sync_volume_recursively(source_path, target_path, prev_run_info)\n",
    "        \n",
    "    except Exception as e:\n",
    "        log_entry = {\n",
    "            \"entity_type\": \"volume_sync\",\n",
    "            \"entity_name\": f\"{source_catalog}.{source_schema}.{volume_name}\",\n",
    "            \"action\": \"sync\",\n",
    "            \"status\": \"error\",\n",
    "            \"message\": f\"An unexpected error occurred during volume sync. Error: {e}\",\n",
    "            \"timestamp\": datetime.now(),\n",
    "            \"results_data\": []\n",
    "        }\n",
    "        insert_log_entry(log_entry)\n",
    "\n",
    "# ==============================================================================\n",
    "# MAIN EXECUTION LOGIC\n",
    "# ==============================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "    spark.sql(\"SET spark.sql.catalog.spark_catalog = 'spark_catalog'\")\n",
    "    \n",
    "    print(\"Starting Databricks Volume DR Sync.\")\n",
    "    \n",
    "    log_entry = {\n",
    "        \"entity_type\": \"info\",\n",
    "        \"entity_name\": \"N/A\",\n",
    "        \"action\": \"start_sync\",\n",
    "        \"status\": \"success\",\n",
    "        \"message\": \"Starting Databricks Volume DR Sync.\",\n",
    "        \"timestamp\": datetime.now(),\n",
    "        \"results_data\": []\n",
    "    }\n",
    "    insert_log_entry(log_entry)\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = []\n",
    "\n",
    "        for catalog_pair in catalog_pair_names_list:\n",
    "            try:\n",
    "                source_catalog, target_catalog = catalog_pair.split(':')\n",
    "            except ValueError:\n",
    "                log_entry = {\n",
    "                    \"entity_type\": \"info\",\n",
    "                    \"entity_name\": catalog_pair,\n",
    "                    \"action\": \"parse_catalog_pair\",\n",
    "                    \"status\": \"error\",\n",
    "                    \"message\": f\"Invalid catalog pair format: {catalog_pair}. Skipping.\",\n",
    "                    \"timestamp\": datetime.now(),\n",
    "                    \"results_data\": []\n",
    "                }\n",
    "                insert_log_entry(log_entry)\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                # 1. Create target catalog if it doesn't exist.\n",
    "                execute_and_log_sql(\n",
    "                    f\"CREATE CATALOG IF NOT EXISTS `{target_catalog}`\",\n",
    "                    \"catalog\",\n",
    "                    target_catalog,\n",
    "                    \"create\",\n",
    "                    f\"Catalog `{target_catalog}` created or already exists.\",\n",
    "                    f\"Failed to create catalog `{target_catalog}`.\"\n",
    "                )\n",
    "                \n",
    "                # 2. Get list of schemas in the source catalog.\n",
    "                schemas_df = spark.sql(f\"SHOW SCHEMAS IN `{source_catalog}`\")\n",
    "                schemas_to_process = [row['databaseName'] for row in schemas_df.collect()]\n",
    "                \n",
    "                for source_schema in schemas_to_process:\n",
    "                    # 3. Ignore 'information_schema'.\n",
    "                    if source_schema.lower() == 'information_schema':\n",
    "                        log_entry = {\n",
    "                            \"entity_type\": \"schema\",\n",
    "                            \"entity_name\": f\"{source_catalog}.{source_schema}\",\n",
    "                            \"action\": \"skip\",\n",
    "                            \"status\": \"success\",\n",
    "                            \"message\": \"Skipping 'information_schema'.\",\n",
    "                            \"timestamp\": datetime.now(),\n",
    "                            \"results_data\": []\n",
    "                        }\n",
    "                        insert_log_entry(log_entry)\n",
    "                        continue\n",
    "\n",
    "                    try:\n",
    "                        # 4. Create target schema if it doesn't exist.\n",
    "                        execute_and_log_sql(\n",
    "                            f\"CREATE SCHEMA IF NOT EXISTS `{target_catalog}`.`{source_schema}`\",\n",
    "                            \"schema\",\n",
    "                            f\"{target_catalog}.{source_schema}\",\n",
    "                            \"create\",\n",
    "                            f\"Schema `{source_schema}` in catalog `{target_catalog}` created or already exists.\",\n",
    "                            f\"Failed to create schema `{source_schema}`.\"\n",
    "                        )\n",
    "                        \n",
    "                        # 5. Get list of volumes in the source schema.\n",
    "                        volumes_df = spark.sql(f\"SHOW VOLUMES IN `{source_catalog}`.`{source_schema}`\")\n",
    "                        volumes_to_process = [row['volume_name'] for row in volumes_df.collect()]\n",
    "                        \n",
    "                        for volume_name in volumes_to_process:\n",
    "                            # 6. Submit volume sync tasks to the thread pool.\n",
    "                            future = executor.submit(\n",
    "                                process_volume_sync,\n",
    "                                source_catalog,\n",
    "                                source_schema,\n",
    "                                target_catalog,\n",
    "                                source_schema, # Use source_schema as target_schema for simplicity\n",
    "                                volume_name\n",
    "                            )\n",
    "                            futures.append(future)\n",
    "\n",
    "                    except Exception as e:\n",
    "                        log_entry = {\n",
    "                            \"entity_type\": \"schema\",\n",
    "                            \"entity_name\": f\"{source_catalog}.{source_schema}\",\n",
    "                            \"action\": \"process\",\n",
    "                            \"status\": \"error\",\n",
    "                            \"message\": f\"An error occurred while processing schema. Error: {e}\",\n",
    "                            \"timestamp\": datetime.now(),\n",
    "                            \"results_data\": []\n",
    "                        }\n",
    "                        insert_log_entry(log_entry)\n",
    "\n",
    "            except Exception as e:\n",
    "                log_entry = {\n",
    "                    \"entity_type\": \"catalog\",\n",
    "                    \"entity_name\": source_catalog,\n",
    "                    \"action\": \"process\",\n",
    "                    \"status\": \"error\",\n",
    "                    \"message\": f\"An error occurred while processing catalog pair. Error: {e}\",\n",
    "                    \"timestamp\": datetime.now(),\n",
    "                    \"results_data\": []\n",
    "                }\n",
    "                insert_log_entry(log_entry)\n",
    "\n",
    "        # Wait for all tasks to complete and retrieve results.\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            try:\n",
    "                future.result()\n",
    "            except Exception as e:\n",
    "                log_entry = {\n",
    "                    \"entity_type\": \"info\",\n",
    "                    \"entity_name\": \"N/A\",\n",
    "                    \"action\": \"future_result\",\n",
    "                    \"status\": \"error\",\n",
    "                    \"message\": f\"An unexpected error occurred in a thread. Error: {e}\",\n",
    "                    \"timestamp\": datetime.now(),\n",
    "                    \"results_data\": []\n",
    "                }\n",
    "                insert_log_entry(log_entry)\n",
    "                \n",
    "    print(\"Databricks Volume DR Sync completed.\")\n",
    "\n",
    "    log_entry = {\n",
    "        \"entity_type\": \"info\",\n",
    "        \"entity_name\": \"N/A\",\n",
    "        \"action\": \"end_sync\",\n",
    "        \"status\": \"success\",\n",
    "        \"message\": \"Databricks Volume DR Sync completed.\",\n",
    "        \"timestamp\": datetime.now(),\n",
    "        \"results_data\": []\n",
    "    }\n",
    "    insert_log_entry(log_entry)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f309a787-ad33-4ff8-9a97-25ea41c8bc72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# spark.sql(f\"\"\"\n",
    "#     DELETE\n",
    "#     FROM {log_table_name}\n",
    "#     WHERE notebook_name = '{notebook_name}'\n",
    "# \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99752649-dcbe-4fff-b1d7-de708a0652a8",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"entity_name\":574},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1758226483849}",
       "filterBlob": "{\"version\":1,\"filterGroups\":[{\"enabled\":true,\"filterGroupId\":\"fg_8daa1a8c\",\"op\":\"OR\",\"filters\":[{\"filterId\":\"f_c05e134b\",\"enabled\":true,\"columnId\":\"entity_name\",\"dataType\":\"string\",\"filterType\":\"contains\",\"filterValue\":\"satya\",\"filterConfig\":{}}],\"local\":false,\"updatedAt\":1758562320352}],\"syncTimestamp\":1758562320352}",
       "queryPlanFiltersBlob": "[{\"kind\":\"call\",\"function\":\"or\",\"args\":[{\"kind\":\"call\",\"function\":\"ilike\",\"args\":[{\"kind\":\"identifier\",\"identifier\":\"entity_name\"},{\"kind\":\"literal\",\"value\":\"%satya%\",\"type\":\"string\"}]}]}]",
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Display logs for the last 24 hours\n",
    "print(f\"\\n--- Displaying logs from {log_table_name} for the last 24 hours ---\")\n",
    "# past_24_hours = datetime.now() - timedelta(hours=24)\n",
    "log_query_df = spark.sql(f\"\"\"\n",
    "    SELECT *\n",
    "    FROM {log_table_name} where notebook_name = '{notebook_name}'\n",
    "    ORDER BY timestamp DESC\n",
    "\"\"\")\n",
    "\n",
    "display(log_query_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c47613d-0b98-4928-ac8c-dc0f4a44054a",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1754946235548}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %sql\n",
    "# drop table \n",
    "# -- select * from \n",
    "# -- users.satyendranath_sure.dr_log_table_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0c1e6bd-1c5a-4213-a6b3-3ceb7cb46cc8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d40b06da-3b23-412b-a7c5-e1356fb0336e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68de7672-75ff-4d70-b73a-43f3fd2173e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90aa59aa-7073-4fe7-9fed-47c6bc3318ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97ebd6ef-8e17-4a9f-bccf-f86aab80bb3e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9de1c82-4ed0-4f29-9965-81a2ea4dc5cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "dbe_41a50460-c90b-4840-9288-afcb847395d5",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8748648698906789,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "DR_Volume_data",
   "widgets": {
    "catalog_names": {
     "currentValue": "_eo_amperity:dr_eo_amperity, accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample:dr_accuweather, test_not:dr_test_not, satya_share:satya_dr_catalog",
     "nuid": "5f06e975-cdb8-43fe-a80b-4c55190ca561",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "catalog_names",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "catalog_names",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "log_table_name": {
     "currentValue": "users.satyendranath_sure.dr_log_table_name",
     "nuid": "6bec95bd-2265-43c8-b89b-7602c26ce306",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "users.satyendranath_sure.dr_log_table_name",
      "label": null,
      "name": "log_table_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "users.satyendranath_sure.dr_log_table_name",
      "label": null,
      "name": "log_table_name",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "max_workers": {
     "currentValue": "5",
     "nuid": "6d8539ad-a4d2-4f54-bf5c-9b7689d1d058",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "5",
      "label": null,
      "name": "max_workers",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "5",
      "label": null,
      "name": "max_workers",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "target_schema_owner": {
     "currentValue": "dr_eo_amperity.bronze:grp_test_clt_usa, dr_eo_amperity.silver:grp_test_clt_usa, dr_eo_amperity.default:grp_test_clt_usa",
     "nuid": "af424e30-5de1-47ac-8079-3fea6dfda1df",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "target_schema_owner",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "target_schema_owner",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}