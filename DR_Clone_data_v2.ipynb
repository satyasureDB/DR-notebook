{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ebfa0b4f-d67b-4bcf-82ba-631b8130d3e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Script Documentation\n",
    "\n",
    "This document describes the functionality of a Databricks script designed to process catalog pairs, create schemas, set ownership, and clone tables or create views, with comprehensive logging of all actions. The script uses parallel processing for efficiency and logs results in a structured table.\n",
    "\n",
    "## Parameters Overview\n",
    "\n",
    "The notebook accepts the following parameters to drive the logic for processing catalog pairs, schema ownership, parallel execution, and logging:\n",
    "\n",
    "1. **catalog_pair_names_list**:\n",
    "   - **Type**: List of strings\n",
    "   - **Description**: A list of strings in the format `source_catalog:target_catalog`, specifying pairs of source and target catalogs. The script clones tables or creates views from the source catalog to the target catalog.\n",
    "   - **Example Value**: `['_eo_amperity:dr_eo_amperity', 'accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample:dr_accuweather', 'test_not:dr_test_not', 'satya_share:satya_dr_catalog']`\n",
    "   - **Usage**: Iterates over catalog pairs to replicate schemas, tables, and views from the source to the target catalog.\n",
    "\n",
    "2. **target_schema_owner_list**:\n",
    "   - **Type**: List of strings\n",
    "   - **Description**: A list of strings in the format `catalog.schema:owner_group`, defining ownership for schemas in the target catalog. Matching schemas are assigned the specified `owner_group`.\n",
    "   - **Example Value**: `['dr_eo_amperity.bronze:grp_test_clt_usa', 'dr_eo_amperity.silver:grp_test_clt_usa', 'dr_eo_amperity.default:grp_test_clt_usa']`\n",
    "   - **Usage**: Assigns ownership to schemas in the target catalog after creation to enforce access control.\n",
    "\n",
    "3. **max_workers**:\n",
    "   - **Type**: Integer\n",
    "   - **Description**: Specifies the maximum number of worker threads for parallel processing of tables and views using `ThreadPoolExecutor`.\n",
    "   - **Example Value**: `5`\n",
    "   - **Usage**: Controls parallelism to optimize performance during table/view processing.\n",
    "\n",
    "4. **log_table_name**:\n",
    "   - **Type**: String\n",
    "   - **Description**: The fully qualified name of the table where logs are stored.\n",
    "   - **Example Value**: `users.satyendranath_sure.dr_log_table_name`\n",
    "   - **Usage**: Stores detailed logs of all actions (e.g., catalog creation, schema ownership changes, table cloning) in a structured format.\n",
    "\n",
    "### Example Parameter Usage\n",
    "\n",
    "```python\n",
    "catalog_pair_names_list = [\n",
    "    '_eo_amperity:dr_eo_amperity',\n",
    "    'accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample:dr_accuweather',\n",
    "    'test_not:dr_test_not',\n",
    "    'satya_share:satya_dr_catalog'\n",
    "]\n",
    "target_schema_owner_list = [\n",
    "    'dr_eo_amperity.bronze:grp_test_clt_usa',\n",
    "    'dr_eo_amperity.silver:grp_test_clt_usa',\n",
    "    'dr_eo_amperity.default:grp_test_clt_usa'\n",
    "]\n",
    "max_workers = 5\n",
    "log_table_name = 'users.satyendranath_sure.dr_log_table_name'\n",
    "\n",
    "## Script Functions\n",
    "\n",
    "The script is organized into several functions, each with a specific role in the overall workflow.\n",
    "\n",
    "#### `main(catalog_pair_names_list, target_schema_owner_list, max_workers)`\n",
    "This is the entry point of the script. It first sequentially collects all table and view tasks to be performed. It then uses a `ThreadPoolExecutor` to process these tasks in parallel, with the number of workers limited by `max_workers`.\n",
    "\n",
    "#### `collect_all_table_tasks(catalog_pair_names_list, target_schema_owner_list)`\n",
    "This function iterates through each `catalog_pair_names_list`. For each pair, it first creates the target catalog, then retrieves all schemas from the source catalog. It creates each schema in the target catalog and sets its ownership based on `target_schema_owner_list`. Finally, it collects a list of all tables and views from the source schemas to be processed later.\n",
    "\n",
    "#### `create_catalog_if_not_exists(target_catalog)`\n",
    "This function creates the specified catalog if it doesn't already exist. All actions are logged to the designated log table.\n",
    "\n",
    "#### `create_schema_and_set_owner(target_catalog, schema, target_schema_owner_list)`\n",
    "This function first creates the schema within the target catalog. It then attempts to set the schema's ownership to the owner group specified in `target_schema_owner_list` if a matching entry is found.\n",
    "\n",
    "#### `process_table_or_view(source_catalog, target_catalog, schema_name, table_row)`\n",
    "This function is responsible for cloning tables and creating views. It first determines whether the object is a table or a view. If it's a table, it uses a `CREATE OR REPLACE TABLE... CLONE` command. If it's a view, it uses `CREATE OR REPLACE VIEW` with the original view definition. Temporary tables and views are skipped.\n",
    "\n",
    "\n",
    "## Log Table Schema\n",
    "\n",
    "The script creates and populates a log table, which records the results of every action performed. This is useful for auditing and troubleshooting.\n",
    "\n",
    "| Column       | Type                       | Description                                       |\n",
    "|--------------|----------------------------|---------------------------------------------------|\n",
    "| `entity_type`  | `STRING`                   | The type of object being processed (e.g., `catalog`, `schema`, `table`, `view`). |\n",
    "| `entity_name`  | `STRING`                   | The fully qualified name of the object.           |\n",
    "| `action`       | `STRING`                   | The action taken (e.g., `create`, `clone`, `change_ownership`, `skip`). |\n",
    "| `status`       | `STRING`                   | The outcome of the action (`success`, `error`, `skipped`). |\n",
    "| `message`      | `STRING`                   | A detailed message describing the action's result. |\n",
    "| `timestamp`    | `TIMESTAMP`                | The time the action was logged.                   |\n",
    "| `results_data` | `ARRAY<STRUCT<...>>`       | Additional key-value pairs of data from successful SQL commands. |\n",
    "\n",
    "\n",
    "## Execute the main function\n",
    "main(catalog_pair_names_list, target_schema_owner_list, max_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93ee5629-626d-4ad6-adb4-3a7605c391fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# dbutils.widgets.removeAll()\n",
    "dbutils.widgets.text(\"catalog_names\", \"\")#source and target catalog names like dictionary\n",
    "dbutils.widgets.text(\"target_schema_owner\", \"\")\n",
    "dbutils.widgets.text(\"max_workers\", \"5\")\n",
    "dbutils.widgets.text(\"log_table_name\", \"users.satyendranath_sure.dr_log_table_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff63f959-a4f1-46a1-ad76-524b7ef329fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_eo_amperity:dr_eo_amperity', 'accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample:dr_accuweather', 'test_not:dr_test_not', 'satya_share:satya_dr_catalog'] ['dr_eo_amperity.bronze:grp_test_clt_usa', 'dr_eo_amperity.silver:grp_test_clt_usa', 'dr_eo_amperity.default:grp_test_clt_usa'] 5 users.satyendranath_sure.dr_log_table_name\n"
     ]
    }
   ],
   "source": [
    "catalog_pair_names_list = [catalog_pair.strip() for catalog_pair in dbutils.widgets.get(\"catalog_names\").split(\",\")]\n",
    "target_schema_owner_list = [catalog_schema_pair.strip() for catalog_schema_pair in dbutils.widgets.get(\"target_schema_owner\").split(\",\")]\n",
    "max_workers = int(dbutils.widgets.get(\"max_workers\"))\n",
    "log_table_name = dbutils.widgets.get(\"log_table_name\")\n",
    "print(catalog_pair_names_list, target_schema_owner_list, max_workers, log_table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "946565cb-9b3c-4b5d-ad9d-f7274b949ca2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dr_eo_amperity\ndr_accuweather\ndr_test_not\nsatya_dr_catalog\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for catalog_pair in catalog_pair_names_list:\n",
    "#     second_value = catalog_pair.split(\":\")[1]\n",
    "#     print(second_value)\n",
    "#     spark.sql(f\"DROP CATALOG IF EXISTS `{second_value}` CASCADE\")\n",
    "# spark.sql(f\"DROP table if exists `{log_table_name}`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe5b881f-feae-477f-a041-0e9f2aae4166",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting sequential catalog and schema processing...\nProcessing catalog pair: _eo_amperity -> dr_eo_amperity\n  Processing schema: `_eo_amperity`.`bronze` -> `dr_eo_amperity`.`bronze`\n  Processing schema: `_eo_amperity`.`default` -> `dr_eo_amperity`.`default`\n  Skipping schema: `_eo_amperity`.`information_schema`\n  Processing schema: `_eo_amperity`.`silver` -> `dr_eo_amperity`.`silver`\nProcessing catalog pair: accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample -> dr_accuweather\n  Processing schema: `accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample`.`forecast` -> `dr_accuweather`.`forecast`\n  Skipping schema: `accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample`.`information_schema`\nProcessing catalog pair: test_not -> dr_test_not\nProcessing catalog pair: satya_share -> satya_dr_catalog\n  Skipping schema: `satya_share`.`information_schema`\n  Processing schema: `satya_share`.`satyendranath_sure` -> `satya_dr_catalog`.`satyendranath_sure`\nCollected 35 tables/views to process.\nStarting parallel table processing...\nProcessing table/view: `_eo_amperity`.`bronze`.`e_commerce_customers`\nProcessing table/view: `_eo_amperity`.`bronze`.`loyalty_members`\nProcessing table/view: `_eo_amperity`.`bronze`.`pos_customers`\nProcessing table/view: `_eo_amperity`.`bronze`.`web_visitors`\nProcessing table/view: `_eo_amperity`.`bronze`.`wifi_connections`\nProcessing table/view: `_eo_amperity`.`default`.`analytics_tae`\nProcessing table/view: `_eo_amperity`.`default`.`customer_360`\nProcessing table/view: `_eo_amperity`.`default`.`customer_attributes`\nProcessing table/view: `_eo_amperity`.`default`.`email_engagement_attributes`\nProcessing table/view: `_eo_amperity`.`default`.`email_engagement_summary`\nProcessing table/view: `_eo_amperity`.`default`.`predicted_affinity`\nProcessing table/view: `_eo_amperity`.`default`.`predicted_clv_attributes`\nProcessing table/view: `_eo_amperity`.`default`.`transaction_attributes`\nProcessing table/view: `_eo_amperity`.`default`.`transaction_attributes_extended`\nProcessing table/view: `_eo_amperity`.`default`.`unified_transactions_reporting`\nProcessing table/view: `_eo_amperity`.`silver`.`fiscal_calendar`\nProcessing table/view: `_eo_amperity`.`silver`.`unified_coalesced`\nProcessing table/view: `_eo_amperity`.`silver`.`unified_compliance`\nProcessing table/view: `_eo_amperity`.`silver`.`unified_customer`\nProcessing table/view: `_eo_amperity`.`silver`.`unified_household`\nProcessing table/view: `_eo_amperity`.`silver`.`unified_itemized_transactions`\nProcessing table/view: `_eo_amperity`.`silver`.`unified_loyalty`\nProcessing table/view: `_eo_amperity`.`silver`.`unified_loyalty_events`\nProcessing table/view: `_eo_amperity`.`silver`.`unified_merged`\nProcessing table/view: `_eo_amperity`.`silver`.`unified_paid_media`\nProcessing table/view: `_eo_amperity`.`silver`.`unified_scores`\nProcessing table/view: `_eo_amperity`.`silver`.`unified_transactions`\nProcessing table/view: `accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample`.`forecast`.`us_postal_daily_imperial`\nProcessing table/view: `accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample`.`forecast`.`us_postal_daily_metric`\nProcessing table/view: `accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample`.`forecast`.`us_postal_daynight_imperial`\nProcessing table/view: `accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample`.`forecast`.`us_postal_daynight_metric`\nProcessing table/view: `accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample`.`forecast`.`us_postal_hourly_imperial`\nProcessing table/view: `accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample`.`forecast`.`us_postal_hourly_metric`\nProcessing table/view: `satya_share`.`satyendranath_sure`.`satya_external_table`\nProcessing table/view: `satya_share`.`satyendranath_sure`.`satya_managed_table`\nProcessing complete.\n"
     ]
    }
   ],
   "source": [
    "import traceback\n",
    "import concurrent.futures\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType, ArrayType\n",
    "from datetime import datetime\n",
    "\n",
    "# Get the current notebook name\n",
    "notebook_path = dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get()\n",
    "notebook_name = notebook_path.split('/')[-1]\n",
    "\n",
    "# Schema definition for log table\n",
    "log_table_schema = StructType([\n",
    "    StructField(\"notebook_name\", StringType(), True),\n",
    "    StructField(\"entity_type\", StringType(), True),\n",
    "    StructField(\"entity_name\", StringType(), True),\n",
    "    StructField(\"action\", StringType(), True),\n",
    "    StructField(\"status\", StringType(), True),\n",
    "    StructField(\"message\", StringType(), True),\n",
    "    StructField(\"timestamp\", TimestampType(), True),\n",
    "    StructField(\"results_data\", ArrayType(StructType([\n",
    "        StructField(\"key\", StringType(), True),\n",
    "        StructField(\"value\", StringType(), True)\n",
    "    ])), True)\n",
    "])\n",
    "\n",
    "\n",
    "# Create log table\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {log_table_name}\n",
    "    (\n",
    "        notebook_name STRING,\n",
    "        entity_type STRING,\n",
    "        entity_name STRING,\n",
    "        action STRING,\n",
    "        status STRING,\n",
    "        message STRING,\n",
    "        timestamp TIMESTAMP,\n",
    "        results_data ARRAY<STRUCT<key: STRING, value: STRING>>\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "def insert_log_entry(log_data):\n",
    "    try:\n",
    "        # Add the notebook_name to the log_data dictionary\n",
    "        log_data[\"notebook_name\"] = notebook_name\n",
    "        spark.createDataFrame([log_data], schema=log_table_schema).write.mode(\"append\").saveAsTable(log_table_name)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to insert log entry: {log_data}. Error: {e}\")\n",
    "\n",
    "def execute_and_log_sql(sql_command, entity_type, entity_name, action, success_message, error_message):\n",
    "    try:\n",
    "        results_df = spark.sql(sql_command)\n",
    "        results = [\n",
    "            {\"key\": col_name, \"value\": str(row[col_name])}\n",
    "            for row in results_df.collect()\n",
    "            for col_name in row.__fields__\n",
    "        ]\n",
    "        log_entry = {\n",
    "            \"entity_type\": entity_type,\n",
    "            \"entity_name\": entity_name,\n",
    "            \"action\": action,\n",
    "            \"status\": \"success\",\n",
    "            \"message\": success_message,\n",
    "            \"timestamp\": datetime.now(),\n",
    "            \"results_data\": results\n",
    "        }\n",
    "    except Exception as e:\n",
    "        log_entry = {\n",
    "            \"entity_type\": entity_type,\n",
    "            \"entity_name\": entity_name,\n",
    "            \"action\": action,\n",
    "            \"status\": \"error\",\n",
    "            \"message\": f\"{error_message}. Error: {e}\",\n",
    "            \"timestamp\": datetime.now(),\n",
    "            \"results_data\": []\n",
    "        }\n",
    "    \n",
    "    insert_log_entry(log_entry)\n",
    "\n",
    "\n",
    "def create_catalog_if_not_exists(target_catalog):\n",
    "    sql_command = f\"CREATE CATALOG IF NOT EXISTS `{target_catalog}`\"\n",
    "    execute_and_log_sql(\n",
    "        sql_command, \n",
    "        \"catalog\", \n",
    "        f\"`{target_catalog}`\", \n",
    "        \"create\",\n",
    "        f\"Catalog `{target_catalog}` created or already exists.\",\n",
    "        f\"Failed to create catalog `{target_catalog}`.\"\n",
    "    )\n",
    "\n",
    "def get_source_schemas(source_catalog):\n",
    "    try:\n",
    "        return [row.databaseName for row in spark.sql(f\"SHOW DATABASES IN `{source_catalog}`\").collect()], None\n",
    "    except Exception as e:\n",
    "        return None, f\"Failed to get schemas from source catalog `{source_catalog}`. Error: {e}\"\n",
    "\n",
    "def create_schema_and_set_owner(target_catalog, schema, target_schema_owner_list):\n",
    "    target_schema_fqn = f\"`{target_catalog}`.`{schema}`\"\n",
    "    \n",
    "    # Create schema\n",
    "    execute_and_log_sql(\n",
    "        f\"CREATE SCHEMA IF NOT EXISTS {target_schema_fqn}\",\n",
    "        \"schema\",\n",
    "        target_schema_fqn,\n",
    "        \"create\",\n",
    "        f\"Schema {target_schema_fqn} created or already exists.\",\n",
    "        f\"Failed to create schema {target_schema_fqn}.\"\n",
    "    )\n",
    "\n",
    "    # Set ownership\n",
    "    owner_info = f\"{target_catalog}.{schema}\"\n",
    "    for owner_mapping in target_schema_owner_list:\n",
    "        schema_to_match, owner_group = owner_mapping.split(':')\n",
    "        if owner_info == schema_to_match:\n",
    "            execute_and_log_sql(\n",
    "                f\"ALTER SCHEMA {target_schema_fqn} OWNER TO `{owner_group}`\",\n",
    "                \"schema\",\n",
    "                target_schema_fqn,\n",
    "                \"change_ownership\",\n",
    "                f\"Ownership of schema {target_schema_fqn} changed to `{owner_group}`.\",\n",
    "                f\"Failed to change ownership of schema {target_schema_fqn}.\"\n",
    "            )\n",
    "            return\n",
    "    insert_log_entry({\n",
    "        \"entity_type\": \"schema\",\n",
    "        \"entity_name\": target_schema_fqn,\n",
    "        \"action\": \"change_ownership\",\n",
    "        \"status\": \"skipped\",\n",
    "        \"message\": \"No ownership mapping found for this schema. Skipping ownership change.\",\n",
    "        \"timestamp\": datetime.now(),\n",
    "        \"results_data\": []\n",
    "    })\n",
    "\n",
    "def get_source_tables_and_views(source_catalog, schema_name):\n",
    "    try:\n",
    "        return spark.sql(f\"SHOW TABLES IN `{source_catalog}`.`{schema_name}`\").collect(), None\n",
    "    except Exception as e:\n",
    "        return None, f\"Failed to get tables from source schema `{source_catalog}`.`{schema_name}`. Error: {e}\"\n",
    "\n",
    "def process_table_or_view(source_catalog, target_catalog, schema_name, table_row):\n",
    "    table_name = table_row.tableName\n",
    "    is_temporary = table_row.isTemporary\n",
    "    source_table_fqn = f\"`{source_catalog}`.`{schema_name}`.`{table_name}`\"\n",
    "    target_table_fqn = f\"`{target_catalog}`.`{schema_name}`.`{table_name}`\"\n",
    "    \n",
    "    print(f\"Processing table/view: {source_table_fqn}\")\n",
    "\n",
    "    if is_temporary:\n",
    "        insert_log_entry({\n",
    "            \"entity_type\": \"table/view\",\n",
    "            \"entity_name\": source_table_fqn,\n",
    "            \"action\": \"clone/create\",\n",
    "            \"status\": \"skipped\",\n",
    "            \"message\": f\"Skipping temporary table/view {source_table_fqn}.\",\n",
    "            \"timestamp\": datetime.now(),\n",
    "            \"results_data\": []\n",
    "        })\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        desc_info = {\n",
    "            row.col_name: row.data_type \n",
    "            for row in spark.sql(f\"DESCRIBE TABLE EXTENDED {source_table_fqn}\").collect()\n",
    "            if row.col_name and row.data_type\n",
    "        }\n",
    "        \n",
    "        if desc_info.get(\"Type\") == \"VIEW\":\n",
    "            view_text = desc_info.get(\"View Text\")\n",
    "            if view_text:\n",
    "                execute_and_log_sql(\n",
    "                    f\"CREATE OR REPLACE VIEW {target_table_fqn} AS {view_text}\",\n",
    "                    \"view\",\n",
    "                    target_table_fqn,\n",
    "                    \"create\",\n",
    "                    f\"View {target_table_fqn} created successfully.\",\n",
    "                    f\"Failed to create view {target_table_fqn}.\"\n",
    "                )\n",
    "            else:\n",
    "                insert_log_entry({\n",
    "                    \"entity_type\": \"view\",\n",
    "                    \"entity_name\": source_table_fqn,\n",
    "                    \"action\": \"create\",\n",
    "                    \"status\": \"error\",\n",
    "                    \"message\": \"View text definition not found.\",\n",
    "                    \"timestamp\": datetime.now(),\n",
    "                    \"results_data\": []\n",
    "                })\n",
    "        else:\n",
    "            execute_and_log_sql(\n",
    "                f\"CREATE OR REPLACE TABLE {target_table_fqn} CLONE {source_table_fqn}\",\n",
    "                \"table\",\n",
    "                target_table_fqn,\n",
    "                \"clone\",\n",
    "                f\"Table {target_table_fqn} cloned successfully.\",\n",
    "                f\"Failed to clone table {target_table_fqn}.\"\n",
    "            )\n",
    "    except Exception as e:\n",
    "        insert_log_entry({\n",
    "            \"entity_type\": \"table/view\",\n",
    "            \"entity_name\": source_table_fqn,\n",
    "            \"action\": \"clone/create\",\n",
    "            \"status\": \"error\",\n",
    "            \"message\": f\"Failed to process table/view {source_table_fqn}. Error: {e}\",\n",
    "            \"timestamp\": datetime.now(),\n",
    "            \"results_data\": []\n",
    "        })\n",
    "\n",
    "def collect_all_table_tasks(catalog_pair_names_list, target_schema_owner_list):\n",
    "    all_tasks = []\n",
    "    \n",
    "    for catalog_pair_string in catalog_pair_names_list:\n",
    "        try:\n",
    "            source_catalog, target_catalog = catalog_pair_string.split(':')\n",
    "            print(f\"Processing catalog pair: {source_catalog} -> {target_catalog}\")\n",
    "            insert_log_entry({\n",
    "                \"entity_type\": \"catalog_pair\",\n",
    "                \"entity_name\": f\"{source_catalog} -> {target_catalog}\",\n",
    "                \"action\": \"process\",\n",
    "                \"status\": \"start\",\n",
    "                \"message\": f\"Starting processing for catalog pair: {source_catalog} -> {target_catalog}\",\n",
    "                \"timestamp\": datetime.now(),\n",
    "                \"results_data\": []\n",
    "            })\n",
    "\n",
    "            create_catalog_if_not_exists(target_catalog)\n",
    "            schemas, error = get_source_schemas(source_catalog)\n",
    "            \n",
    "            if error:\n",
    "                insert_log_entry({\n",
    "                    \"entity_type\": \"catalog\",\n",
    "                    \"entity_name\": f\"`{source_catalog}`\",\n",
    "                    \"action\": \"get_schemas\",\n",
    "                    \"status\": \"error\",\n",
    "                    \"message\": error,\n",
    "                    \"timestamp\": datetime.now(),\n",
    "                    \"results_data\": []\n",
    "                })\n",
    "                continue\n",
    "\n",
    "            for schema in schemas:\n",
    "                if schema == 'information_schema':\n",
    "                    print(f\"  Skipping schema: `{source_catalog}`.`{schema}`\")\n",
    "                    insert_log_entry({\n",
    "                        \"entity_type\": \"schema\",\n",
    "                        \"entity_name\": f\"`{source_catalog}`.`{schema}`\",\n",
    "                        \"action\": \"skip\",\n",
    "                        \"status\": \"success\",\n",
    "                        \"message\": \"Skipping `information_schema`.\",\n",
    "                        \"timestamp\": datetime.now(),\n",
    "                        \"results_data\": []\n",
    "                    })\n",
    "                    continue\n",
    "\n",
    "                print(f\"  Processing schema: `{source_catalog}`.`{schema}` -> `{target_catalog}`.`{schema}`\")\n",
    "                create_schema_and_set_owner(target_catalog, schema, target_schema_owner_list)\n",
    "                tables, tables_error = get_source_tables_and_views(source_catalog, schema)\n",
    "                \n",
    "                if tables_error:\n",
    "                    insert_log_entry({\n",
    "                        \"entity_type\": \"schema\",\n",
    "                        \"entity_name\": f\"`{source_catalog}`.`{schema}`\",\n",
    "                        \"action\": \"get_tables\",\n",
    "                        \"status\": \"error\",\n",
    "                        \"message\": tables_error,\n",
    "                        \"timestamp\": datetime.now(),\n",
    "                        \"results_data\": []\n",
    "                    })\n",
    "                    continue\n",
    "\n",
    "                for table_row in tables:\n",
    "                    all_tasks.append((source_catalog, target_catalog, schema, table_row))\n",
    "\n",
    "            insert_log_entry({\n",
    "                \"entity_type\": \"catalog_pair\",\n",
    "                \"entity_name\": f\"{source_catalog} -> {target_catalog}\",\n",
    "                \"action\": \"process\",\n",
    "                \"status\": \"sequential_complete\",\n",
    "                \"message\": f\"Finished sequential processing for catalog pair: {source_catalog} -> {target_catalog}\",\n",
    "                \"timestamp\": datetime.now(),\n",
    "                \"results_data\": []\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            insert_log_entry({\n",
    "                \"entity_type\": \"catalog_pair\",\n",
    "                \"entity_name\": f\"Processing of {catalog_pair_string}\",\n",
    "                \"action\": \"unhandled_error\",\n",
    "                \"status\": \"error\",\n",
    "                \"message\": f\"An unhandled error occurred: {e}\\n{traceback.format_exc()}\",\n",
    "                \"timestamp\": datetime.now(),\n",
    "                \"results_data\": []\n",
    "            })\n",
    "\n",
    "    return all_tasks\n",
    "\n",
    "def main(catalog_pair_names_list, target_schema_owner_list, max_workers):\n",
    "    print(\"Starting sequential catalog and schema processing...\")\n",
    "    \n",
    "    # Collect all table tasks sequentially\n",
    "    all_tasks = collect_all_table_tasks(catalog_pair_names_list, target_schema_owner_list)\n",
    "    print(f\"Collected {len(all_tasks)} tables/views to process.\")\n",
    "    # print(all_tasks)\n",
    "\n",
    "    # Process all tables in parallel\n",
    "    print(\"Starting parallel table processing...\")\n",
    "    # with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "    #     executor.map(\n",
    "    #         lambda task: process_table_or_view(*task),\n",
    "    #         all_tasks\n",
    "    #     )\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Submit all tasks at once and store futures\n",
    "        futures = [executor.submit(process_table_or_view, *task) for task in all_tasks]\n",
    "        # Wait for all tasks to complete\n",
    "        concurrent.futures.wait(futures)\n",
    "        # Optionally, check for exceptions\n",
    "        for future in futures:\n",
    "            try:\n",
    "                future.result()  # Ensure any exceptions are raised\n",
    "            except Exception as e:\n",
    "                print(f\"Task generated an exception: {e}\")\n",
    "\n",
    "    print(\"Processing complete.\")\n",
    "\n",
    "# Run job\n",
    "main(catalog_pair_names_list, target_schema_owner_list, max_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99752649-dcbe-4fff-b1d7-de708a0652a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Display logs for the last 24 hours\n",
    "print(f\"\\n--- Displaying logs from {log_table_name} for the last 24 hours ---\")\n",
    "# past_24_hours = datetime.now() - timedelta(hours=24)\n",
    "log_query_df = spark.sql(f\"\"\"\n",
    "    SELECT *\n",
    "    FROM {log_table_name}\n",
    "    ORDER BY timestamp DESC\n",
    "\"\"\")\n",
    "\n",
    "display(log_query_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c47613d-0b98-4928-ac8c-dc0f4a44054a",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1754946235548}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %sql\n",
    "# drop table \n",
    "# -- select * from \n",
    "# -- users.satyendranath_sure.dr_log_table_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0c1e6bd-1c5a-4213-a6b3-3ceb7cb46cc8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d40b06da-3b23-412b-a7c5-e1356fb0336e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68de7672-75ff-4d70-b73a-43f3fd2173e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90aa59aa-7073-4fe7-9fed-47c6bc3318ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97ebd6ef-8e17-4a9f-bccf-f86aab80bb3e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9de1c82-4ed0-4f29-9965-81a2ea4dc5cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "dbe_41a50460-c90b-4840-9288-afcb847395d5",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8748648698906789,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "DR_Clone_data_v2",
   "widgets": {
    "catalog_names": {
     "currentValue": "_eo_amperity:dr_eo_amperity, accuweather_daily_and_hourly_forecasts_u_s_postal_codes_sample:dr_accuweather, test_not:dr_test_not, satya_share:satya_dr_catalog",
     "nuid": "5f06e975-cdb8-43fe-a80b-4c55190ca561",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "catalog_names",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "catalog_names",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "log_table_name": {
     "currentValue": "users.satyendranath_sure.dr_log_table_name",
     "nuid": "6bec95bd-2265-43c8-b89b-7602c26ce306",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "users.satyendranath_sure.dr_log_table_name",
      "label": null,
      "name": "log_table_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "users.satyendranath_sure.dr_log_table_name",
      "label": null,
      "name": "log_table_name",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "max_workers": {
     "currentValue": "5",
     "nuid": "6d8539ad-a4d2-4f54-bf5c-9b7689d1d058",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "5",
      "label": null,
      "name": "max_workers",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "5",
      "label": null,
      "name": "max_workers",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "target_schema_owner": {
     "currentValue": "dr_eo_amperity.bronze:grp_test_clt_usa, dr_eo_amperity.silver:grp_test_clt_usa, dr_eo_amperity.default:grp_test_clt_usa",
     "nuid": "af424e30-5de1-47ac-8079-3fea6dfda1df",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "target_schema_owner",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "target_schema_owner",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}